# 数据库高可用

## 概述

可用性定义为系统保持正常运行时间的百分比，高可用可以理解为系统可用时间的百分比很高，也就是说服务可用的时间很高，数据没有丢失，也没有其他异常。比如，一个未预热的数据库突然承受大流量的冲击，最开始的阶段，响应时间可能会很长，这个时候很难说服务可用。

我们一般用百分比来表示可用性，举例如下。

□ 99.999%的可用性表示全年5分钟故障时间。

□ 99.99%的可用性表示全年1小时故障时间。

□ 99.9%表示全年8小时故障时间。

提高可用性的成本可能会很高，其策略也可能会很复杂，因此我们需要尽可能地平衡“停机”（downtime）成本和减少“停机”时间的成本，衡量系统失败的概率和所造成的损失，对比投入的成本，不要过度设计。

我们可以分解系统的关键部分和非关键部分，这样可以让你更好地设计可用性策略，因为提高一个小系统的可用性会更容易。有时我们为了架构简单，不得不保留“单点”，那么我们可以使用更可靠的硬件和主机来尽量减少风险。

一般影响数据库服务可用性的主要因素是硬件、网络故障或性能问题、软件Bug等。如果使用了读写分离的架构，还可能因为复制延时或复制错误导致从库的数据滞后，数据不一致，这也会对可用性造成影响。生产人员的误操作，比如误删除了数据库文件和数据库表，都可能导致数据库服务的可用性下降。针对如上的影响因素，我们可以制定如下的一些措施，以提高可用性。

1）上线或升级新的软硬件之前，充分做好测试验证。

2）制定合理的备份策略，并定期恢复验证。

3）严格按照流程规范操作数据库，隔离生产环境和测试、开发环境。

4）架构力求简单、可靠，因为复杂的策略可能导致维护和处理问题变得困难，也很难实现高可用策略，记住，解决复杂问题的最好方法就是让复杂的问题不再出现。

5）做好监控，尽量在问题爆发之前就能预警。

6）应回顾和分析故障事件，尽量避免故障的再次发生。

高可用不仅仅是技术的问题，也是管理的问题，有正确的流程、规范和步骤，有良好的文档，有训练有素的维护人员，才可以减少故障发生的概率，并能在故障发生后快速恢复。

## 故障

### **单点故障**

单点故障（SPOF）指某个系统的一部分，如果它停止工作了，将会导致整个系统停止工作。在我们的架构设计中，要尽量避免单点故障。

要避免单点故障，我们首先应找到可能导致整个系统失效的关键的组件，综合评估，在满足我们可用性的要求下，应该如何避免单点故障，或者减少单点故障爆发的可能性。

一般我们靠增加冗余的方式来解决单点故障，冗余的级别和方式不一。从设备的角度，我们可以对主机的单个组件进行冗余，比如使用多个网卡，我们也可以对整个主机所有的关键部件进行冗余，在更高的级别上，我们可以对整个主机进行冗余，或者对整个IDC机房进行冗余。从组织管理的角度，我们还可以对维护数据库的人员进行冗余。MySQL的主从架构本质上也是增加一个冗余的从节点来提高可用性。

如下将详细介绍一些常用的解决单点故障的技术。

1）使用负载均衡软硬件设备，比如对于一组读库，我们可以在前端放置一个负载均衡设备，以解决后端某个从库异常的故障，你可能还需要考虑负载均衡设备自身的高可用性。

2）使用共享存储、网络文件系统、分布式文件系统或复制的磁盘（DRBD）。

传统的数据库产品，如Oracle RAC使用的是基于SAN的共享存储存放数据，数据库的多个实例并发访问共享存储存取数据，应用通过配置在数据库主机上的虚拟IP访问数据库，如果某个数据库主机宕机，其他数据库实例接管虚拟IP，那么应用仍然可以访问到数据库。

MySQL官方介绍了一个实现网络RAID的方案DRBD。也有人使用网络文件系统NFS或分布式文件系统存储共享的数据库文件。

使用共享存储是比较传统的做法，由于成本比较高，且共享存储自身可能也会成为单点，因此互联网架构中很少使用这类方案，有些人为了确保主库数据的安全性，把二进制日志存放到共享存储中，这也是一种可以接受的做法。

使用网络RAID，即DRBD虽然是可行的，但现实中用得并不多，主要原因在于目前的SSD已经足够快了，DRBD自身会成为整个系统的瓶颈，而且会导致主机的浪费，因为只有一半的主机可用。因此作为折中的方案，可以只用DRBD复制二进制日志。

笔者没有使用过NFS存储共享的数据库文件，网络文件系统难以实现高吞吐，NFS更适用的场景是存放一些共享的备份文件。有些人选择使用分布式文件系统来存放数据库文件，由于分布式文件系统本身的复杂性，你需要考虑它的维护成本及团队人员的技能等因素，如果传统的方法可以存放数据文件，那么不建议使用这么一个“笨重”的方案。

3）基于主从复制的数据库切换。

目前MySQL使用最多的高可用方案是MySQL数据库主从切换，也就是说，基于主从复制的冗余。通过对主库增加一个或多个副本（备库），在发生故障的情况下，把生产流量切换到副本上，以确保服务的正常运行。随着主机性能的发展，基于主机之间的高可用是主流也是趋势。

### **MySQL故障切换**

基于数据库复制架构的切换是目前MySQL高可用的主流解决方案。我们把数据库成双成对地设置成主从架构，应用平时只访问主库，如果主库宕机了，从库可以替补使用，且满足一定的条件，那么我们可以把应用的流量切换到从库，使服务仍然可用。

由于数据库切换依赖的是MySQL的主从复制架构，所以你需要深刻了解MySQL的复制原理和机制，确保MySQL的同步一直是可用的。你需要尽可能地保证数据已经同步到了从库，以免丢失数据。

数据库可以配置成主从架构，也可以配置成主主架构。我们建议使用主从架构，这是最稳健、最可靠的方案。有些人把数据库配置成主主架构的原因是，他们认为这样做可以更便于切换及回切。配置成主主架构的时候，你需要小心处理主键冲突等复制问题，在从库上进行操作时需要非常小心，因为错误的操作也会同步到主库。配置成主主架构只是为了方便切换，现实中，仍然需要确保仅有一个主库提供服务，另一个节点可作为备用。

除了单点故障，有时我们也可以为了其他目的进行切换，比如在大表中修改表结构，为了避免影响业务，临时把所有流量切换到从库。这种情况下，配置成主主架构会更方便。

为了简化容量管理，以确保切换数据库流量之后，数据库主机能够正常提供服务，应该确保主备机器的软硬件配置尽量一致。由于数据库从库的数据一般并未“预热”，热点数据也没有被加载到内存，所以在进行流量切换的初始时刻，可能会难以接受其性能，你可以预先运行一些SQL预热数据。

对于写入事务比较多的业务，在发生故障的情况下进行主从切换，可能会丢失数据和导致主从不一致，一般情况下，互联网业务的可用性会高于数据一致性，丢失很少的事务是可以接受的。一些数据也是允许丢失的，比如丢失一些评论是可以接受的，如果需要绝对的不能丢失数据，那么你的方案的实现成本会很高，比如为了确保不丢失主库的日志，你可能需要共享存储来存储主库的日志，还可能需要使用全同步或半同步的技术确保数据的变更已经被传送到了从库。

对于数据库的切换，我们有如下的一些方式。

1）通过修改程序的配置文件实现切换。程序配置文件里有数据库的路由信息，我们可以修改程序的配置文件实现数据库流量的切换，在大多数情况下，我们需要重启应用。比如JAVA服务，默认配置下，我们需要重新启动应用服务。在服务非常多的情况下，也有把数据库配置信息存储在数据库中的。

2）修改内网DNS。

我们可以在生产环境中配置内网DNS，通过修改内网DNS指向的数据库服务器的IP，实现主库在故障情况下的切换，这种方式，往往也需要重启应用服务。由于内网DNS可能不归属于DBA团队掌控，DNS服务器的维护和高可用也需要成本，而且更改内网DNS也需要时间，所以这种方式用得比较少。

3）修改主机的hosts文件。

/etc/hosts里可以配置与数据库服务器的域名对应的IP，但是还不够理想。而且在有很多应用服务器的时候，维护一份统一的hosts文件的成本也会比较高。

4）一些能够实现高可用的工具集，如MHA、MMM，它们用于监控数据库节点的存活状况，通过修改配置文件或漂移主库IP的方式来实现数据库的高可用。MMM通过漂移虚拟IP的方式处理单点故障，但许多生产实践证明，其作为一套自动切换方案并不是很可靠，如果需要使用，建议只使用手动切换的功能。MHA是Perl编写的一套MySQL故障切换工具，支持通过修改全局配置和漂移虚拟IP两种方式处理单点故障，已经在许多生产环境中得到了验证，是值得考虑的方案。

你也可以自己编写脚本监控数据库节点的可用性，漂移虚拟机IP实现切换，需要留意的是，漂移IP的方式存在一个缺陷，其严重依赖硬件的可靠性，需要主机、网络设备的配合工作。在生产环境中，可能会因为网络硬件的原因导致虚拟IP不能正常漂移。

5）对于大规模的数据库集群，需要更智能地处理单点切换，应该尽量不依赖自己无法控制的因素，我们可以使用独立的Proxy代理的方式实现单点切换。所有的流量都经过Proxy，Proxy智能地处理后端的数据库主节点宕机故障，需要留意的是，你还需要处理好Proxy自身的高可用性。实现Proxy的成本很高，一些互联网公司已经有自己成熟的数据库Proxy。理论上，Proxy是可以代理本地IDC的流量的，也可以代理其他IDC的数据库流量，但由于网络延时和安全的考虑，一般建议仅代理本地IDC的流量。如果需要配置跨IDC的数据库切换，更可靠的方案是，在应用层切换流量，也就是说，让用户去访问正常IDC的应用服务器。

6）通过客户端、框架配合实现单点切换，相对于使用Proxy的方式，这种方式更轻量级。

## 架构

目前MySQL高可用方案有很多，几种典型的高可用架构选型有:

1、主从或主主半同步复制：通过依赖MySQL本身的复制，Master制作一个或多个热副本，在Master故障时，将服务切换到热副本从而达到高可用的效果。

2、MHA+多节点集群：基于MHA的集群方案，通常和其他第三方方案组合实现

3、分布式协议：基于分布式协议的高可用方案，常见的有Galera Cluster，PXC和MGR

4、基于共享存储方案：如SAN存储，这种方案可以实现网络中不同服务器的数据共享，共享存储能够为数据库服务器和存储解耦。

5、基于磁盘复制方案：如DRBD，DRDB是一个以linux内核模块方式实现的块级别同步复制技术。它通过网卡将主服务器的每个块复制到另外一个服务器块设备上，并在主设备提交块之前记录下来。类似共享存储解决方案。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC84B.tmp.png) 

 

## 选型

目前MySQL高可用方案有很多，几种典型的高可用架构选型有:

注：两个主流的架构：MMM架构和MHA架构。

### **主从或主主半同步复制**

通过依赖MySQL本身的复制，Master制作一个或多个热副本，在Master故障时，将服务切换到热副本从而达到高可用的效果。

注：这种可能会出现数据不一致的问题。

 

### **MMM**

MMM（Master-Master replication manager for MySQL）是一套支持***\*双主故障切换和双主日常管理\****的脚本程序。MMM采用Perl语言开发，主要用来监控和管理MySQL Master-Master（双主）复制，虽然叫做双主复制，但是***\*业务上同一时刻只允许对一个主进行写操作，另一台备选主上提供部分读服务，以加速在主主切换时备选主的预热\****。可以说MMM这套脚本程序一方面实现了故障切换的功能，另一方面其内部附加的工具脚本也可以实现多个slave的read负载均衡。

​	由于***\*MMM无法完全地保证数据一致性，所以MMM适用于对数据一致性要求不是很高，但是又想最大程度的保证业务可用性的场景\*******\*。对于那些对数据的一致性一致性要求很高的业务，非常不建议采用MMM这种高可用架构\****。

架构图如下：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC84C.tmp.jpg) 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC84D.tmp.jpg) 

​	假设存在Master1（db1），Master2（db2），Slave（db3）,db1、db2、db3之间为一主两从的复制关系，一旦发生db2、db3延时于db1时，这个时刻db1 MySQL宕机，db3将会等待数据追上db1后，再重新指向新的主db2，进行change master to db2操作，在db1宕机的过程中，一旦db2落后于db1，这时发生切换，db2变成可写状态，数据的一致性将无法保证（即slave节点db3追上原来主节点db1，然后指向新的主节点db2，此时数据不一致）。

注：这里MMM架构，其实本质上还是主从的架构，只不过是在主（正常情况下接收写操作）宕机的时候，另外一个主（正常情况下只会接收读操作）会变为主，但是从节点需要追上原来的主节点后才可以指向新的主节点（这个就不能保证完全一致性和实时性了）。

### **MHA+多节点集群**

基于MHA的集群方案，通常和其他第三方方案组合实现。

 

无论是高可用计算架构，还是高可用存储架构，其本质的设计目的都是为了解决部分服务器故障的场景下，如何保证系统能够继续提供服务。但在一些极端场景下，有可能所有服务器都出现故障。例如，典型的有机房断电、机房火灾、地震、水灾……这些极端情况会导致某个系统所有服务器都故障，或者业务整体瘫痪，而且即使有其他地区的备份，把备份业务系统全部恢复到能够正常提供业务，花费的时间也比较长，可能是半小时，也可能是12小时。因为备份系统平时不对外提供服务，可能会存在很多隐藏的问题没有发现。如果业务期望达到即使在此类灾难性故障的情况下，业务也不受影响，或者在几分钟内就能够很快恢复，那么就需要设计异地多活架构。

 

#### **概述**

MHA架构和MMM架构有什么区别呢？最大的区别在于：***\*MHA会把丢失的数据在每个save节点上补齐。\****

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC85D.tmp.jpg) 

从图7-1可以看到，当master宕机时，MHA管理机会试图scp丢失的那一部分binlog，然后把该binlog拷贝到最新的slave机器上，再补齐差异的binlog并应用。当最新的slave补齐数据后，再把它的relay-log拷贝到其他的slave上，以识别差异并应用。至此，整个恢复过程结束，从而保证切换后的数据是一致的。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC85E.tmp.jpg) 

 

MHA（Master High Availability）是一套优秀的作为MySQL高可用环境下故障切换和主从提升的高可用软件。在MySQL故障切换过程中，***\*MHA能做到在0~\*******\*30\*******\*秒之内自动完成数据库的故障切换操作，并且在进行故障切换过程中，MHA能在最大程度上保证数据的一致性，以达到真正意义上的高可用\****。

​	它由两部分组成：MHA Manager（管理节点）和MHA Node（数据节点）。MHA Manager可以单独部署在一台独立的机器上管理多个master-slave集群，也可以部署在一台slave上。MHA Node运行在每台MySQL服务器上，MHA Manager会定时探测集群中的master节点，当master出现故障时，它可以自动将**最新数据**的slave提升为新的master（这样就避免数据不一致），然后将所有其他的slave重新指向新的master。整个故障转移过程对应用程序是完全透明的。

注：这里直接将最新的slave节点设置为新的主节点，与MMM架构中从节点追上旧的主节点然后指向新的主节点的方式不同，这个不需要追上原来的主节点，直接使用这个最新的从节点的数据。

​	在MHA自动故障切换过程中，MHA试图从宕机的主服务器上保存二进制日志，最大程度地保证数据不丢失，但这并不总是可行的。例如，主服务器硬件故障或无法通过ssh访问，MHA无法保存二进制日志，只进行故障转移而丢失了最新数据。使用MySQL5.5的**半同步复制**，可以大大降低数据丢失的风险。如果只有一个slave已经收到了最新的二进制日志，MHA可以将最新的二进制日志应用于其他所有的slave服务器上，因此它们彼此保持一致性。

#### **原理**

​	目前MHA主要支持一主多从的架构，要搭建MHA，要求一个复制集群中必须***\*最少有三台数据库服务器\****，一主二从，即一台充当master，一台充当备用master，另一台充当slave。	MHA的工作原理：

1、从宕机崩溃的master保存二进制日志事件（binlog events）；

2、识别含有最新更新的slave；

3、应用差异的中继日志（relay log）到其他slave；

4、应用从master保存的二进制日志事件（binlog events）；

5、提升一个slave为新master；

6、使其他的slave连接新的master进行复制。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC85F.tmp.jpg) 

当master宕机时，MHA管理机会试图scp丢失的那一部分binlog，然后把该binlog拷贝到最新的slave机器上，再补齐差异的binlog并应用。当最新的slave补齐数据后，再把它的relay-log拷贝到其他的slave上，以识别差异并应用。至此，整个恢复过程结束，从而保证切换后的数据是一致的。

恢复过程如下：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC860.tmp.jpg) 

#### **故障转移**

MHA提供了三种故障转移模式，下面分别说明各自的使用场景。

***\*自动故障转移\****

在当前已存在的主从复制环境中，MHA可以监控master主机故障，并且故障会自动转移。即使有些slave没有接收新的relay log events, ***\*MHA也会从\*******\*最新的slave\*******\*自动识别差异的relay log events，并应用差异的event事件到其他slaves\****。因此所有的slave都是一致的。

如果采用自动切换模式，则***\*需要开启半同步复制（semi replication）\****，以确保slave已经接收到master的binlog，因为master宕机，MHA管理机有可能无法远程拷贝scp那一缺失的binlog，那么数据就会出现不一致。

 

***\*手动故障转移\****

MHA可以用来只做故障转移，而不监测master, MHA只作为故障转移的交互。

如果采用默认的***\*异步复制模式\****，由于master的宕机无法得知binlog是否全部发送到slave上，则此时需要等待master的崩溃恢复完成，以便把未发送的binlog同步到slave上。如果这时采用自动切换模式，那么数据就会不一致。

只有在机器长时间启动不了（如硬件主板损坏）或者崩溃恢复时间很长已严重影响业务，并且不在意那一部分丢失的数据的情况下，才可以人工介入手工处理master故障转移。

 

***\*在线平滑切换\****

如果需要机器的维护，则将master在线切换到其他主机上（例如更换原master坏掉的硬盘），这并不是master崩溃引起的故障转移。***\*在线切换通常需要0.5～2秒，并且会阻塞写（会执行FLUSHTABLES WITH READ LOCK命令加全局读锁）操作，建议在凌晨业务低峰期执行在线切换\****。

 

#### **安装配置**

 

#### **应用场景**

顾名思义，异地多活架构的关键点就是异地、多活，其中异地就是指地理位置上不同的地方，类似于“不要把鸡蛋都放在同一篮子里”；多活就是指不同地理位置上的系统都能够提供业务服务，这里的“活”是活动、活跃的意思。判断一个系统是否符合异地多活，需要满足两个标准：

正常情况下，用户无论访问哪一个地点的业务系统，都能够得到正确的业务服务。

某个地方业务异常的时候，用户访问其他地方正常的业务系统，能够得到正确的业务服务。

与“活”对应的是字是“备”，备是备份，正常情况下对外是不提供服务的，如果需要提供服务，则需要大量的人工干预和操作，花费大量的时间才能让“备”变成“活”。

单纯从异地多活的描述来看，异地多活很强大，能够保证在灾难的情况下业务都不受影响。那是不是意味着不管什么业务，我们都要去实现异地多活架构呢？其实不然，因为实现异地多活架构不是没有代价的，相反其代价很高，具体表现为：

系统复杂度会发生质的变化，需要设计复杂的异地多活架构。

成本会上升，毕竟要多在一个或者多个机房搭建独立的一套业务系统。

因此，异地多活虽然功能很强大，但也不是每个业务不管三七二十一都要上异地多活。例如，常见的新闻网站、企业内部的IT系统、游戏、博客站点等，如果无法承受异地多活带来的复杂度和成本，是可以不做异地多活的，只需要做异地备份即可。因为这类业务系统即使中断，对用户的影响并不会很大，例如，A新闻网站看不了，用户换个新闻网站即可。而共享单车、滴滴出行、支付宝、微信这类业务，就需要做异地多活了，这类业务系统中断后，对用户的影响很大。例如，支付宝用不了，就没法买东西了；滴滴用不了，用户就打不到车了。

当然，如果业务规模很大，能够做异地多活的情况下还是尽量。首先，这样能够在异常的场景下给用户提供更好的体验；其次，业务规模很大肯定会伴随衍生的收入，例如广告收入，异地多活能够减少异常场景带来的收入损失。同样以新闻网站为例，虽然从业务的角度来看，新闻类网站对用户影响不大，反正用户也可以从其他地方看到基本相同的新闻，甚至用户几个小时不看新闻也没什么问题。但是从网站本身来看，几个小时不可访问肯定会影响用户对网站的口碑；其次几个小时不可访问，网站上的广告收入损失也会很大。

 

#### **架构模式**

根据地理位置上的距离来划分，异地多活架构可以分为同城异区、跨城异地、跨国异地。接下来我详细解释一下每一种架构的细节与优缺点。

 

***\*同城异区\****

同城异区指的是将业务部署在同一个城市不同区的多个机房。例如，在北京部署两个机房，一个机房在海淀区，一个在通州区，然后将两个机房用专用的高速网络连接在一起。

如果我们考虑一些极端场景（例如，美加大停电、新奥尔良水灾），同城异区似乎没什么作用，那为何我们还要设计同城异区这种架构呢？答案就在于“同城”。

同城的两个机房，距离上一般大约就是几十千米，通过搭建高速的网络，同城异区的两个机房能够实现和同一个机房内几乎一样的网络传输速度。这就意味着虽然是两个不同地理位置上的机房，但逻辑上我们可以将它们看作同一个机房，这样的设计大大降低了复杂度，减少了异地多活的设计和实现复杂度及成本。

那如果采用了同城异区架构，一旦发生新奥尔良水灾这种灾难怎么办呢？很遗憾，答案是无能为力。但我们需要考虑的是，这种极端灾难发生概率是比较低的，可能几年或者十几年才发生一次。其次，除了这类灾难，机房火灾、机房停电、机房空调故障这类问题发生的概率更高，而且破坏力一样很大。而这些故障场景，同城异区架构都可以很好地解决。因此，结合复杂度、成本、故障发生概率来综合考虑，同城异区是应对机房级别故障的最优架构。

 

***\*跨城异地\****

跨城异地指的是业务部署在不同城市的多个机房，而且距离最好要远一些。例如，将业务部署在北京和广州两个机房，而不是将业务部署在广州和深圳的两个机房。

为何跨城异地要强调距离要远呢？前面我在介绍同城异区的架构时提到同城异区不能解决新奥尔良水灾这种问题，而两个城市离得太近又无法应对如美加大停电这种问题，跨城异地其实就是为了解决这两类问题的，因此需要在距离上比较远，才能有效应对这类极端灾难事件。

跨城异地虽然能够有效应对极端灾难事件，但“距离较远”这点并不只是一个距离数字上的变化，而是量变引起了质变，导致了跨城异地的架构复杂度大大上升。距离增加带来的最主要问题是两个机房的网络传输速度会降低，这不是以人的意志为转移的，而是物理定律决定的，即光速真空传播大约是每秒30万千米，在光纤中传输的速度大约是每秒20万千米，再加上传输中的各种网络设备的处理，实际还远远达不到理论上的速度。

除了距离上的限制，中间传输各种不可控的因素也非常多。例如，挖掘机把光纤挖断、中美海底电缆被拖船扯断、骨干网故障等，这些线路很多是第三方维护，针对故障我们根本无能为力也无法预知。例如，广州机房到北京机房，正常情况下RTT大约是50毫秒左右，遇到网络波动之类的情况，RTT可能飙升到500毫秒甚至1秒，更不用说经常发生的线路丢包问题，那延迟可能就是几秒几十秒了。

以上描述的问题，虽然同城异区理论上也会遇到，但由于同城异区距离较短，中间经过的线路和设备较少，问题发生的概率会低很多。而且同城异区距离短，即使是搭建多条互联通道，成本也不会太高，而跨城异区距离太远，搭建或者使用多通道的成本会高不少。

跨城异地距离较远带来的网络传输延迟问题，给异地多活架构设计带来了复杂性，如果要做到真正意义上的多活，业务系统需要考虑部署在不同地点的两个机房，在数据短时间不一致的情况下，还能够正常提供业务。这就引入了一个看似矛盾的地方：数据不一致业务肯定不会正常，但跨城异地肯定会导致数据不一致。

如何解决这个问题呢？重点还是在“数据”上，即根据数据的特性来做不同的架构。如果是强一致性要求的数据，例如银行存款余额、支付宝余额等，这类数据实际上是无法做到跨城异地多活的。我们来看一个假设的例子，假如我们做一个互联网金融的业务，用户余额支持跨城异地多活，我们的系统分别部署在广州和北京，那么如果挖掘机挖断光缆后，会出现如下场景：

用户A余额有10000元钱，北京和广州机房都是这个数据。

用户A向用户B转了5000元钱，这个操作是在广州机房完成的，完成后用户A在广州机房的余额是5000元。

由于广州和北京机房网络被挖掘机挖断，广州机房无法将余额变动通知北京机房，此时北京机房用户A的余额还是10000元。

用户A到北京机房又发起转账，此时他看到自己的余额还有10000元，于是向用户C转账10000元，转账完成后用户A的余额变为0。

用户A到广州机房一看，余额怎么还有5000元？于是赶紧又发起转账，转账5000元给用户D；此时广州机房用户A的余额也变为0了。

最终，本来余额10000元的用户A，却转了20000元出去给其他用户。

对于以上这种假设场景，虽然普通用户很难这样自如地操作，但如果真的这么做，被黑客发现后，后果不堪设想。正因为如此，支付宝等金融相关的系统，对余额这类数据，一般不会做跨城异地的多活架构，而只能采用同城异区这种架构。

而对数据一致性要求不那么高，或者数据不怎么改变，或者即使数据丢失影响也不大的业务，跨城异地多活就能够派上用场了。例如，用户登录（数据不一致时用户重新登录即可）、新闻类网站（一天内的新闻数据变化较少）、微博类网站（丢失用户发布的微博或者评论影响不大），这些业务采用跨城异地多活，能够很好地应对极端灾难的场景。

 

***\*跨国异地\****

跨国异地指的是业务部署在不同国家的多个机房。相比跨城异地，跨国异地的距离就更远了，因此数据同步的延时会更长，正常情况下可能就有几秒钟了。这种程度的延迟已经无法满足异地多活标准的第一条：“正常情况下，用户无论访问哪一个地点的业务系统，都能够得到正确的业务服务”。例如，假设有一个微博类网站，分别在中国的上海和美国的纽约都建了机房，用户A在上海机房发表了一篇微博，此时如果他的一个关注者B用户访问到美国的机房，很可能无法看到用户A刚刚发表的微博。虽然跨城异地也会有此类同步延时问题，但正常情况下几十毫秒的延时对用户来说基本无感知的；而延时达到几秒钟就感觉比较明显了。

因此，跨国异地的“多活”，和跨城异地的“多活”，实际的含义并不完全一致。跨国异地多活的主要应用场景一般有这几种情况：

 

为不同地区用户提供服务

例如，亚马逊中国是为中国用户服务的，而亚马逊美国是为美国用户服务的，亚马逊中国的用户如果访问美国亚马逊，是无法用亚马逊中国的账号登录美国亚马逊的。

 

只读类业务做多活

例如，谷歌的搜索业务，由于用户搜索资料时，这些资料都已经存在于谷歌的搜索引擎上面，无论是访问英国谷歌，还是访问美国谷歌，搜索结果基本相同，并且对用户来说，也不需要搜索到最新的实时资料，跨国异地的几秒钟网络延迟，对搜索结果是没有什么影响的。

 

#### **注意事项**

1）防止网络抖动误切换（脑裂）造成数据不一致

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC871.tmp.jpg) 

2）VIP没有采用Keepalived，就是怕存在网络抖动问题，如果出现脑裂，那么从库也会抢夺VIP，由于主库和从库都持有VIP，因此会造成IP冲突，影响业务。Keepalived只能实现一个节点监控master，而通过自带的脚本可以实现两个节点监控master。

3）mha依赖ssh, ssh有时会出现连接慢或者不通的情况，如何解决？设置ssh连接超时时间(sshtimeout)

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC872.tmp.jpg) 

从图7-5和图7-6中可以看出，缩短了连接超时的时间，加快了故障切换的速度。

4）死掉的原master如何与提升为新的master建立同步复制关系？

把下面这段语句复制下来，待死掉的原master恢复好后，执行该语句，即可与新提升的master建立同步复制关系。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC873.tmp.jpg) 

5）MHA结合半同步复制（semi replication）时应注意些什么？

互联网金融公司，数据不丢失是第一位的。开启半同步复制势必会影响性能，所以针对这种情况，将DB服务器通过VLAN划分为一个独立网段，与应用相隔离，保证有一个良好的网络环境。此外，由于MHA是基于ssh公私钥认证的，有些公司禁止ssh互通，以避免黑客入侵后，又跳到其他DB服务器里，所以将DB与应用隔离是必要的。

6）MHA 0.56最新版本不支持MariaDB 10的GTID复制，仅支持甲骨文MySQL的GTID复制。

#### **故障演练**

***\*场景一：master自动监控和故障转移\****

***\*场景二：master手工故障转移\****

***\*场景三：在线平滑切换\****

#### **跨城架构设计技巧**

每个架构的关键点：

同城异区

关键在于搭建高速网络将两个机房连接起来，达到近似一个本地机房的效果。架构设计上可以将两个机房当作本地机房来设计，无须额外考虑。

 

跨城异地

关键在于数据不一致的情况下，业务不受影响或者影响很小，这从逻辑的角度上来说其实是矛盾的，架构设计的主要目的就是为了解决这个矛盾。

 

跨国异地

主要是面向不同地区用户提供业务，或者提供只读业务，对架构设计要求不高。

基于这个分析，跨城异地多活是架构设计复杂度最高的一种，接下来将介绍跨城异地多活架构设计的一些技巧

 

***\*技巧1：保证核心业务的异地多活\****

“异地多活”是为了保证业务的高可用，但很多架构师在考虑这个“业务”时，会不自觉地陷入一个思维误区：我要保证所有业务都能“异地多活”！

 

假设我们需要做一个“用户子系统”，这个子系统负责“注册”“登录”“用户信息”三个业务。为了支持海量用户，我们设计了一个“用户分区”的架构，即正常情况下用户属于某个主分区，每个分区都有其他数据的备份，用户用邮箱或者手机号注册，路由层拿到邮箱或者手机号后，通过Hash计算属于哪个中心，然后请求对应的业务中心。基本的架构如下：

 

这样一个系统，如果3个业务要同时实现异地多活，会发现这些难以解决的问题：

注册问题

A中心注册了用户，数据还未同步到B中心，此时A中心宕机，为了支持注册业务多活，可以挑选B中心让用户去重新注册。看起来很容易就支持多活了，但仔细思考一下会发现这样做会有问题：一个手机号只能注册一个账号，A中心的数据没有同步过来，B中心无法判断这个手机号是否重复，如果B中心让用户注册，后来A中心恢复了，发现数据有冲突，怎么解决？实际上是无法解决的，因为同一个手机号注册的账号不能以后一次注册为准；而如果B中心不支持本来属于A中心的业务进行注册，注册业务的多活又成了空谈。

 

如果我们修改业务规则，允许一个手机号注册多个账号不就可以了吗？

这样做是不可行的，类似一个手机号只能注册一个账号这种规则，是核心业务规则，修改核心业务规则的代价非常大，几乎所有的业务都要重新设计，为了架构设计去改变业务规则（而且是这么核心的业务规则）是得不偿失的。

 

用户信息问题

 

用户信息的修改和注册有类似的问题，即A、B两个中心在异常的情况下都修改了用户信息，如何处理冲突？

 

由于用户信息并没有账号那么关键，一种简单的处理方式是按照时间合并，即最后修改的生效。业务逻辑上没问题，但实际操作也有一个很关键的“坑”：怎么保证多个中心所有机器时间绝对一致？在异地多中心的网络下，这个是无法保证的，即使有时间同步也无法完全保证，只要两个中心的时间误差超过1秒，数据就可能出现混乱，即先修改的反而生效。

 

还有一种方式是生成全局唯一递增ID，这个方案的成本很高，因为这个全局唯一递增ID的系统本身又要考虑异地多活，同样涉及数据一致性和冲突的问题。

 

综合上面的简单分析可以发现，如果“注册”“登录”“用户信息”全部都要支持异地多活，实际上是挺难的，有的问题甚至是无解的。那这种情况下我们应该如何考虑“异地多活”的架构设计呢？答案其实很简单：优先实现核心业务的异地多活架构！

 

对于这个模拟案例来说，“登录”才是最核心的业务，“注册”和“用户信息”虽然也是主要业务，但并不一定要实现异地多活，主要原因在于业务影响不同。对于一个日活1000万的业务来说，每天注册用户可能是几万，修改用户信息的可能还不到1万，但登录用户是1000万，很明显我们应该保证登录的异地多活。

 

对于新用户来说，注册不了的影响并不明显，因为他还没有真正开始使用业务。用户信息修改也类似，暂时修改不了用户信息，对于其业务不会有很大影响。而如果有几百万用户登录不了，就相当于几百万用户无法使用业务，对业务的影响就非常大了：公司的客服热线很快就被打爆，微博、微信上到处都在传业务宕机，论坛里面到处是抱怨的用户，那就是互联网大事件了！

 

而登录实现“异地多活”恰恰是最简单的，因为每个中心都有所有用户的账号和密码信息，用户在哪个中心都可以登录。用户在A中心登录，A中心宕机后，用户到B中心重新登录即可。

 

如果某个用户在A中心修改了密码，此时数据还没有同步到B中心，用户到B中心登录是无法登录的，这个怎么处理？这个问题其实就涉及另外一个设计技巧了，稍后再谈。

 

***\*技巧2：保证核心数据最终一致性\****

 

异地多活本质上是通过异地的数据冗余，来保证在极端异常的情况下业务也能够正常提供给用户，因此数据同步是异地多活架构设计的核心。但大部分架构师在考虑数据同步方案时，会不知不觉地陷入完美主义误区：我要所有数据都实时同步！

 

数据冗余是要将数据从A地同步到B地，从业务的角度来看是越快越好，最好和本地机房一样的速度最好。但让人头疼的问题正在这里：异地多活理论上就不可能很快，因为这是物理定律决定的。

 

因此异地多活架构面临一个无法彻底解决的矛盾：业务上要求数据快速同步，物理上正好做不到数据快速同步，因此所有数据都实时同步，实际上是一个无法达到的目标。

 

既然是无法彻底解决的矛盾，那就只能想办法尽量减少影响。有几种方法可以参考：

 

尽量减少异地多活机房的距离，搭建高速网络

 

和同城异区架构类似，但搭建跨城异地的高速网络成本远远超过同城异区的高速网络，成本巨大，一般只有巨头公司才能承担。

 

尽量减少数据同步，只同步核心业务相关的数据

 

简单来说就是不重要的数据不同步，同步后没用的数据不同步，只同步核心业务相关的数据。

 

以前面的“用户子系统”为例，用户登录所产生的token或者session信息，数据量很大，但其实并不需要同步到其他业务中心，因为这些数据丢失后重新登录就可以再次获取了。

 

这时你可能会想到：这些数据丢失后要求用户重新登录，影响用户体验！

 

确实如此，毕竟需要用户重新输入账户和密码信息，或者至少要弹出登录界面让用户点击一次，但相比为了同步所有数据带来的代价，这个影响完全可以接受。为什么这么说呢，还是卖个关子我会在后面分析。

 

保证最终一致性，不保证实时一致性

 

最终一致性就是BASE理论，即业务不依赖数据同步的实时性，只要数据最终能一致即可。例如，A机房注册了一个用户，业务上不要求能够在50毫秒内就同步到所有机房，正常情况下要求5分钟同步到所有机房即可，异常情况下甚至可以允许1小时或者1天后能够一致。

 

最终一致性在具体实现时，还需要根据不同的数据特征，进行差异化的处理，以满足业务需要。例如，对“账号”信息来说，如果在A机房新注册的用户5分钟内正好跑到B机房了，此时B机房还没有这个用户的信息，为了保证业务的正确，B机房就需要根据路由规则到A机房请求数据。

 

而对“用户信息”来说，5分钟后同步也没有问题，也不需要采取其他措施来弥补，但还是会影响用户体验，即用户看到了旧的用户信息，这个问题怎么解决呢？好像又是一个解决不了的问题，在最后会给出答案。

 

***\*技巧3：采用多种手段同步数据\****

 

数据同步是异地多活架构设计的核心，幸运的是基本上存储系统本身都会有同步的功能。例如，MySQL的主备复制、Redis的Cluster功能、Elasticsearch的集群功能。这些系统本身的同步功能已经比较强大，能够直接拿来就用，但这也无形中将我们引入了一个思维误区：只使用存储系统的同步功能！

 

既然说存储系统本身就有同步功能，而且同步功能还很强大，为何说只使用存储系统是一个思维误区呢？因为虽然绝大部分场景下，存储系统本身的同步功能基本上也够用了，但在某些比较极端的情况下，存储系统本身的同步功能可能难以满足业务需求。

 

以MySQL为例，MySQL 5.1版本的复制是单线程的复制，在网络抖动或者大量数据同步时，经常发生延迟较长的问题，短则延迟十几秒，长则可能达到十几分钟。而且即使我们通过监控的手段知道了MySQL同步时延较长，也难以采取什么措施，只能干等。

 

Redis又是另外一个问题，Redis 3.0之前没有Cluster功能，只有主从复制功能，而为了设计上的简单，Redis 2.8之前的版本，主从复制有一个比较大的隐患：从机宕机或者和主机断开连接都需要重新连接主机，重新连接主机都会触发全量的主从复制。这时主机会生成内存快照，主机依然可以对外提供服务，但是作为读的从机，就无法提供对外服务了，如果数据量大，恢复的时间会相当长。

 

综合上面的案例可以看出，存储系统本身自带的同步功能，在某些场景下是无法满足业务需要的。尤其是异地多机房这种部署，各种各样的异常情况都可能出现，当我们只考虑存储系统本身的同步功能时，就会发现无法做到真正的异地多活。

 

解决的方案就是拓开思路，避免只使用存储系统的同步功能，可以将多种手段配合存储系统的同步来使用，甚至可以不采用存储系统的同步方案，改用自己的同步方案。

 

还是以前面的“用户子系统”为例，我们可以采用如下几种方式同步数据：

 

消息队列方式

 

对于账号数据，由于账号只会创建，不会修改和删除（假设我们不提供删除功能），我们可以将账号数据通过消息队列同步到其他业务中心。

 

二次读取方式

 

某些情况下可能出现消息队列同步也延迟了，用户在A中心注册，然后访问B中心的业务，此时B中心本地拿不到用户的账号数据。为了解决这个问题，B中心在读取本地数据失败时，可以根据路由规则，再去A中心访问一次（这就是所谓的二次读取，第一次读取本地，本地失败后第二次读取对端），这样就能够解决异常情况下同步延迟的问题。

 

存储系统同步方式

 

对于密码数据，由于用户改密码频率较低，而且用户不可能在1秒内连续改多次密码，所以通过数据库的同步机制将数据复制到其他业务中心即可，用户信息数据和密码类似。

 

回源读取方式

 

对于登录的session数据，由于数据量很大，我们可以不同步数据；但当用户在A中心登录后，然后又在B中心登录，B中心拿到用户上传的session id后，根据路由判断session属于A中心，直接去A中心请求session数据即可；反之亦然，A中心也可以到B中心去获取session数据。

 

重新生成数据方式

 

对于“回源读取”场景，如果异常情况下，A中心宕机了，B中心请求session数据失败，此时就只能登录失败，让用户重新在B中心登录，生成新的session数据。

 

注意：以上方案仅仅是示意，实际的设计方案要比这个复杂一些，还有很多细节要考虑。

 

综合上述的各种措施，最后“用户子系统”同步方式整体如下：

 

 

 

***\*技巧4：只保证绝大部分用户的异地多活\****

 

前面在给出每个思维误区对应的解决方案时，留下了几个小尾巴：某些场景下我们无法保证100%的业务可用性，总是会有一定的损失。例如，密码不同步导致无法登录、用户信息不同步导致用户看到旧的信息等，这个问题怎么解决呢？

其实这个问题涉及异地多活架构设计中一个典型的思维误区：我要保证业务100%可用！但极端情况下就是会丢一部分数据，就是会有一部分数据不能同步，有没有什么巧妙能做到100%可用呢？

很遗憾，答案是没有！异地多活也无法保证100%的业务可用，这是由物理规律决定的，光速和网络的传播速度、硬盘的读写速度、极端异常情况的不可控等，都是无法100%解决的。所以针对这个思维误区，我的答案是“忍”！也就是说我们要忍受这一小部分用户或者业务上的损失，否则本来想为了保证最后的0.01%的用户的可用性，做一个完美方案，结果却发现99.99%的用户都保证不了了。

对于某些实时强一致性的业务，实际上受影响的用户会更多，甚至可能达到1/3的用户。以银行转账这个业务为例，假设小明在北京XX银行开了账号，如果小明要转账，一定要北京的银行业务中心才可用，否则就不允许小明自己转账。如果不这样的话，假设在北京和上海两个业务中心实现了实时转账的异地多活，某些异常情况下就可能出现小明只有1万元存款，他在北京转给了张三1万元，然后又到上海转给了李四1万元，两次转账都成功了。这种漏洞如果被人利用，后果不堪设想。

当然，针对银行转账这个业务，虽然无法做到“实时转账”的异地多活，但可以通过特殊的业务手段让转账业务也能实现异地多活。例如，转账业务除了“实时转账”外，还提供“转账申请”业务，即小明在上海业务中心提交转账请求，但上海的业务中心并不立即转账，而是记录这个转账请求，然后后台异步发起真正的转账操作，如果此时北京业务中心不可用，转账请求就可以继续等待重试；假设等待2个小时后北京业务中心恢复了，此时上海业务中心去请求转账，发现余额不够，这个转账请求就失败了。小明再登录上来就会看到转账申请失败，原因是“余额不足”。

不过需要注意的是“转账申请”的这种方式虽然有助于实现异地多活，但其实还是牺牲了用户体验的，对于小明来说，本来一次操作的事情，需要分为两次：一次提交转账申请，另外一次是要确认是否转账成功。

虽然我们无法做到100%可用性，但并不意味着我们什么都不能做，为了让用户心里更好受一些，我们可以采取一些措施进行安抚或者补偿，例如：

 

挂公告

说明现在有问题和基本的问题原因，如果不明确原因或者不方便说出原因，可以发布“技术哥哥正在紧急处理”这类比较轻松和有趣的公告。

事后对用户进行补偿

例如，送一些业务上可用的代金券、小礼包等，减少用户的抱怨。

补充体验

对于为了做异地多活而带来的体验损失，可以想一些方法减少或者规避。以“转账申请”为例，为了让用户不用确认转账申请是否成功，我们可以在转账成功或者失败后直接给用户发个短信，告诉他转账结果，这样用户就不用时不时地登录系统来确认转账是否成功了。

核心思想

异地多活设计的理念可以总结为一句话：采用多种手段，保证绝大部分用户的核心业务异地多活！

 

跨城架构设计步骤

第1步：业务分级

按照一定的标准将业务进行分级，挑选出核心的业务，只为核心业务设计异地多活，降低方案整体复杂度和实现成本。

 

常见的分级标准有下面几种：

访问量大的业务

以用户管理系统为例，业务包括登录、注册、用户信息管理，其中登录的访问量肯定是最大的。

核心业务

以QQ为例，QQ的主场景是聊天，QQ空间虽然也是重要业务，但和聊天相比，重要性就会低一些，如果要从聊天和QQ空间两个业务里面挑选一个做异地多活，那明显聊天要更重要（当然，此类公司如腾讯，应该是两个都实现了异地多活的）。

 

产生大量收入的业务

同样以QQ为例，聊天可能很难为腾讯带来收益，因为聊天没法插入广告；而QQ空间反而可能带来更多收益，因为QQ空间可以插入很多广告，因此如果从收入的角度来看，QQ空间做异地多活的优先级反而高于QQ聊天了。

 

以我们一直在举例的用户管理系统为例，“登录”业务符合“访问量大的业务”和“核心业务”这两条标准，因此我们将登录业务作为核心业务。

 

第2步：数据分类

 

挑选出核心业务后，需要对核心业务相关的数据进一步分析，目的在于识别所有的数据及数据特征，这些数据特征会影响后面的方案设计。

 

常见的数据特征分析维度有：

 

数据量

 

这里的数据量包括总的数据量和新增、修改、删除的量。对异地多活架构来说，新增、修改、删除的数据就是可能要同步的数据，数据量越大，同步延迟的几率越高，同步方案需要考虑相应的解决方案。

 

唯一性

 

唯一性指数据是否要求多个异地机房产生的同类数据必须保证唯一。例如用户ID，如果两个机房的两个不同用户注册后生成了一样的用户ID，这样业务上就出错了。

 

数据的唯一性影响业务的多活设计，如果数据不需要唯一，那就说明两个地方都产生同类数据是可能的；如果数据要求必须唯一，要么只能一个中心点产生数据，要么需要设计一个数据唯一生成的算法。

 

实时性

 

实时性指如果在A机房修改了数据，要求多长时间必须同步到B机房，实时性要求越高，对同步的要求越高，方案越复杂。

 

可丢失性

 

可丢失性指数据是否可以丢失。例如，写入A机房的数据还没有同步到B机房，此时A机房机器宕机会导致数据丢失，那这部分丢失的数据是否对业务会产生重大影响。

 

例如，登录过程中产生的session数据就是可丢失的，因为用户只要重新登录就可以生成新的session；而用户ID数据是不可丢失的，丢失后用户就会失去所有和用户ID相关的数据，例如用户的好友、用户的钱等。

 

可恢复性

 

可恢复性指数据丢失后，是否可以通过某种手段进行恢复，如果数据可以恢复，至少说明对业务的影响不会那么大，这样可以相应地降低异地多活架构设计的复杂度。

 

例如，用户的微博丢失后，用户重新发一篇一模一样的微博，这个就是可恢复的；或者用户密码丢失，用户可以通过找回密码来重新设置一个新密码，这也算是可以恢复的；而用户账号如果丢失，用户无法登录系统，系统也无法通过其他途径来恢复这个账号，这就是不可恢复的数据。

 

我们同样以用户管理系统的登录业务为例，简单分析如下表所示。

 

 

 

第3步：数据同步

 

确定数据的特点后，我们可以根据不同的数据设计不同的同步方案。常见的数据同步方案有：

 

存储系统同步

 

这是最常用也是最简单的同步方式。例如，使用MySQL的数据主从数据同步、主主数据同步。

 

这类数据同步的优点是使用简单，因为几乎主流的存储系统都会有自己的同步方案；缺点是这类同步方案都是通用的，无法针对业务数据特点做定制化的控制。例如，无论需要同步的数据量有多大，MySQL都只有一个同步通道。因为要保证事务性，一旦数据量比较大，或者网络有延迟，则同步延迟就会比较严重。

 

消息队列同步

 

采用独立消息队列进行数据同步，常见的消息队列有Kafka、ActiveMQ、RocketMQ等。

 

消息队列同步适合无事务性或者无时序性要求的数据。例如，用户账号，两个用户先后注册了账号A和B，如果同步时先把B同步到异地机房，再同步A到异地机房，业务上是没有问题的。而如果是用户密码，用户先改了密码为m，然后改了密码为n，同步时必须先保证同步m到异地机房，再同步n到异地机房；如果反过来，同步后用户的密码就不对了。因此，对于新注册的用户账号，我们可以采用消息队列同步了；而对于用户密码，就不能采用消息队列同步了（也可以使用加一个时间数据块的方法实现，但过于复杂）。

 

重复生成

 

数据不同步到异地机房，每个机房都可以生成数据，这个方案适合于可以重复生成的数据。例如，登录产生的cookie、session数据、缓存数据等。

 

我们同样以用户管理系统的登录业务为例，针对不同的数据特点设计不同的同步方案，如下表所示。

 

 

 

第4步：异常处理

 

无论数据同步方案如何设计，一旦出现极端异常的情况，总是会有部分数据出现异常的。例如，同步延迟、数据丢失、数据不一致等。异常处理就是假设在出现这些问题时，系统将采取什么措施来应对。异常处理主要有以下几个目的：

 

问题发生时，避免少量数据异常导致整体业务不可用。

 

问题恢复后，将异常的数据进行修正。

 

对用户进行安抚，弥补用户损失。

 

常见的异常处理措施有这几类：

 

\1. 多通道同步

 

多通道同步的含义是采取多种方式来进行数据同步，其中某条通道故障的情况下，系统可以通过其他方式来进行同步，这种方式可以应对同步通道处故障的情况。

 

以用户管理系统中的用户账号数据为例，我们的设计方案一开始挑选了消息队列的方式进行同步，考虑异常情况下，消息队列同步通道可能中断，也可能延迟很严重；为了保证新注册账号能够快速同步到异地机房，我们再增加一种MySQL同步这种方式作为备份。这样针对用户账号数据同步，系统就有两种同步方式：MySQL主从同步和消息队列同步。除非两个通道同时故障，否则用户账号数据在其中一个通道异常的情况下，能够通过另外一个通道继续同步到异地机房，如下图所示。

 

 

 

多通道同步设计的方案关键点有：

 

一般情况下，采取两通道即可，采取更多通道理论上能够降低风险，但付出的成本也会增加很多。

 

数据库同步通道和消息队列同步通道不能采用相同的网络连接，否则一旦网络故障，两个通道都同时故障；可以一个走公网连接，一个走内网连接。

 

需要数据是可以重复覆盖的，即无论哪个通道先到哪个通道后到，最终结果是一样的。例如，新建账号数据就符合这个标准，而密码数据则不符合这个标准。

 

\2. 同步和访问结合

 

这里的访问指异地机房通过系统的接口来进行数据访问。例如业务部署在异地两个机房A和B，B机房的业务系统通过接口来访问A机房的系统获取账号信息，如下图所示。

 

 

 

同步和访问结合方案的设计关键点有：

 

接口访问通道和数据库同步通道不能采用相同的网络连接，不能让数据库同步和接口访问都走同一条网络通道，可以采用接口访问走公网连接，数据库同步走内网连接这种方式。

 

数据有路由规则，可以根据数据来推断应该访问哪个机房的接口来读取数据。例如，有3个机房A、B、C，B机房拿到一个不属于B机房的数据后，需要根据路由规则判断是访问A机房接口，还是访问C机房接口。

 

由于有同步通道，优先读取本地数据，本地数据无法读取到再通过接口去访问，这样可以大大降低跨机房的异地接口访问数量，适合于实时性要求非常高的数据。

 

\3. 日志记录

 

日志记录主要用于用户故障恢复后对数据进行恢复，其主要方式是每个关键操作前后都记录相关一条日志，然后将日志保存在一个独立的地方，当故障恢复后，拿出日志跟数据进行对比，对数据进行修复。

 

为了应对不同级别的故障，日志保存的要求也不一样，常见的日志保存方式有：

 

服务器上保存日志，数据库中保存数据，这种方式可以应对单台数据库服务器故障或者宕机的情况。

 

本地独立系统保存日志，这种方式可以应对某业务服务器和数据库同时宕机的情况。例如，服务器和数据库部署在同一个机架，或者同一个电源线路上，就会出现服务器和数据库同时宕机的情况。

 

日志异地保存，这种方式可以应对机房宕机的情况。

 

上面不同的日志保存方式，应对的故障越严重，方案本身的复杂度和成本就会越高，实际选择时需要综合考虑成本和收益情况。

 

\4. 用户补偿

 

无论采用什么样的异常处理措施，都只能最大限度地降低受到影响的范围和程度，无法完全做到没有任何影响。例如，双同步通道有可能同时出现故障、日志记录方案本身日志也可能丢失。因此，无论多么完美的方案，故障的场景下总是可能有一小部分用户业务上出问题，系统无法弥补这部分用户的损失。但我们可以采用人工的方式对用户进行补偿，弥补用户损失，培养用户的忠诚度。简单来说，系统的方案是为了保证99.99%的用户在故障的场景下业务不受影响，人工的补偿是为了弥补0.01%的用户的损失。

 

常见的补偿措施有送用户代金券、礼包、礼品、红包等，有时为了赢得用户口碑，付出的成本可能还会比较大，但综合最终的收益来看还是很值得的。例如暴雪《炉石传说》2017年回档故障，暴雪给每个用户大约价值人民币200元的补偿，结果玩家都求暴雪再来一次回档，形象地说明了玩家对暴雪补偿的充分认可。

 

### **分布式协议**

基于分布式协议的高可用方案，常见的有Galera Cluster，PXC和MGR。

注：MGR是MySQL最新的一种分布式协议，在分布式数据库中已经有基于Paxos协议实现的数据一致性（比如OceanBase和TiDB）。

 

### **基于共享存储方案**

如SAN存储，这种方案可以实现网络中不同服务器的数据共享，共享存储能够为数据库服务器和存储解耦。

 

### **基于磁盘复制方案**

如DRBD，DRDB是一个以linux内核模块方式实现的块级别同步复制技术。它通过网卡将主服务器的每个块复制到另外一个服务器块设备上，并在主设备提交块之前记录下来。类似共享存储解决方案。

 

## 总结

MySQL高可用方案的建议，这些也是基于一些高可用的实践所做的总结。

\1) 行业内多活的设计目标不是多写，需要先实现跨机房的高可用容灾和计划内的机房间数据切换

\2) 引入consul的域名管理，解决VIP方案带来的一些潜在瓶颈（域名的业务属性，实现单机多实例，读写分离的域名配置）

\3) 对于consul的整体定位不局限于集群环境，在单实例，集群，分布式中间件方向都可以采用，consul是作为一种通用的基础域名服务。

\4) 同机房高可用方案的落地，需要和应用方对接程序端对域名的支持情况，在不同语言的客户端侧会有一些配置的差异。

\5) 在已有高可用方案MHA基础上平滑过渡和改进，在后续新业务尝试引入MGR的方案

\6) consul 业务API的开发，对数据库层面的业务可持续性访问（服务注销，服务发现）做一些补充和定制，保证consul服务的技术可控

\7) 异机房高可用实现应用无缝切换，计划内切换，会有业务中断/延时，保证可控的基础上，应用端无须修改连接配置，需要测试DNS的域名转发等策略，计划外切换，需要做确认才可完成。

\8) 对已有的分布式方案，可以采用MGR,中间件，consul的组合方案，实现读写分布式扩展。

# 解决方案

​	目前常用的高可用方案，自底层而上包括：

1、 数据库层面——主从复制，MySQL采用这种方式

金融领域很少采用，因为这种模式将主节点和从节点以及主从节点之间的网络环境紧紧地绑在一起，主节点的稳定性将不再由他们自己决定，而要同时看从节点和网络环境：一旦从节点或者网络环境抖动，主节点的性能就会受到直接影响。如果主节点和从节点之间是跨机房设置跨城市部署，发生这种概率的机会更大，影响也会更加显著。从某种程度上讲，和单节点模式相比，这种模式下主节点的稳定性不但没有增加，反而是降低了。

由于“主从复制”模式中缺少第三方仲裁者的角色，当主从节点之间的心跳信号异常时，从节点无法靠自己判断到底是主点故障了，还是主从之间网络故障了。此时，如果从节点认为是主节点故障而将自己自动切换为主节点，就极容易导致“双主”、“脑裂（brain split）”的局面，对用户来说这是绝对无法接受的结果。所以，数据库“主从复制”技术从来不会提供“从节点自动切换为主节点”的功能，一定要由人来确认主节点确实故障了，并手动发起从节点的切主动作，这就大大增加了系统恢复的时间（RTO）。

2、 底层硬件层面

在主机层面用HACMP技术以应对主机故障，或者在存储层面采取复制技术（比如FlashCopy）未提交数据冗余等，会使灾难切换方案变得很复杂，并且会有相对较长的故障恢复时间（RTO），所以通常不是数据库用户的首选。

3、 备份软件

支持异种数据库之间相互复制数据的产品，比如IBM CDC和Oracle Golden Gate（OGG）。这些产品的特点是比较灵活，可以支持异种数据库之间的数据复制，也可以指定只复制数据库中的部分对象（比如只复制指定几张数据表的数据）。但这些产品的缺点也很明显：首先相对于数据库主从复制来说，时延较大，通常会达到秒级以上，而且往往做不到数据库层面100%的完全复制。因此，这种方式通常作为不同数据库产品之间做数据“准”实时同步的手段，而不会作为数据库产品实现高可用及容灾的手段。

4、 集群层面

分布式多副本数据一致性技术，通常是基于Paxos协议或者Raft协议来实现。这种技术会将数据保存在多份副本上，每一次对数据的修改操作都会强同步到多数副本上，在保证了数据冗余的同时，不再像“主从复制”技术那样依赖某个数据节点的稳定性，从而消除了传统主从复制技术下从节点给主节点带来的危险。同时，在主节点故障的情况下，其余节点会自动选举出新的主节点以实现高可用（个别从节点故障则完全不影响服务），整个过程非常快速且完全无需人为干预。因此这种技术不仅能保证RPO=0，而且大大减小了RTO，相比传统主从复制技术，可以提供更强大的高可用能力。

此外，为了抵御机房级灾难和城市级灾难，可以将多份副本分散部署在多个机房里甚至多个城市里，以避免机房级灾难或者城市级灾难损失多数派副本。这样就具备了机房级和城市级容灾的能力，进一步加强了高可用的能力。

 

## MySQL+Hearbeat+DRBD

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC884.tmp.jpg) 

 

# 分布式数据库实践

## TDSQL

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC885.tmp.jpg) 

参考：

https://cloud.tencent.com/developer/article/1604225

https://cloud.tencent.com/edu/learning/live-3058

https://www.bilibili.com/video/BV1jT4y1w7vJ

***\*架构：\****

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC886.tmp.jpg) 

负载均衡通过LVS或F5/A10等实现，同时借助VIP实现网络映射。

SQL引擎是无状态的，也存在多个主备关系高可用。

Zookeeper管理各个集群的组建，并且将相关信息可视化。

HDFS用于数据的备份。

 

### **强同步复制**

MySQL原生的主从复制方式包括：异步（安全性差），半同步，同步，可以在主从之间设置一个超时时间来改善，这样半同步转异步，但是这样也会存在数据丢失（如果从库接收成功，但是网络存在故障，ACK信号一直未被主库接收，主库就会认为这个从库没有复制成功）。

强同步机制：任何一笔应答前端成功的请求除了在主机落盘成功外还会再其中一台备机落盘成功（必须存在2或以上的从库）。

强同步性能：在原生异步复制的基础上做了性能改良，接近异步复制。

强同步机制是TDSQL数据不会丢、不会错的最核心的保障。

### **容灾切换**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC896.tmp.jpg) 

特点：

1、主机可读可写，备机只读，任何时候只有一个主机提供服务

2、宁愿拒绝服务，不提供错误的服务

3、整个切换过程完全自动化，无需人为干预

4、严格的切换流程，确保切换前后数据的强一致性

过程：主节点发生宕机，重新选举新的主节点，并提供服务，同时保证数据的一致性。

### **部署方案**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC897.tmp.jpg) 

推荐机型配置：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC898.tmp.jpg) 

注：网关（即SQL引擎/proxy），主要用于计算，CPU和内存要求高。

#### **同城单中心**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC899.tmp.jpg) 

场景：

1、IDC资源紧张，只有一个数据中心

2、业务要最佳性能，不能容忍跨IDC网络延迟

3、作为异地灾备机房

4、测试环境

要求：

主备节点跨机架（不然在同一机架没有高可用而言了）

注：

IDC命名尽量有意义，例如IDC_HZ_Beijing_0401_000001，尽量与“城市+机房+房间+机架号”信息相对应。

#### **同城双中心**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8AA.tmp.jpg) 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8AB.tmp.jpg) 

注：同城两中心相距不要太远，相差不要超过1ms。

此方案跨中心强同步，ZK多数派部署在备中心。实现主中心宕掉之后能够自动切换到备中心，实现跨IDC容灾切换下具备两套的数据一致性。但是备中心故障会引起主中心不可用。

主IDC主机所在的机房与同城IDC备机采用同步方式，与本机房的备机采用异步方式（如果采用同步则会出现问题，比如本地机房备机同步完成，同城机房备机还没有完成，根据强同步，已经认为完成了主备同步，此时主机所在机房故障，则所有的数据都在本地机房同城机房并没有完成同步，这样就造成数据丢失）。

主IDC的备机不能缺少，因为如果同城IDC宕掉，此时主IDC只有一个主库，根据强同步此时主库是不提供服务的。

针对同步备IDC宕掉的情况，可以采用多数派部署在主机房的方案，如下：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8AC.tmp.jpg) 

但是，主IDC宕掉仍然存在问题，可以采用同城三中心方案。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8AD.tmp.jpg) 

#### **同城三中心**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8BD.tmp.jpg) 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8BE.tmp.jpg) 

 

#### **两地三中心**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8BF.tmp.jpg) 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8C0.tmp.jpg) 

#### **两地四中心**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8C1.tmp.jpg) 

1、同城三中心集群化部署，简化同步策略，运营简单，数据可用性、一致性高

2、单中心故障不影响数据服务

3、同城生产集群三中心多活

4、整个城市故障可以人工切换

#### **部署方案总结**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8D2.tmp.jpg) 

## OceanBase

参考：

https://developer.aliyun.com/article/712723

https://tech.antfin.com/community/live/773

### **多副本日志同步**

### **分布式选举**

### **节点故障处理策略**

### **部署方案**

#### **单机房3副本**

#### **同城3机房3副本**

#### **3地3机房5副本**

#### **同城2机房3副本**

#### **2地3机房5副本**

#### **集群间数据复制**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8D3.tmp.jpg) 

## TiDB

### **同城多中心部署**

作为 NewSQL 数据库，TiDB 兼顾了传统关系型数据库的优秀特性、NoSQL 数据库可扩展性以及跨数据中心场景下的高可用。

#### **Raft算法**

Raft 是一种分布式一致性算法，在 TiDB 集群的多种组件中，PD 和 TiKV 都通过 Raft 实现了数据的容灾。Raft 的灾难恢复能力通过如下机制实现：

Raft 成员的本质是日志复制和状态机。Raft 成员之间通过复制日志来实现数据同步；Raft 成员在不同条件下切换自己的成员状态，其目标是选出 leader 以提供对外服务。

Raft 是一个表决系统，它遵循多数派协议，在一个 Raft Group 中，某成员获得大多数投票，它的成员状态就会转变为 leader。也就是说，当一个 Raft Group 还保有大多数节点 (majority) 时，它就能够选出 leader 以提供对外服务。

遵循 Raft 可靠性的特点，放到现实场景中：

想克服任意 1 台服务器 (host) 的故障，应至少提供 3 台服务器。

想克服任意 1 个机柜 (rack) 的故障，应至少提供 3 个机柜。

想克服任意 1 个数据中心（dc，又称机房）的故障，应至少提供 3 个数据中心。

想应对任意 1 个城市的灾难场景，应至少规划 3 个城市用于部署。

可见，原生 Raft 协议对于偶数副本的支持并不是很友好，考虑跨城网络延迟影响，或许同城三数据中心是最适合部署 Raft 的高可用及容灾方案。

 

#### **同城三中心方案**

同城三数据中心方案，即同城存有三个机房部署 TiDB 集群，同城三数据中心间的数据同步通过集群自身内部（Raft 协议）完成。同城三数据中心可同时对外进行读写服务，任意中心发生故障不影响数据一致性。

##### 架构图

集群 TiDB、TiKV 和 PD 组件分别分布在 3 个不同的数据中心，这是最常规且高可用性最高的方案。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8D4.tmp.jpg) 

优点：

所有数据的副本分布在三个数据中心，具备高可用和容灾能力

任何一个数据中心失效后，不会产生任何数据丢失 (RPO = 0)

任何一个数据中心失效后，其他两个数据中心会自动发起 leader election，并在合理长的时间内（通常情况 20s 以内）自动恢复服务

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8D5.tmp.jpg) 

缺点：

性能受网络延迟影响。具体影响如下：

对于写入的场景，所有写入的数据需要同步复制到至少 2 个数据中心，由于 TiDB 写入过程使用两阶段提交，故写入延迟至少需要 2 倍数据中心间的延迟。

对于读请求来说，如果数据 leader 与发起读取的 TiDB 节点不在同一个数据中心，也会受网络延迟影响。

TiDB 中的每个事务都需要向 PD leader 获取 TSO，当 TiDB 与 PD leader 不在同一个数据中心时，它上面运行的事务也会因此受网络延迟影响，每个有写入的事务会获取两次 TSO。

##### 架构优化图

如果不需要每个数据中心同时对外提供服务，可以将业务流量全部派发到一个数据中心，并通过调度策略把 Region leader 和 PD leader 都迁移到同一个数据中心。这样一来，不管是从 PD 获取 TSO，还是读取 Region，都不会受数据中心间网络的影响。当该数据中心失效后，PD leader 和 Region leader 会自动在其它数据中心选出，只需要把业务流量转移至其他存活的数据中心即可。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8E6.tmp.jpg) 

优点：

集群 TSO 获取能力以及读取性能有所提升。具体调度策略设置模板参照如下：

-- 其他机房统一驱逐 leader 到业务流量机房

config set label-property reject-leader LabelName labelValue

 

-- 迁移 PD leader 并设置优先级

member leader transfer pdName1

member leader_priority pdName1 5

member leader_priority pdName2 4

member leader_priority pdName3 3

缺点：

写入场景仍受数据中心网络延迟影响，这是因为遵循 Raft 多数派协议，所有写入的数据需要同步复制到至少 2 个数据中心

TiDB Server 数据中心级别单点

业务流量纯走单数据中心，性能受限于单数据中心网络带宽压力

TSO 获取能力以及读取性能受限于业务流量数据中心集群 PD、TiKV 组件是否正常，否则仍受跨数据中心网络交互影响

 

### **两地三中心部署**

## Oracle

参考：

https://mp.weixin.qq.com/s/k5u0kHt85D-ple8tw6KyLg

 

## GoldenDB

### **架构**

#### **HA**

##### 安装

**安装pacemaker/cman/pcs插件：**

yum install lvm2-cluster corosync pacemaker pcs fenceagents-all -y

**卸载NetManager：**

yum remove NetworkManager

**关闭corosync服务：**

systemctl disable corosync.service

或

chkconfig corosync off 

 

###### 配置hosts

1、配置双机hosts文件

假设两台管理节点hostname为：dbmgr01 dbmgr02

分别修改两台管理节点的/etc/hosts文件

2、修改hosts文件属性

保护hosts文件被修改：chattr +i /etc/hosts

###### 双机配置

**初始化集群**

1、初始化集群配置

1）创建hacluster用户与密码

2）分别在dbmgr01和dbmgr02主机上执行

3）passwd hacluster

2、启动pcsd服务

分别在dbmgr01与dbmgr02主机上执行：

systemctl start pcsd

systemctl enable pcsd

或

service pcsd start

chkconfig pcsd on 

3、认证pcs

仅在dbmgr01主机上执行，用户名密码为初始化集群设置

pcs clusyer auth dbmgr01 dbmgr02

**创建集群**

手动在dbmgr01、dbmgr02创建/etc/cluster目录，仅在dbmgr01主机上执行

pcs cluster setup --start --name dbmgrcluster dbmgr01 dbmgr02

启动集群

pcs cluster start -all

验证集群

pcs status

**配置集群参数**

以下操作仅在dbmgr01上执行：

1、设置全局参数

pcs property set no-quorum-policy=ignore  //设置不需判决盘

pcs resource op defaults on-fail=migrate  //设置迁移为缺省动作

pcs property set stonith -enabled=false //不使用fence_ip服务

2、创建浮动IP

pcs resouce create managerIP ocf:heartbeat:IPaddr2 ip=”” cidr_netmask=24 --group manager

3、创建资源规则关系

pcs constraint location manager pregers dbmgr01=100

pcs constraint location manager pregers dbmgr02=200

 

###### OMM安装

**检查HA状态**

pcs status

**OMM安装**

1、创建/home/setup目录

2、上传安装文件解压

3、修改配置文件

4、执行安装脚本

参数说明：

[deviceip]：OMM访问IP

[ommuser]：OMM用户名，默认goldendb

[port]：OMM数据库访问端口

[super_user_name]：OMM数据库高级用户

[super_user_pass]：OMM高级数据库用户密码

[normal_user_name]：OMM普通用户

[normal_user_pass]：OMM普通用户密码

**OMM双机配置**

1、停OMM数据库

需在dbmgr01与dbmgr02上分别执行：

su - goldendb

dbmoni -stop

2、修改管理节点主机1、2上的mysql用户的userid，保持一致
	groupmod -g 600 dba

usermod -u 601 -U goldendb

chown -R goldendb:dba /home/goldendb/

3、修改OMM配置文件

需分别修改dbmgr01和dbmgr02的配置文件：~/was/tomcat/webapps/uniportal/WEB-INF/classes/omm-system.xml

 

**OMM数据库配置**

1、修改OMM数据配置文件

需在dbmgr01和dbmgr02上分别执行：

rpl_semi_sync_master_enabled=ON

rpl_semi_sync_master_quick_sync_enabled=ON

rpl_semi_sync_slave_enabled=ON

2、分别启动两个OMM数据库

su - goldendb

dbmoni -start

3、仅启动dbmgr01的OMM进程

su - goldendb

jtool -start

**登录OMM**

 

###### 安装后操作

**配置OMM数据库高可用**

1、关闭OMM与Manager组件

需要在dbmgr01与dbmgr02上分别执行：

su - goldendb

jtool -was stop

su - zxmanager

dbmoni -stop

2、创建半同步用户

需要在dbmgr01和dbmgr02上粉笔而执行

create user ‘repl@’%’’ identified by ‘’;

GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO ‘repl’@’%’ identified by ‘’;

flush privileges;

再执行：

stop slave;

reset slave all;

reset master;

**同步OMM数据库数据**

1、关闭OMM数据库

需在dbmgr01和dbmgr02上分别执行：

dbmoni -stop

2、手动打包数据文件

在dbmgr01主上，手动打包OMM目录下data目录，复制到dbmgr02备机上

3、手动恢复数据文件

在备机dbmgr02上，goldendb用户下清空data目录

在goldendb用户下，释放masterdata.tar数据

删除auto文件：rm -rf ~/data/data/auto.cnf

4、重启OMM数据库

需在dbmgr01和dbmgr02上分别执行：

dbmoni -start

**建立主备关系**

1、建立主备关系

仅在dbmgr02上执行：

stop slave;

reset slave all;

reset master

change master to master_host=’’, master_user=’repl’,

master_password=’’,master_port=3306,master_auto_position=1;

start slave;

2、查看主备关系

连接主RDB查看：

show status like ‘Rpl_semi_sync_master_status’;

连接备机RDB查看：

show status like ‘%semi%slave%’;

**复制OMM资源管理脚本**

 

**添加OMM的HA资源**

1、添加OMM的HA双机管理资源

pcs resource create rdbActive lsb:rdbActive.sh op monitor interval=5s timeout=100s start interval=0s timeout=30s --group manager

pcs resource create rdbStandby lsb:rdbStandby.sh op monitor interval=5s timeout=30s 

pcs resource create ommsh lsb:ommop monitor interval=5s timeout=100s --group manager

2、添加资源约束条件

1）rdbActive启动后再启动rdbStandby

pcs constraint order start rdbActive then start rdbStandby

pcs constraint colocation add rdbStandby with rdbActive score=-INFINITY

2）rdbActive启动后再启动OMM节点

pcs constraint order start rdbActive then start ommsh

**查看HA状态**

**复制组件资源管理脚本**

**添加管理组件的HA资源**

1、添加HA管理资源

仅在dbmgr01上执行：

pcs resource create mds sh lsb:metadataserverha.sh op monitor interval=5s timeout=100s --group manager

pcs resource create pmsh lsb:proxymanagerha.sh op monitor interval=5s timeout=10s --group manager

pcs resource create cmsh lsb:cmha.sh monitor interval=5s timeout=100s --group manager

2、添加资源约束条件

pcs constraint order start rdbActive then start mdssh

pcs constraint order start mdssh then start pmsh

pcs constraint order start mdssh then start cmsh

pcs constraint colocation add ommsh with rdbActive score=-INFINITY

pcs constraint colocation add mdssh with rdbActive score=-INFINITY

pcs constraint colocation add pmsh with rdbActive score=-INFINITY

pcs constraint colocation add cmsh with rdbActive score=-INFINITY

 

**查看HA状态**

##### 特点

#### **Zookeeper**

### **组件高可用**

​	采用去中心化设计，整个集群无单点。

#### **计算节点(Proxy)高可用**

GoldenDB中的计算节点是无状态的，所有的计算节点集群都对外提供服务，当其中一个计算节点出现故障的时候，业务访问会路由到其它正常计算节点，对业务基本无感知。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8E7.tmp.jpg) 

注：OMMAgent是通过ZK实现选主的，也就是说对应的各个组件都是通过ZAB协议实现选主的。

##### 部署方案

​	计算节点是无状态节点，一般要求为每个业务至少配置2个计算节点。

##### 高可用方案

为降低计算节点异常对业务的影响，可以采用的措施：

1、 监控节点健康状况（客户端定时发心跳语句保活）；

2、 在某些节点发生故障时，实现故障的接管；

3、 异常事务清理

#### **数据节点(DB)高可用**

DB数据节点是通过一主多备的方式实现高可用，主备之间通过快同步复制技术实现数据同步。DB节点之间的主备切换可以通过OMM（Insight管理界面）发起，当DB主节点出现故障时，由CM（ClusterManager）发起故障切换流程。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8E8.tmp.jpg) 

注：具体切换哪个为主，这个是通过insight配置各个group的优先级，一般都是本地机房优先（前提条件是这个group的DB GTID应该满足最大才可以）。

##### 部署方案

​	为了提升数据服务的可靠性和可用性，安全组Group一般由多个数据节点组成。其中一个数据节点为主节点，提供读写服务，其他节点为备节点，提供读服务，在一个安全组中可以设置多个副本。

​	通过数据节点主备间通过数据库的复制技术来进行主备机之间的数据同步，主机至多个备机之间为星型复制模式，即主机直接向多个备机进行数据同步，具体方式目前支持半同步和异步两种。

##### 高可用方案

###### 基本流程

主备异常切换流程：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8F8.tmp.jpg) 

GoldenDB数据节点主备间通过数据库的复制技术来进行主备机之间的数据同步，主机至多个备机间为星型复制模式，即主机直接向多个备机进行数据同步，具体支持半同步、异步和快同步方式。

结合高可用和高性能，GoldenDB目前数据节点支持三种高可用策略：

###### 高可用策略

**最大性能策略**

安全组对外提供最大的写性能，副本间采用异步复制，即一旦日志数据写到主节点，事务即可完全提交，该种策略时主节点和备节点之间的数据复制独立于事务提交过程，其同步速率完全依赖于主节点和备节点之间的网络带宽和速度，主节点和备节点之间存在不一致的可能，在主节点异常时存在丢数据的风险。

​	

**最大保护策略**

​	日志数据必须同时写到主节点和至少一个备节点，事务才能提交，这种机制能够保证数据节点和备用数据节点之间数据完全一致，主机异常时不会丢数据，但是该方案一方面降低了安全组的写可用性（备节点不可用时，安全组无法对外提供写能力），另一方面性能也降低。

 

**最大可用策略**

​	最大保护策略和最大性能策略之间取了折中。正常情况下运行在最大保护模式下，而当主节点和备节点无法同步复制超过一定时间时就会切换到到最大性能策略，待同步故障解决后再切换回最大保护策略。

上述三种模式用户可以根据需要选择，在具体实现上GoldenDB系统由管理节点（CM）监控安全组内主备节点间的数据复制进展及健康情况、数据节点的自身健康情况等。在主节点异常后，管理节点需要根据各个备节点的数据复制进展、物理位置（同一个机房）从备节点中选举出新的主节点，并组织相关数据节点重构安全组内的数据复制关系。当异常节点恢复后，能够及时发现该节点，并将该节点作为备节点加入到安全组内的数据复制关系中。

 

###### 副本一致性

异常切换和人工演练（非强制指定主）流程，就是主挂了，如何判断是否存在一致性副本的问题。之前的流程，没有考虑到多个DB节点故障的问题，master挂了，认为master到开始处理异常切换（向MDS查询是否存在一致性副本）时间段，没有备机挂。

为了减少误切的可能性，在MDS返回一致性副本的情况下，对照DB节点故障进行考虑。根据DB的心跳上报给CM的状态来进行判断DB状态。

**说明：**

容忍时间：CM在第一次检测到master挂的时候，往前推x秒内，这个s秒叫做容忍时间。

容忍时间=心跳间隔 * 心跳次数 + 20s（误差补偿）

FBM（Fault before master）：在容忍时间内，备机都是坏的状态

Good：在容忍时间内，备机都是好的

FAM（Fault after or simultaneous master）：在容忍时间内，状态不全是坏的状态，也不全是好状态

FAM_S：与主机相同team的FAM

FAM_D：与主机不通过team的FAM

根据心跳判断DB状态，如果DB的心跳记录都在容忍时间内，记录不足，认为是FAM。

**注意：**

如果我们在向备机获取binlog位置的时候，发送消息失败，超时收不到响应或者响应返回失败，我们认为此时备机是坏的状态。此时Good备机变为FAM。

如果备机是Good，在之后有时好，有时坏，将根据它当时的状态，确定它是FAM还是Good。

**逻辑判断：**

如果MDS返回中间水位（低于高水位），如果存在FAM，则认为不存在一致性副本

如果MDS返回正常水位，如果存在FAM_D，则认为不存在一致性副本

 

多DB节点故障一致性副本具体方案：

1、记录DB的状态并更以及变更时间，并保存到DB结构体里面，为了防止内存占用过多，只记录状态变更，如果状态未变更，则不算变更记录，时间戳也不变化

2、区分第一次进入异常状态和不是第一次进入异常状态，判断逻辑，主DB由好的状态（非DB_Failed并且非DB_Stopped）变为坏的状态（DB_Failed或者DB_Stopped），如果是第一次异常，记录这次检测到master挂的时间戳，并存储该group内db的所有状态变更历史

3、把该group内的相同城市的所有DB状态和变更时间信息遍历，根据该时间戳和group的db的状态变更记录，在容忍时间内，如果都是坏的，认为FBM（主机之前挂的备机），如果都是好的，则认为备机为好的，否则为FAM

4、讨论获取binlog位置的步骤，如果请求发送不出去，响应返回失败，响应超时，均认为备机挂了，挂的备机中，之前认定位Good的备机，变成了FAM

5、如果找到了FAM，就可以按照下面逻辑进行一致性判断：

如果MDS返回中间水位（低于高水位），如果存在FAM，则认为不存在一致性副本

如果MDS返回正常水位，如果存在FAM_D，则认为不存在一致性副本

注：主计数表示主节点也算作可用的db数，如果group中只有一个主机可用，其他备机都异常，这样也可以对外提供服务；如果选择主节点不计数，即主节点不当做可用节点，如果只有主节点可用，那么对外可用节点数为0，不能提供服务。

**具备切换条件后，将GTID最大的节点置为新主，并解除只读，修正备机的主备关系。**

 

#### **全局事务管理节点(GTM)高可用**

GTM事务节点通过一主多备的方式实现高可用，主备之间通过MQ增量实时同步消息的方式同步GTID信息。

GTM主备节点间进行全量和增量的数据同步，保证主备机之间数据一致。目前系统的策略为最大可用策略，即当主节点和备节点无法同步超时后直接回复客户端成功响应消息，待故障解决后GTM主节点向备节点进行全量数据推送。

全局事务的状态必须是持久化的，且不随着单节点异常、掉电等异常的发生而丢失。考虑性能，GTM的主备节点均支持实时持久化和定时周期持久化，用户可以在高可用和高性能之间根据需要作出合理配置。

 

GTM节点之间的主备切换可以通过OMM（Insight管理界面）发起，当GTM主节点出现故障时，由MDS（Metadataserver）发起故障切换流程。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8F9.tmp.jpg) 

##### 部署方案

​	全局事务管理需要支持双机热备部署，主备双活，实现故障时秒级切换。

##### 高可用方案

###### 多点故障

###### 故障切换

GTM切换：

1、GTM切换是由元数据控制的

2、GTM主备链路：如果备异常，主没有收到消息，分为两种情况：

1）不同步GTID时，主在2s*8后向元数据上报备机不可用状态

2）同步GTID时，同步失败，立马向元数据上报备机不可用

**故障切换**

**人工切换**

**机房切换**

#### **管理节点(PM/CM/MDS)高可用**

GoldenDB支持通过HA或Zookeeper两种方案实现管理节点高可用。

1）管理节点HA方案

HA是通过Pacemaker实现双机热备，通常把正在执行的业务称为活动节点，活动节点的备份称为备用节点。当活动节点出现问题，导致正在运行的业务不能正常运行时，备用节点此时会侦测到，并立即接管活动节点来执行任务，从而实现业务的不中断或短暂中断。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8FA.tmp.jpg) 

管理节点同机房内采用HA方案，本地和同城机房通过快同步方式同步数据。

2）管理节点zookeeper方案

在zookeeper管理中，所有的事务请求都必须由一个全局唯一的服务器来协调处理（称为Leader），其它服务器称为follower服务器。Leader负责将客户端事务请求转换为事务proposal，并将该proposal分发给集群所有的follower服务器，之后Leader需要等待所有follower服务器的反馈，当超过半数的follower正确的反馈后，leader则会再次向所有的follower服务器分发commit消息，要求将前一个proposal进行提交。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC8FB.tmp.jpg) 

Zookeeper方案中的zookeeper集群建议使用奇数，防止出现脑裂

Zookeeper集群的每台服务器都会在内存中维护当前服务器的状态，并且每台机器间都相互保持通信，当存在一半的机器能够正常工作，整个集群都能正常对外服务

在管理节点切换的过程中，可能会存在zookeeper主节点和RDB主节点不在一台服务器上的情况

 

##### 部署方案

​	管理节点采用双机冷备的模式，可以实现秒级切换。主要包括以下两点：

1、数据高可用。

***\*管理节点间通过DRBD进行数据实时同步\****。将数据写入主节点本地文件系统时，数据还将会被发送到网络中备节点上，当主节点系统出现故障时，备节点上还会保留有一份相同的数据。

2、服务高可用。

采用HA软件（现在已经弃用，采用ZK）实时监控主节点的运行状态，当确认主节点系统异常时，HA软件启动备节点，GoldenDB兼容的HA软件包括Linux的高可用集群解决方案PackMaker+Corosync、中兴新支点的高可用集群软件NewStar。

##### 高可用方案(ZK)

管理节点的高可用方案：zookeeper

管理组件：agent通过与ZK建立临时节点参加选主

非管理组件：agent通过ZK的watch机制实现对当前管理节点信息的监听

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC90C.tmp.jpg) 

###### 初始化流程

1、每个组件下面均有一个ommagent代理模块，安装ommagent的时候，规划的所有管理节点信息（管理节点IP、城市信息、机房信息、选主优先级编号）会写入到ommagent的配置文件中（如etc/haconfig.xml和netconfig.xml）；

2、ommagent进程启动时读取配置文件中管理节点信息并缓存，然后读取所在服务器上访配置文件os.ini，获取当前服务器的IP；

3、ommagent根据上面获取的两个信息（服务器IP和管理节点信息）可以判断当前服务器的角色：是否为管理节点服务器，作为全局变量保存

4、ommagent进程判断当前服务器是否为管理节点并且优先级编号为最小的，如果是：则在在ZK创建持久节点/ha/mds_election和/ha/omm_election（不存在再创建）；

5、ommagent进程判断当前服务器是否为管理节点，如果是：则在ZK创建临时节点/ha/mds_elction/mds_X和/ha/mds_elction/omm_X，X是优先级编号，并向节点写入该服务器对应的ID、IP、PORT值

6、ommagent进程判断当前服务器是否为管理节点：

1）如果是，则watch节点/ha/mds_election和/ha/omm_election的NodeChildrenChanged事件，并获取/ha/mds_election和/ha/omm_election下所有子节点列表并作为全局缓存

2）处理事件中子节点增加或删除动作，更新上面缓存中的记录

3）判断/ha取值为空串或有值但其IP不在子节点列表缓存中则执行选主流程

注：/ha/mds_electioon取值为空串对应的是首次选主：首次选主的原则为参与选主的节点超过半数以上注册即可触发，将最小ID值的节点选择为主用；有值但其中的IP不在子节点列表说明原主挂了需要选择新主。

4）如果不是，则watch节点/ha/mds_election的NodeDataChanged事件，事件处理时通过socket接口发消息通知GoldenDB组件最新的主管理节点变更后的信息

7、ommagent进程获取/ha/mds_election和/ha/omm_election节点取值，如果不为空串并且election_ip等于本服务器IP，则启动独立线程检查当前服务器管理组件是否运行正常：检查dbmoni进程是否存在，不存在的话则主动删除对应的/ha/mds_election/mds_X临时节点（触发重新选择主管理节点流程）

###### 选主流程

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC90D.tmp.jpg) 

**1、*****\*定时任务监听\****管理节点组件状态、omm相关进程（TOMCAT、ActiveMQ）及元数据（RDB=Relation Database）状态

管理节点状态：

Tomcat/ActiveMQ：ps -ef | grep

RDB状态：mysql server status

2、监听出现问题（进程不存在、状态异常等）触发***\*整体切换流程\****

3、根据一直运行的脚本获取高低水位状态，在切换开始时，检验是否低于高水位，如果低于高水位，不触发切换逻辑

4、3步骤校验高于高水位，杀死本地的管理节点（这个如果故障原因为管理节点ommagent故障，无法判断，如果停管理节点进程，ommagent是杀掉临时节点），同时需要杀死OMM

5、等待所有的备机回放完成

6、查询所有参与选主的管理节点：根据是否允许跨机房切换来确定参与选择的管理节点——返回参与选主管理节点信息

7、判断参与选主管理节点中有无一致性副本，如无一致性副本，选取GTID最大且优先级最高（确保选出主唯一）的管理节点为主

8、若有一致性副本，选取一致性副本中优先级最高的作为新主

9、选出新主后根据标志位判断主是否需要拉数据，如果需要拉数据则调用新主拉数据脚本

10、确保新主数据最大后，启动新主的omm，建立RDB主备管理

11、校验RDB中主备一致性，若成功则返回新主IP并提供服务

 

#### **Monitor监控进程**

每一个组件都采用dbmoni监控，如果检测到未启动或异常，会调用dbmoni -start重新启动。

注：使用dbmoni -start启动后如果采用GDB调试，会认为进程hang住，则会杀掉进程然后重启，这个时候使用dbstart/dbstop启停服务即可，也可以在os.ini中配置检测到系统hang住不重启（check_module_hang=0）。

 

### **故障切换**

#### **同城切换**

#### **异地切换**

#### **组件切换**

##### 计算节点(Proxy)异常切换

 

##### 管理节点(PM/CM/MDS)异常切换

以zookeeper管理为例，定时任务会监听管理节点组件及OMM相关进程和RDB的状态，并将状态信息保存到zookeeper中。当监听出现问题（进程不存在、状态异常等）触发整体切换流程，切换流程如下：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC90E.tmp.jpg) 

1、根据一直运行的脚本获取高低水位信息，在切换开始时检验是否低于高水位

2、如果低于高水位，不触发切换逻辑；高于高水位时，杀本地管理节点及OMM

3、等待所有备机回放完成

4、所有参与选主的管理节点：根据是否允许跨机房切换来确定参与选主的管理节点，返回参与选主管理节点的信息

5、判断参与选主管理节点中有无一致性副本

6、如无一致性副本，则选取gtid最大且优先级最高的管理节点为主；如有一致性副本，则选取一致性副本中优先级最高的作为新主

7、选出新主后根据标志位判断新主是否需要拉数据，如果无一致性副本，则需要向新主拉数据

8、确保新主数据最大后，启动新主的OMM、管理节点，建立RDB主备关系

9、校验RDB主备一致性，若成功则返回新主ip并提供服务

 

管理节点中的RDB数据库异常切换流程同上。其中管理节点异常切换还需要注意以下几点：

1、管理节点中的组件（MDS、CM、PM和OMM）其中一个状态异常会触发整体切换

2、管理组件和RDB是不同的切换逻辑，当管理组件异常时会触发管理组件的主切换，但是RDB的主备没有发生切换，所以会出现管理组件的主和RDB的主不在同一个节点的情况

3、Zookeeper作为管理组件和RDB状态的管理者，RDB主备之间还是通过快同步方式同步

 

##### GTM事务节点异常切换

###### 切换策略

在OMM界面可以设置切换的策略：

跨机房自动切换配置：该配置项只针对GTM故障切换有效。

主机是否计数配置：team中GTM响应个数是否包括主机。

增量持久化方式：GTM本地写GTID的方式，分为仅主机做增量持久化、主备机均不作增量持久化、主备机均做增量持久化。默认为主机做增量持久化。

Team中GTM响应数量：主备增量同步时，每个team中回复注GTM的备机数目达到该配置项时，认为该team响应达标。当配置数大于实际数时，所有备机响应即可，最多不超过4个。主要用于GTM高低水位计数。

备机响应team数低于低水位线：主备增量同步时，至少有该配置项指定目的team达标时GTM主机才会给proxy端回复批量申请、释放GTID成功响应，否则GTM主机告警低于低水位，系统只读，并给proxy端回复批量申请、释放GTID失败响应。该配置项取值范围为0-4的整数。

备机响应team数高水位：主备增量同步时，备机响应team数目达到该配置时给proxy端回复批量申请、释放GTID成功响应，否则GTM主机告警低于高水位需要修复系统。该配置项取值范围为0-4的整数。

 

###### 切换条件判断

1、GTM切换由MDS控制

2、GTM主备链路：GTM备通过OS链路向GTM主上报自己的状态，如果备机异常，主机没有收到消息，分为两种情况：

1）不同步GTID时，主在2s*8次后向MDS上报备机为不可用状态

2）同步GTID时，同步失败，立马向MDS上报不可用

GTM是否可切换判断条件如下：

1、低于高水位

2、备机异常

3、主机计数，主机与异常备机属于多个team；主机不计数，多个异常备机属于多个team

满足1&&2||2&&3不允许切换（水位配置为1:1，有异常备机就不可切）

 

判断逻辑如下：

1、MDS增加一个缓存，当收到OS发送的GTM断链事件时将GTM信息加入到缓存，当收到主GTM上报的某个备机可用/不可用事件时将GTM信息从缓存中移除；

2、根据1中的方法，当主GTM异常后，触发GTM主备切换，此时MDS缓存中存在的GTM就是和主GTM同时异常或者在主GTM异常之后的异常GTM；

3、根据缓存中的异常GTM，按照如下方法判断是否可以切换：

1）如果当前GTM集群的水位信息为高水位，则需要判断配置项里主机是否计数：

1.1）主机计数，主机与异常备机属于多个team，禁止切换

1.2）主机不计数，多个异常备机属于多个team，禁止切换

2）如果水位信息为低于高水位，需要查看内存是否有异常备机，有就禁止切换

3）如果当前水位低于低水位，则禁止切换

4）若不可切换，则告警

4、切换完成后，缓存中信息清除，omm界面手动切换也清除

 

###### 故障切换

GTM主异常，分为两种情形：

1、GTM主机器异常，由操作系统通过系统链路告知MDS，GTM异常了，触发故障主备切换流程，且MDS告警

2、GTM主断网或者网络延迟，操作系统无法感知，MDS通过OS每隔2s检测一次链路，检测8次链路均异常，MDS再等5s后看GTM是否恢复，如果16+5秒后GTM主仍没有恢复，则触发主备切换流程，且MDS告警

GTM异常切换流程如下：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC90F.tmp.jpg) 

1、MDS设置原主GTM为不可用状态，并通知PM（PM通知proxy）、CM禁用GTM

2、MDS收到PM、CM禁用GTM的响应消息后通知所有备GTM停止服务，并选择一个GTM作为新主（先本地备再同城备，异地备不参与选主）

3、所有备GTM清除与原主的链路，并向MDS回停服响应，MDS收到停服响应后，更新GTM元数据中主从信息

4、MDS通过OS链路通知所有可用状态的GTM启动服务和主从切换结果

5、从GTM收到启动服务请求后向新的主GTM建立链路

6、主GTM收到启动服务请求后向备GTM全量同步GTID和sequence数据

7、主GTM给MDS回复主从切换应答；新的主GTM确认有一台从GTM全量同步完成后升级为active状态；

8、MDS通知PM（通知proxy）、CM启用GTM并下发新的主GTM信息；（proxy收到启动服务请求后创建与主GTM链路、CM收到启动服务请求后创建与主GTM链路）

9、MDS给OMM回应主从切换应答，刷新OMM，主从切换成功

###### 人工切换

流程说明：

1、OMM界面发起主从切换

2、MDS设置原master GTM为停服状态病态通知PM、CM禁用GTM

3、Proxy、CM收到禁用GTM请求后停止访问GTM，proxy去除与原master GTM的链路维护，PM、CM给MDS回禁用GTM响应消息

4、MDS收齐PM、CM禁用GTM的响应消息后通知原master GTM停服和新的master GTM消息

5、原master GTM收到停服请求后自身降级（置为INACTIVE状态），并向所有链路正常的slave GTM全量同步GTID

6、原master GTM确认向新的master GTM全量同步完成后给MDS回停服响应

7、MDS收到原master GTM停服响应后，更新MDS元数据中主从消息，并将原master GTM置为可用状态

8、MDS通知所有可用状态的GTM启动服务和主从切换结果

9、Slave GTM收到启服请求后向新的master GTM建链

10、Master GTM收到启服请求后向slave GTM全量同步GTID和sequence数据

11、Master GTM给MDS回主从切换应答：新master GTM确认有一台slave GTM全量同步完成后升级为ACTIVE（高可用模式下没有slave GTM直接升级为ACTIVE）

12、MDS通知PM、CM启用GTM并下发新的master GTM消息（proxy收到启服请求后创建与master GTM的链路）

13、PM、CM给MDS回启用GTM响应消息

14、MDS给OMM回主从切换应答，刷新OMM，主从切换完成

 

###### 机房切换

**心跳表**

Proxy纳入OMM界面管理后，绑定集群的连接实例并启用，当MDS感知到元数据中有运行状态的proxy时，MDS会选择一个proxy在集群上创建心跳表（复制表），并每隔5s插入一条心跳数据，共异地故障切换使用。

心跳表的内容包括：

表中保留当前最新周期的数据，历史周期的数据对分布式数据库系统来说意义不大，不需要保留，也就是该表永远只有1条数据。这条数据记录了该条记录的时间，该时刻的活跃GTID，以及最大GTID。

心跳数据相比分布式事务做了特殊处理，不申请GTID，这样就节省了GTID资源。分布式事务申请GTID是为了保证全局事务的一致性，在需要回滚的时候能够根据GTID去定位需要回滚的语句。而心跳表与业务无关，异地故障切换时系统能够容忍心跳数据分布式不一致导致的误差，至于如何处理切换时刻心跳数据的不一致。

**人工演练**

**切换场景**

同城和异地灾备组网环境中，人工演练切换支持如下场景：

1、从城市A的机房1切换到城市A的机房2，从城市A的机房2切换到城市A的机房1

2、从城市A的机房2切换到城市B，从城市B切换到城市A的机房2

3、从城市A的机房1切换到城市B，从城市B切换到城市A的机房1

有关城市间回切的说明，主要是DB角色切换的说明，是原主和逻辑主的切换，所以切换时逻辑主在哪个机房，该DBGroup的新主就运行在哪个机房。

 

**切换步骤**

**同城机房间切换步骤**

假设分布式数据库系统运行在城市A的机房1，现在要做人工演练，将系统从城市A的机房1迁到城市A的机房2，接着再从城市A的机房2迁回城市A的机房1。

人工演练脚本的执行步骤如下：

1、城市A的机房1切换到机房2

1）停掉机房1的OMM进程

2）停掉机房1的manager进程

3）RDB复制关系调整。机房2的RDB升级为主机，机房1的RDB切换为机房2的RDB备机，城市B的RDB（如果有异地组网的话）切换为城市A的机房2的RDB的备机。切换前先校验一下原来的复制关系，确保即将升级为主的备机已经同步到原主的所有数据

4）DNS域名映射调整。将DNS域名解析的地址由机房1的IP调整为机房2的IP

5）启动机房2的manager进程

6）将GTM和DB主切换到机房2，DB切换时会对各个DBGroup停服

7）启动机房2的OMM进程

2、城市A的机房2切换到机房1

经由上述切换步骤，分布式数据库系统运行在城市A的机房2了，再次调用同城人工演练切换脚本，将系统迁回机房2。

**城市间切换步骤**

假设分布式数据库系统运行在城市A的机房1，现在要做人工演练，将系统从城市A的机房1迁到城市B的机房，接着再从城市B的机房迁回城市A的机房1。

人工演练脚本的执行步骤如下：

1、城市A的机房1切换到城市B的机房

1）禁用城市A的连接实例，为步骤2）获取快照做铺垫。

2）校验两个城市的环境。校验城市A的主机DB和城市B的逻辑主DB的数据是否一致，获取城市A的主机GTM的maxgtid。DB数据不一致或者获取不到maxgtid，那么脚本跳出不再执行。

3）停掉城市A的OMM进程。

4）停掉城市A的manager进程。

5）RDB复制关系调整。城市B的RDB升级为主机，城市A机房1的RDB切换为城市B的RDB的备机，城市A的机房2的RDB（如果有城市A的同城组网的话）切换为城市B的RDB的备机。切换前会先校验一下原来的复制关系，确保即将升级为主的备机已经同步到原主的所有数据。

6）DNS域名映射调整。将DNS域名解析的地址由城市A机房1的IP调整为城市B的机房IP。

7）启动城市B的机房的manager进程。

8）将城市A的DB主机和城市B的逻辑主DB作角色切换，两个角色互换，切换时会对各个DBGroup停服。对城市B中的novalid状态的GTM进程set maxgtid操作，将城市B的GTM升级为主机，城市A的GTM降级为novalid状态。

9）启动城市B机房的OMM进程，启动城市B的连接实例。

2、城市B的机房切换到城市A的机房1

经由上述切换步骤，分布式数据库系统运行在城市B的机房了，再次调用异地人工演练脚本，将系统迁回城市A的机房1.

 

**故障切换**

**适用场景**

机房间切换，支持只有管理节点切换到另一个机房、DBCluster的某一或者所有分片切换到另一机房、GTM切换到另一机房，支持管理节点、DB分片、GTM中的某两个或者机房整体切换到另一个机房的场景。其中，DB分片和GTM故障切换到另一机房分别由CM和MDS总控。管理节点切换到另一机房，需要手动操作，机房整体切换，也需要人工干预。

城市间切换由两个前提，停止业务和城市A整体瘫痪。不支持DB某一分片，或者GTM，或者管理节点单独切换到另一个城市。

 

**同城机房切换**

假设分布式数据库系统运行在城市A的机房1，突然该机房发生了意外（断网、断电、火灾等），机房整体瘫痪无法提供服务了，业务也无法提供，此时需要将系统迁移到城市A的机房2。

步骤如下：

1、RDB复制关系调整。将机房2的RDB升级为主RDB，城市B的RDB切换为机房2的RDB备机（如果存在异地组网）。

2、DNS域名映射调整，将DNS域名解析的地址由城市A的机房1的IP调整为机房2的IP。

3、启动机房2的manager和OMM进程。

4、如果DBCluster切换策略配置了不可自动切换同城，那么需要人工干预，执行dbtool命令将机房2的DB升级为主机。如果配置的是可以自动切换到同城，那么CM进程启动后，就会自动选择机房2的DB升级为主机。

5、如果GTM切换策略配置了不可自动切换到同城，那么需要人工干预，执行dbtool命令将机房2的GTM升级为主机。如果配置的是可以自动切换到同城，那么MDS进程启动后，会自动选择机房2的GTM升级为主。

6、启动Proxy进程，启用连接实例，恢复业务。

7、机房1恢复。

机房1的DB恢复后，如果存在差异化binlog，该DB接入新主时是会自动回滚的，不会影响接入，也不需要人工干预。

这里介绍一下差异化binlog出现的原因，原主宕机时，有数据的binlog已经落盘，但是没有来得及同步给备机，也就是这部分数据没有commit成功，对用户来说是失败的，所以新主接管原主恢复后回滚掉这部分数据，也跟用户的感知一致。

8、调用机房间人工演练脚本将系统迁回机房1。

 

**异地机房切换**

假设分布式数据库系统运行在城市A，突然城市A发生了地震，导致城市A的两个机房都瘫痪了，业务无法提供，这个时候需要将系统迁到城市B。

大致步骤和原理：

1、城市B接管

1）RDB复制关系调整，将城市B的RDB升级为主机RDB。

2）DNS域名映射调整，将DNS域名解析的地址由城市A机房的IP调整为城市B机房的IP。

3）启动城市B的manager和OMM进程。

4）获取城市B逻辑主上的T0位置。这里主要从心跳表中获取一个离故障发生时刻最近的心跳数据全局一致性的时刻，以及该时刻的活跃GTID以及最大GTID。

假设故障时刻，异地的三个分片同步到的数据不一致，分别对应T1、T2、T3时刻，那么该步骤获取到的T0时刻就是min{T1,T2,T3}。所以说，心跳表的数据不申请GTID，可能导致的故障时刻城市A的心跳数据分布不一致。分布式数据库异地灾备方案能够容忍。因为各分片同步也会有差异，切换最终找到的T0时刻一定是分布式一致的。这样，最多对切换的精度有一定的影响，但并不是说，心跳数据申请GTID了，异地接管的精度就一定能够提升。不是的，主要还是看异地的同步情况。

5）将城市B的逻辑主回滚到T0时刻，并做purge。

6）将城市B的逻辑主升级为主机。

7）将异地的GTM升级为主机。

8）启用异地proxy的连接实例，恢复业务。

2、城市A回顾

城市A的DB恢复后，如果数据比当前城市B的新主多，这里差异化数据来源有所不同，主要是城市B的逻辑主（异步复制）没有同步到城市A的主机数据，主机上多的数据对于用户而言是执行成功了的。那么需要对这些差异化数据作分析，是否需要执行，如何执行，分布式数据库系统无法提供解决方案，局点根据业务情况自行分析，因为一般局点业务都是加密的，我们无法分析。

3、调用城市间人工演练脚本将系统迁回城市A

##### DB数据节点异常切换

ClusterManager根据以下条件之一判断DB主节点异常：

1、主节点的DBagent和ClusterManager链接断开（默认是丢3个心跳）

2、主节点上所有备节点和主节点的IO线程断开链接

当ClusterManager判断DB数据主节点出现异常的时候，会触发DB切换流程：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC920.tmp.jpg) 

1、将主节点停止服务，将该分片所有的DB设置为只读，等待所有备机回放完成（默认60s）

2、通过脚本获得高低水位信息

3、获取所有备机的GTID，超过60s则认为备机异常，设置该备机异常

4、当出现多点故障时，需判断

1）低于低水位时，DB不能故障切换

2）高低水位之间，存在FAM_S或FAM_D的DB不能切换

3）高水位时，存在多个FAM_D不能切换

5、如果可以切换则进行下一步，否则不满足一致性切换则切换失败

6、具备切换条件后，将GTID最大的节点置为新主，并解除只读，修正备机的主备关系

注：DB数据节点切换优先切换本地节点，如果跨机房切换设置没打开，当本地副本不可用时，切换会失败。

 

### **容灾**

#### **衡量指标**

​	在容灾恢复方面，目前业界公认有三个目标值。

1、 恢复时间：企业能够忍受多长时间没有IT，处于停业状态；

2、 网络多长时间能够恢复；

3、 业务层面的恢复。

整个恢复过程中，最关键的衡量指标有两个：一个是RTO，一个是RPO。

所谓RTO，Recovery Time Object，它是指灾难发生后，从IT系统当前导致业务停顿开始，到IT系统恢复至可以支持各部门运作、恢复运营之时，此两点之间的时间段称为RTO。

所谓RPO，Recovery Point Object，是指从系统和应用数据而言，要实现能够恢复至可以支持各部门业务运作，系统及生产数据应恢复到怎么样的更新程度。这种更新程度可以是上一周的备份数据，也可以是上一次交易的实时数据。

#### **同城双活**

##### 概述

同城容灾的主要功能是：

1、本地机房宕机，管理节点通过手工切换到同城机房，其他节点（GTM、DB）由管理节点自动切换到同城备机房；

2、人工演练需要支持一键式机房切换；

3、支持当前主GTM异常情况下重新选主功能；

4、支持当前主DB异常情况下重新选主功能。

##### 架构

主站机房和同城机房之间采用专线网络。

管理节点采用域名或VIP访问方式，元数据通过***\*半同步复制\****。

GTM采用一主多从形式，由MDS选主，GTM采用分组管理，主从间采用消息同步。

DB采用同步方式，对从机采用分组管理。

主机异常需主从切换时，优先选择本地机房的节点为主，本地机房没有可选节点则选择同城机房的节点。

 

##### 设计思路

###### 同城故障容灾

本地机房宕机后，管理节点MDS，CM手工启动，CM通过域名或者VIP与管理节点连接。

同城机房MDS启动后，会对原主GTM进行监控，如果链路异常，则发起GTM异常切换。

同城机房CM启动后，会对原主DB进行健康，如果异常，则发起DB异常切换。

###### 管理节点容灾

本地机房与同城机房间元数据使用DB半同步复制，本地机房宕机后，同城机房管理节点手动启动（HA方式自动切换），非管理节点通过域名或VIP方式连接管理节点。

###### GTM异常切换

主GTM异常后MDS等三个心跳周期，如果MDS与主GTM链路依旧异常，则认为主GTM故障，MDS根据所有slave GTM上报的信息选举出新的master GTM 。

**时序图**

**PM/CM处理禁用原主GTM请求时宕机**

**本地机房宕机且同城机房配置不可切换**

**补数据过程中临时主机宕机**

**补数据过程中本地机房从GTM宕机**

**新主GTM向从GTM全量同步从GTM宕机**

**MDS广播新主GTM时网络抖动**

**新主GTM启服过程中宕机**

**从GTM宕机**

###### GTM手工切换

GTM手工切换，选择一个可用的slave GTM即可进行主从关系的切换。

**时序图**

**PM/CM处理禁用原主GTM请求时宕机**

**原主GTM向新主GTM全量同步过程中新主GTM宕机**

**原主GTM向新主GTM全量同步过程中原主GTM宕机**

**MDS向从GTM广播新主GTM时网络抖动**

###### DB异常切换

Agent每隔5s（可配置）发送心跳数据，CM根据心跳数据确定DB是否处于故障状态，如果DB处于故障状态，并且这个DB是主DB，那么就认为发生异常需要进行DB切换。

**时序图**

**主机宕机**

向MDS查询是否存在一致性副本，不存在则流程结束。反之则CM获取所有从机同步位置，获取同步最新主机，判断最新主机所在机房位置：

1、若在本地机房则直接切为主机

2、若在同城机房并且允许切换到同城机房，则直接切为主机

3、不可切则补数据完成后选择本地一致性从机切换为主机

 

**主机Agent宕机**

dbmoni会定时将DBAgent拉起来，如果拉起失败，出现处理方法同主机宕机。

 

**主机DB宕机**

DBAgent会监控DB状态，发现DB不存在会拉起DB；如果拉起失败，出现处理方法同主机宕机。

 

**从机宕机**

CM告警DB状态。

快同步根据高低水位配置判断是否需要告警，同时决定是否服务降级。

如果在高水位之上（包括高水位），主DB正常服务。

如果在高低水位之间（不包括高水位，也不包括低水位），主DB正常服务且告警。

如果在低水位之下（包括低水位），主DB停止服务。

 

**从机Agent宕机**

dbmoni会定时将DBAgent拉起来，如果拉起失败，出现处理方法同从机宕机。

 

**从机DB宕机**

DBAgent会监控DB状态，发现DB不存在会拉起DB；如果拉起失败，出现处理方法同从机宕机。

 

**本地机房宕机**

管理节点切换为同城机房，数据节点按照配置（是否可以自动切换为同城）切换；若DB不能切换到同城，则通过dbtool命令在CM上强制指定主，实现主从切换。

 

**补数据过程中临时主机宕机**

在不允许自动切换同城的条件下，如果存在数据一致的数据节点，则进行补数据，选出新主机，否则主从切换失败。

 

**补数据过程中普通从机宕机**

同从机宕机。

 

**从机切换主机切换过程中宕机**

重新开始故障切换流程。

 

**从机指向新主（重建复制关系）过程中宕机**

同从机宕机。

 

**从机切换后，原故障主恢复重新加入**

优先自动接入，接入失败告警提示需要人工干预。

自动接入大致流程：

1、CM从新主获取已执行的GTID集合，并发送给原主机

2、原主机查询出自己已执行的GTID集合，与新主已执行你哥的GTID集合进行过滤，获取出待回滚的GTID

3、如果存在待回滚的GTID，则原主机需将待回滚的GTID的数据回滚掉，否则直接接入

注：多GTID场景支持自动截断并设置GTID_PURGED，并接入到主机；如果此时接入主机报错说明缺少binlog，需要人工干预恢复。

 

**故障从机恢复**

 

###### DB手工切换

DB手工切换，分别获取新、旧主DB GTID。比较旧主GTID和新主GTID，如果新主GTID不小于旧主GTID，则进行切换，否则不允许切换。

**时序图**

##### 切换方案

GoldenDB同城容灾方案：

全集群方案：同一个集群跨数据中心部署，将集群中一主多备中主部署在主中心，备部署在备中心。

异常时优先本地中心切换，切换过程对应用全透明。

数据节点和事务节点采用高低水位机制和分组管理，实现业务安全等级的灵活配置。

数据节点和管理节点间通过快同步方式进行复制，提高同城数据同步性能，确保同城环境高可靠性，实现同城RPO=0。

全局事务节点通过消息实时同步。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC921.tmp.png) 

注：

管理节点没有采用数据节点和事务节点那样的分组管理方式，原因是元数据的变动一般来说是比较少的，同步数据量少频率也不高，没有必要采用复杂的管理方式，只需要保证数据不丢失即可。

主管理节点异常时，由本地的双机软件+浮动IP的方式将备机进程拉起，同城之间采用域名的方式访问和发现管理节点，同城机房不支持自动切换。原因有两个：

1、管理节点不会对业务产生影响；

2、机房之间的切换是很谨慎的事情。

数据节点采用分组管理的方式，将同城机房的两个备机组成一个组，主机房一主一备组成一个组，数据通过快同步复制到备机，只要低水位配置成2就可以保证发生机房级别的灾难时业务数据不丢失。发生异常的优先切换到本地机房的服务器。

GTM事务节点采用的方法和数据节点类似，也是分组管理的方法，只是同步数据的方式不一样，GTM采用的是***\*消息持久化\****的方式来同步数据。

总结一下，我们的同城方案，通过分组管理和快同步的机制，能够保证RPO=0，同时性能接近异步复制的性能，配合高低水位的管理机制，业务能够灵活地在高性能和高一致性之间权衡。

##### 适用场景

同城切换就是两中心之间容灾环境的切换，体现容灾环境的高可用能力，以及服务切换的时效性和数据的准确性。

本方案适用场景包括计划切换和故障切换两大类：

1、计划切换

1）本地和同城机房的整体演练性切换；

2）本地和同城机房的维保性切换，比如机房级大修，主机处理性能下降，设备升级或扩容；

3）软件版本在线升级过程中的切换。

2、故障切换

1）同城容灾环境下，某机房发生整体性故障切换，比如断电断网；

2）同城容灾环境下，生产机房管理节点异常，切换到同城。

 

为了方便描述，本方案对三中心机房进行简化定义：

A机房：本地机房

B机房：同城机房

C机房：异地机房

有计划的同城切换，可以选择合适的时间点，例如业务量比较少的凌晨，切换前可以不停应用。

当本地A机房管理节点或整个机房突然出现意料之外的宕机，此时需要启动同城故障切换。

若只有非管理节点的组件发生故障时，比如本地两个GTM均异常时，会自动把主切换到同城GTM上；数据分片当主DB异常时才会发生自动切换，会在同分片选择合适的备DB升为主DB；类似以上这些情况，不再同城故障切换的范畴。

当有计划地准备回切，或者A机房修复好以后，可以选择合适的时机，从B机房回切到A机房，其切换流程与同城计划切换类似。

同城切换，可以按照下述的步骤顺序执行来完成切换，但在实际使用中，多采取事先编排好步骤来自动化完成。

 

###### 计划/人工切换

切换思路：

1、同城计划切换包括业务、所有的管理节点、DB和GTM主节点均切换，切换过程中应用不停；

2、为了尽量减少停服时间，优先管理节点切换，管理节点切换完成后再执行DB和GTM切换；

3、切换之前需要先做巡检，确保集群正常；

4、对于银行等金融机构核心业务而言，验收标准是RTO<=5分钟，RPO=0。

 

执行步骤：

1、管理节点切换（不停服）

1）停止A机房管理节点HA服务

2）切换B机房RDB为主，重建主备关系

3）智能DNS切换

4）启动B机房管理节点HA服务

5）检查B机房主RDB与A机房备机RDB主从状态

6）若存在异地机房C，则调整机房C的RDB复制关系（一般会做回切，不做该步骤）

2、DB和GTM切换（停服）

1）禁用集群内所有连接实例

2）检查B机房指定备DB的集群状态是否正常

3）切换B机房指定备DB为主

4）检查B机房指定GTM的状态是否正常

5）切换B机房指定GTM为主

6）启用集群内所有连接实例

 

###### 故障切换

切换思路：

1、同城故障切换优先切换管理节点

2、同城故障切换最重要步骤在于恢复集群到一致性时刻，原因是本地机房宕机的那一刻，无法确保安全组的数据均全部同步到同城机房，故需要将集群恢复到统一的心跳时间点

3、集群恢复后需要设置GTID的起始值，原因是本地机房宕机时，无法保证同城的GTM收到最新的GTID信息，需要增加一个间隔

4、对于银行等金融机构核心业务演练，验收标准是PTO<=5分钟，RPO=0

 

执行步骤：

1、智能DNS切换

2、切换B机房RDB为主，重建主备关系

3、启动B机房管理节点HA服务

4、禁用集群内所有连接实例

5、切换B 机房一致性副本备机DB为主（脚本自动选择一致性副本）

6、获取集群元数据及一致性心跳时间

7、回滚集群数据到一致性心跳时间带你

8、清除一致性回滚产生的新GTID集合

9、切换B机房一致性副本备GTM为主，设置GTID起始值

10、启用集群内所有连接实例

 

###### 同城回切本地

同城回切本地执行步骤：

1、检查B机房主RDB与A机房备RDB主从状态

2、停止B机房管理节点HA服务

3、切换A机房RDB为主，重建主备关系

4、智能DBS切换

5、启动A机房管理节点HA服务

6、检查A机房主RDB与B机房备机RDB主从状态

7、禁用集群内所有连接实例

8、检查A机房指定备DB的集群状态是否正常

9、切换A机房指定备DB为主

10、检查A机房指定GTM的状态与高低水位配置

11、切换A机房指定GTM为主

12、启用集群内所有连接实例

 

 

#### **异地多活/两地三中心**

两地三中心高可用架构：

1、主机房和同城机房之间采用快同步复制，提升性能保证事务一致性；

2、主机房与异地机房之间采用异步复制方式，主要是考虑性能；

3、管理节点采用HA热备方式（现在方案采用ZK）

##### 异地容灾方案

部署两套同构集群，易于部署运维、数据异步复制，PRO<1分钟

1、支持快同步复制或异步复制

2、支持两地三中心可视化运维管理

3、支持孤岛演练

4、保证异地机房的数据最终一致

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC931.tmp.png) 

核心业务是需要异地容灾系统的，以保证在发生地区级灾难时，核心业务仍可对外提供服务，在我们的异地方案中，两个数据中心部署两套分布式数据库，事务节点之间不再同步消息，而是通过心跳消息的方式，和业务数据一起异步复制到异地机房的服务器上。管理节点同样将元数据通过异步的方式同步到异地机房。

异地机房部署的服务全部处于冷备状态，一旦启动，就会从各个数据节点中找最近的心跳数据，利用心跳数据里的活跃事务列表做一致性检查，达到全局一致的状态，再对外提供服务。我们的异地容灾方案，架构简单，可以通过心跳消息快速找到全局数据的一致点，快速接管业务（目标是可以做到RPO<5s）。

 

推荐数据安全组设置：

1、数据安全组的高水位是2，低水位是1，正常运行时保证本地和同城的备机均提供响应；

2、若有一组数据库在规定时间内无法提供响应，则触发低于高水位告警，数据库正常提供服务；

3、若本地和同城备机均无法提供响应，则主机在只读模式下运行。

##### 适用场景

异地切换就是两地之间容灾环境的切换，体现容灾环境的高可用能力，以及服务切换的时效性和数据的准确性。

本方案适用的场景包括计划切换和故障切换两大类：

1、计划切换

1）本地和异地机房的整体演练性切换；

2）本地和异地机房的维保性切换，比如机房级大修，主机处理性能下降，设备升级或者扩容；

3）软件版本在线升级过程中切换。

2、故障切换

1）异地容灾环境下，某机房发生整体性故障切换，比如断电断网；

2）异地容灾环境下，生产机房管理节点异常，切到异地。

 

有计划的异地切换，可以选择合适的时间点，例如业务量较少的凌晨，需要先停应用，完成切换后再将应用切换到异地。

当A、B机房管理节点或整个机房突然出现意料之外的宕机，此时需要启动异地故障切换，完成切换后再将应用切换到异地。

若只是非管理节点的组件发生故障时，比如本地两个GTM均异常时，会自动把主切换到同城GTM上；数据分片当主DB异常时才会发生自动切换，会在同分片选择合适的备DB升为主DB；类似以上这些情况，不在异地故障切换的范围内。

当有计划的准备回切，或者A机房恢复好以后，可以选择合适的时机，从C机房回切到A机房，其切换流程与异地计划切换相类似，完成切换后再将应用切换到本地。

同城切换，可以按下述的步骤顺序执行来完成切换，但在实际应用中，多采取事先编排好步骤来自动化完成。

###### 计划切换

切换思路：

1、异地计划切换包括业务、所有的管理节点、DB和GTM主节点均切换（这个与单一组件的切换是不同的）；

2、为了尽量减少停服时间，优先管理节点切换，管理节点切换完成后再执行DB和GTM的切换；

3、切换之前需要先做巡检，确保集群正常；

4、一般银行等金融核心系统演练，验收标准都是RTO<=5分钟，RPO=0。

 

执行步骤：

1、检查A机房主RDB与C机房备RDB主从状态；

2、停止A机房管理节点HA服务；

3、切换C机房RDB为主，重建主备关系；

4、智能DNS切换；

5、启动C机房管理节点HA服务；

6、检查C机房主RDB与A机房备机RDB主从状态；

7、批量完成停服状态下C机房的GTM切换与逻辑主DB切换：

1）禁用集群内所有连接实例，停止所有proxy服务；

2）检查各个分片内数据是否一致，并获取全局事务快照；

3）切换C机房的GTM为主，并设置新的起始GTID；

4）切换C机房的逻辑主DB为主，通过城市ID进行；

5）启用集群内所有连接实例，允许所有proxy服务。

 

###### 故障切换

切换思路：

1、异地故障切换优先切换管理节点（为了减少停服时间）；

2、异地故障切换最重要步骤在于恢复集群到一致性的时刻，原因是本地机房宕机的那一刻，无法确保安全组的数据均全部同步到异地机房，故需要将集群恢复到统一的心跳时间点；

3、集群恢复后需要设置GTID的起始值，原因是本地机房宕机时，无法保证异地的GTM收到最新的GTID信息，需要增加一个间隔；

4、对于银行等金融机构核心系统演练，验收标准是RTO<10分钟，RPO<=2分钟。

切换步骤；

1、智能DNS切换；

2、切换C 机房指定备RDB为主，重建主备关系；

3、启动C机房管理节点HA服务；

4、获取集群元数据及一致性心跳时间；

5、回滚集群数据到一致性心跳时间点；

6、切换C机房逻辑主DB到主；

7、清除一致性回滚产生的新的GTID集合；

8、设置C机房GTM的GTID起始值。

 

###### 异地回切

异地回切本地执行步骤：

1、检查C机房主RDB与A机房备RDB主从关系；

2、停止C机房管理节点HA服务；

3、切换A机房RDB为主，重建主备关系；

4、智能DNS切换；

5、启动A机房管理节点HA服务；

6、检查A机房主RDB与C机房备机RDB主从关系；

7、批量完成停服状态下A机房的GTM切换与主RDB切换。

 

#### **孤岛演练**

##### 方案

目的：验证异地机房的灾备能力，在异地机房进行演练

1、可靠性高，不影响生产系统，数据依旧会进行同步

2、使用真正的灾备机进行演练，保证演练的有效性

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsC932.tmp.png) 

##### 适用场景

孤岛演练就是在异地灾备环境中，构建临时性孤岛环境，用于验证业务处理流程的上下贯通能力，以确保异地灾备环境的有效性，避免生产和同城环境同时异常场景，导致对外服务长时间中断。

孤岛演练流程总体可分为两步：

1、创建孤岛演练环境

2、恢复孤岛演练环境

##### 创建孤岛环境

创建孤岛环境的前置条件：

1、检查异地各组件是否处于正常状态

2、配置机器的演练区和同步区：在OMM页面->资源管理，选择异地机房的资源，将所有的异地备DB修改为演练区，异地的逻辑主DB不动

3、隔离演练区：将演练区连接同步区的网络断开，或采取iptables执行脚本隔离策略的方式，将演练区的组件与同步区的组件隔离

4、智能DNS切换：目的是将保持原先本地域名工作的同时，将被划入孤岛区的节点域名都指向孤岛管理节点IP

 

##### 孤岛演练流程

1、初始化孤岛环境内各组件状态

2、物理备份隔离区内RDB节点数据，用于恢复时使用

3、物理备份隔离区内DB节点数据，用于恢复时使用

4、重置隔离区RDB读写权限，原因是备机RDB只读不可写

5、创建孤岛演练环境的元数据：把本地同城的集群信息、主备关系等数据删掉，将异地备DB的元数据通过update语句修改为主DB

6、启动孤岛演练管理组件进程

7、关闭隔离区内所有DB节点自动备份BINLOG功能，不保留演练锁产生的测试数据

8、启动隔离区内GTM进程

9、回滚集群数据到一致性心跳时间点，设置GTID起始值

10、启动隔离区内OMM及所有proxy进程

 

 

 