# 存储信息

## 采集项/采集方式

| 采集项       | 采集方式                                                     |
| ------------ | ------------------------------------------------------------ |
| 磁盘性能     | 1、获取/dev/sd*的性能数据：iostate –d /dev/sd* -xk 1 2 \| grep sd* \| tail –n 12、获取目录/和/home的空间使用率aa=$(df –ma %s \| tail -1)&& aa={aa%%%%%%*}&&echo ${aa##*} |
| 目录使用情况 | du -sh                                                       |

 

## 参考指标

基本指标参考：

| 指标         | 15K SAS磁盘 | 普通SSD | PCIE-SSD |
| ------------ | ----------- | ------- | -------- |
| 延时         | 5ms         | 100us   | 30us     |
| 带宽         | 150MB/s     | 250MB/s | 700MB/s  |
| IOPS（理论） | 200         | 1500    | 6000     |
| 价格         | GB/5元      | GB/20元 | GB/100元 |
| 工作功耗     | 15W         | 5W      | 25W      |
| 空闲损耗     | 10W         | 0.1W    | 12W      |

 

| 机械硬盘类型                                          | 15K SAS硬盘 | 10000 STAT硬盘 | 7200STAT硬盘 |
| ----------------------------------------------------- | ----------- | -------------- | ------------ |
| 寻道时间                                              | 5ms         | 7ms            | 10.5ms       |
| 旋转延迟时间                                          | 2ms         | 3ms            | 4.17ms       |
| IOPS值                                                | 142         | 100            | 68           |
| IOPS1000/(寻道时间+旋转延迟+传输时间)传输时间忽略不计 |             |                |              |

 

## 相关模块

磁盘通常是计算机最慢的子系统，也是最容易出现性能瓶颈的地方，因为磁盘离CPU距离最远而且CPU访问磁盘要涉及到机械操作，比如转轴、寻轨等。访问硬盘和访问内存之间的速度差别是以数量级来计算的，就像1天和1分钟的差别一样。要监测IO性能，有必要了解一下基本原理和Linux是如何处理硬盘和内存之间的IO的。

在理解磁盘IO之前，同样我们需要理解一些概念，例如：

文件系统

VFS

文件系统缓存

页缓存page cache

缓冲区高速缓存buffer cache

目录缓存

inode

inode缓存

noop调用策略

 

# 存储性能

监控磁盘IO的主要工具：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps78A6.tmp.jpg) 

***\*使用方式：\****

查看磁盘基本使用情况：du/df/fdisk

查看系统io信息：iotop

统计io详细信息：iostat -d -x -k 1 10

查看进程级IO的信息：pidstat -d 1 -p pid

查看系统IO的请求，比如可以在发现系统IO异常时，可以使用该命令进行调查，就能指定到底是什么原因导致的IO异常perf record -e block:block_rq_issue -ag^Cperf report

 

如果在进程输出中你没有看到很大的不同，你仍然有选择查看其他东西。我会建议你在 top 输出中检查wa状态，因为大多数时间里服务器性能由于在硬盘上的高I/O读和写降低了性能。如果它很高或者波动，很可能就是它造成的。因此，我们需要检查硬盘上的I/O活动。

我们可以在Linux中使用iotop和iostat命令监控所有的磁盘和文件系统的磁盘I/O统计。

## du

du的英文原义为“disk usage”，含义为显示磁盘空间的使用情况，统计目录（或文件）所占磁盘空间的大小。该命令的功能是逐级进入指定目录的每一个子目录并显示该目录占用文件系统数据块（1024字节）的情况。若没有给出指定目录，则对当前目录进行统计。

du命令的各个选项含义如下：

-s：对每个Names参数只给出占用的数据块总数。

-a：递归地显示指定目录中各文件及子目录中各文件占用的数据块数。若既不指定-s，也不指定-a，则只显示Names中的每一个目录及其中的各子目录所占的磁盘块数。

-b：以字节为单位列出磁盘空间使用情况（系统默认以k字节为单位）。

-k：以1024字节为单位列出磁盘空间使用情况。

-c：最后再加上一个总计（系统默认设置）。

-l：计算所有的文件大小，对硬链接文件，则计算多次。

-x：跳过在不同文件系统上的目录不予统计。

 

***\*实用命令：\****

查看目录占用空间（包括子目录）：du -h 目录

磁盘空间检查（只显示总和）：***\*du -sh\****

查看某个目录下所有文件大小并排序：***\*du -sh 目录/\* | grep G| grep -n -r -k 1\****

如果需要查找大文件：***\*find 目录 -type f -size +10G\****

 

## df

df命令可以获取硬盘被占用了多少空间，目前还剩下多少空间等信息，它也可以显示所有文件系统对i节点和磁盘块的使用情况。

du查看目录大小，df查看磁盘使用情况。

df命令各个选项的含义如下：

-a：显示所有文件系统的磁盘使用情况，包括0块（block）的文件系统，如/proc文件系统。

-k：以k字节为单位显示。

-i：显示i节点信息，而不是磁盘块。

-t：显示各指定类型的文件系统的磁盘空间使用情况。

-x：列出不是某一指定类型文件系统的磁盘空间使用情况（与t选项相反）。

-T：显示文件系统类型。

 

//列出各文件系统的磁盘空间使用情况

\#df

Filesystem      1k-blocks    Used  Available Use% Mounted on

/dev/hda5        381139   332921   28540  93% /

/dev/hda1         46636    6871   37357  16% /boot

/dev/hda3       10041144  6632528  2898556  70% /home

none           127372     0   127372  0% /dev/shm

/dev/hda2       27474876  24130460  1948772  93% /usr

/dev/hda6        256667   232729   10686  96% /var

 

***\*实用命令：\****

查看磁盘剩余空间：df -hl

查看磁盘目录文件类型：df -T

查看文件系统inode使用情况：df -ih

说明：有时候虽然文件系统还是有空间，但若没有足够的inode来存放文件的信息，一样会不能增加新的文件。

所谓的inode是用来存放文件及目录的基本信息（metadata），包含时间、文件名、使用者及群组等。在分割扇区时，系统会先做出一堆inode以供以后使用，inode的数量关系着系统中可以建立的文件及目录总数。如果要存的文件大小都很小，则同样大小的硬盘中会有较多的文件，也就是说需要较多的inode来存储文件及目录。

​	

## free

$ free -m

total    used    free   shared   buffers   cached

Mem:   245998    24545   221453     83     59     541

-/+ buffers/cache:    23944   222053

Swap:   0      0      0

检查的列：

buffers: For the buffer cache, used for block device I/O.

cached: For the page cache, used by file systems.

若buffers和cached接近0，说明I/O的使用率过高，系统存在性能问题。 

Linux中会用free内存作为cache，若应用程序需要分配内存，系统能够快速的将 cache 占用的内存回收，因此free的内存包含cache占用的部分。

 

 

## fdisk

​	fdisk可以划分磁盘分区。下面给出使用fdisk命令进行磁盘分区的操作步骤：

\#fdisk /dev/had   //使用/dev/had作为默认的分区设备

Command (m for help): m  //选择命令选项

Command action

  a  toggle a bootable flag

  b  edit bsd disklabel

  c  toggle the dos compatibility flag

  d  delete a partition

  l  list known partition types

  m  print this menu

  n  add a new partition

  o  create a new empty DOS partition table

  p  print the partition table

  q  quit without saving changes

  s  create a new empty Sun disklabel

  t  change a partition's system id

  u  change display/entry units

  v  verify the partition table

  w  write table to disk and exit

  x  extra functionality (experts only)

用户通过提示键入“m”，可以显示fdisk命令各个参数的说明。

在Linux分区过程，一般是先通过p参数来显示硬盘分区表信息，然后根据信息确定将来的分区。如下所示：

Disk /dev/sda: 4294 MB, 4294967296 bytes

255 heads, 63 sectors/track, 522 cylinders

Units = cylinders of 16065 * 512 = 8225280 bytes

Device Boot   Start    End   Blocks  Id  System

/dev/hda1  *     41    522  3871665  83  Linux

/dev/hda2       1     40   321268+  82  Linux swap

Partition table entries are not in disk order

Command (m for help):

如果想完全改变硬盘的分区格式，就可以通过d参数一个一个地删除存在的硬盘分区。删除完毕，就可以通过n参数来增加新的分区。当按下“n”后，可以看到如下所示：

Command (m for help): n

Command action

  e  extended

  p  primary partition (1-4)

  p

  Partiton number(1-4):1

  First cylinder(1-1023):1

  Last cylinder or + size or +sizeK or + sizeM(1-1023):+258M

这里要选择新建的分区类型，是主分区还是扩展分区；并选择p或是e。然后就是设置分区的大小。

要提醒注意的是，如果硬盘上有扩展分区，就只能增加逻辑分区，不能增加扩展分区。

在增加分区的时候，其类型都是默认的Linux Native，如果要把其中的某些分区改变为其他类型，例如Linux Swap或FAT32等，可以通过命令t来改变，当按下“t”改变分区类型的时候，系统会提示要改变哪个分区、以及改变为什么类型（如果想知道系统所支持的分区类型，键入l），如下所示：

Command (m for help): t

Partition number (1-4): 1

Hex code (type L to list codes): 82

Changed system type of partition 1 to 82 (Linux swap)

改变完了分区类型，就可以按下“w”，保存并退出。如果不想保存，那么可以选择“q”直接退出，如下所示：

Command (m for help)：w

通过如上的操作，就可以按照需要成功地划分磁盘分区了。

 

## lsof

如果磁盘撑爆，可以通过lsof查看是否是产生了太多的临时文件。

参考：

https://www.cnblogs.com/paul8339/p/11983747.html

http://mp.weixin.qq.com/s?__biz=MjM5NzAzMTY4NQ==&mid=2653934523&idx=1&sn=bb5bac6334d91444676d79e9aeb3264b&chksm=bd3b49d18a4cc0c7af2b1c90e92c4241984ca87d9c8197379b868741e341d2bf9494857b9f34&mpshare=1&scene=24&srcid=0204YeySSwh5ubyo9VCQJdNH&sharer_sharetime=1612424265733&sharer_shareid=33f795d236f19ac7c128b2e279563f84#rd

 

## top

综合指令，可以观察系统负载，CPU，内存，进程大概情况。

## dstat

***\*格式：\****dstat [-afv] [options..] [delay [count]]

***\*参数：\****

t：时间戳

c：CPU使用情况

d：disk读写量

r：io请求统计，包括read requests/write requests

m：内存统计

n：网络流量统计

s：swap统计

p：分页统计

y：系统统计

***\*常用指令：\****dstat -tcdrmnsgy

 

***\*说明：\****直接使用dstat，默认使用的是-cdngy参数，分别显示cpu、disk、net、page、system信息。

| 分组     | 分组含义及子项字段含义                                       |
| -------- | ------------------------------------------------------------ |
| CPU状态  | CPU的使用率。显示了用户占比，系统占比、空闲占比、等待占比、硬中断和软中断情况。 |
| 磁盘统计 | 磁盘的读写，分别显示磁盘的读、写总数。                       |
| 网络统计 | 网络设备发送和接受的数据，分别显示的网络收、发数据总数。     |
| 分页统计 | 系统的分页活动。分别显示换入（in）和换出（out）。            |
| 系统统计 | 统计中断（int）和上下文切换（csw）。                         |

 

## sar

## vmstate

## pidstat

pidstat是sysstat工具的一个命令，用于监控全部或指定进程的cpu、内存、线程、设备IO等系统资源的占用情况。pidstat首次运行时显示自系统启动开始的各项统计信息，之后运行pidstat将显示自上次运行该命令以后的统计信息。用户可以通过指定统计的次数和时间来获得所需的统计信息。

pidstat [ 选项 ] [ <时间间隔> ] [ <次数> ]

常用的参数：

-u：默认的参数，显示各个进程的cpu使用统计

-r：显示各个进程的内存使用统计

-d：显示各个进程的IO使用情况

-p：指定进程号

-w：显示每个进程的上下文切换情况

-t：显示选择任务的线程的统计信息外的额外信息

-T { TASK | CHILD | ALL }

这个选项指定了pidstat监控的。TASK表示报告独立的task，CHILD关键字表示报告进程下所有线程统计信息。ALL表示报告独立的task和task下面的所有线程。

***\*注意：\****task和子线程的全局的统计信息和pidstat选项无关。这些统计信息不会对应到当前的统计间隔，这些统计信息只有在子线程kill或者完成的时候才会被收集。

-V：版本号

-h：在一行上显示了所有活动，这样其他程序可以容易解析。

-I：在SMP环境，表示任务的CPU使用率/内核数量

-l：显示命令名和所有参数

 

## iostate

### 概述

iostat被用来报告中央处理单元（CPU）的统计和设备与分区的输出/输出的统计。

iostat命令通过观察与它们平均传输率相关的设备活跃时间来监控系统输入/输出设备负载。

iostat命令生成的报告可以被用来改变系统配置来更好的平衡物理磁盘之间的输入/输出负载。

所有的统计都在iostat命令每次运行时被报告。该报告包含一个CPU头部，后面是一行CPU统计。

在多处理器系统中，CPU统计被计算为系统层面的所有处理器的平均值。设备头行后紧跟显示每个配置的设备一行的统计。

iostat命令生成两种类型的报告，CPU利用率报告和设备利用率报告。

 

***\*iostate有一个弱点，就是它不能对某个进程进行深入分析，进队系统的整体情况进行分析\*******\*。\****

***\*注：如果需要查看具体进程线程的I/O信息，可以使用sar/pidstat。\****

 

### 使用

在iostat命令中有很多参数来检查关于I/O和CPU的变化统计信息。

不加参数运行iostat命令会看到完整的系统统计。

-c：仅显示CPU统计信息，与-d选项互斥

**-d**：仅显示磁盘统计信息，查看所有设备的I/O统计，与-c选项互斥

-d [设备名]：查看具体设备和它的分区的I/O统计信息

-k：以K为单位显示每秒的磁盘请求数，默认单位块

-m：以MB为单位而不是KB查看所有设备的统计，默认以KB显示输出

**-p**：查看所有的设备和分区的I/O统计

-t：在输出数据时，打印搜集数据的时间

-v：打印版本号和帮助信息

**-x**：显示所有设备的详细的I/O统计信息

**-N**：查看LVM磁盘I/O统计报告

***\*常见用法：\****

iostat -d -k 1 10	#查看TPS和吞吐量信息

iostat -d -x -k 1 10	#查看设备使用率（%util）、响应时间（await）

 

***\*CPU相关输出含义：\****

%user：CPU处在用户模式下的时间百分比。

%nice：CPU处在带NICE值的用户模式下的时间百分比。

%system：CPU处在系统模式下的时间百分比。

%iowait：CPU等待输入输出完成时间的百分比（***\*最大阈值40%~50%\****）。

%steal：管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比。

%idle：CPU空闲时间百分比。

***\*说明：\****

1、如果%iowait的值过高，表示硬盘存在I/O瓶颈

2、如果%idle值过高，表示CPU较空闲

3、如果%idle值高但系统响应慢时，可能是CPU等待分配内存，应加大内存容量

4、如果%idle值持续低于10，表示CPU处理能力相对较低，系统中最需要解决的资源是CPU

***\*磁盘相关输出含义（iostat -dx）：\****

rrqm/s：每秒进行merge的读操作数目，即delta(rmerge)/s

wrqm/s：每秒进行merge的写操作数目，即delta(wmerge)/s

r/s：每秒完成的读I/O设备次数，即delta(rio)/s

w/s：每秒完成的写I/O设备次数，即delta(wio)/s

rsec/s：每秒读扇区数，即delata(rsect)/s

wsec/s：每秒写扇区数，即delta(wsect)/s

avgrq-sz：平均每次设备I/O操作的数据大小（扇区），即delta(rsect+wsect)/delta(rio+wio)

avgqu-sz：平均I/O队列长度，即delta(aveq)/s/1000（因为aveq的单位为毫秒）

await：平均每次设备I/O操作的等待时间（毫秒），即delta(ruse+wuse)/delta(rio+wio)

svctm：平均每次设备I/O操作的服务时间（毫秒），即delta(use)/delta(rio+wio)

%util：一秒中有百分之多少的时间用于I/O操作，或者说一秒钟有多少时间I/O队列是非空的，即delta(use/s/1000)（因为use的单位为毫秒）

 

## iotop

### 概述

​	iotop 是一个类似top的工具，用来显示实时的磁盘活动。

iotop 监控Linux内核输出的I/O使用信息，并且**显示一个系统中进程或线程的当前 I/O使用情况**。

它**显示每个进程/线程读写I/O带宽**。它同样显示当等待换入和等待I/O的线程/进程花费的时间的百分比。

Total DISK READ和Total DISK WRITE的值一方面表示了进程和内核线程之间的总的读写带宽，另一方面也表示内核块设备子系统的。

Actual DISK READ和Actual DISK WRITE的值表示在内核块设备子系统和下面硬件（HDD、SSD等等）对应的实际磁盘I/O带宽。

### 使用

iotop 命令有很多参数来检查关于磁盘I/O的变化：

\# iotop

如果你想检查那个进程实际在做I/O，那么运行iotop命令加上-o或者--only参数。

\# iotop –only

◈ IO：它显示每个进程的I/O利用率，包含磁盘和交换。

◈ SWAPIN：它只显示每个进程的交换使用率。

 

运行iotop查看进程对磁盘的读写：

[root@localhost] $ iotop  #查看全部进程的磁盘读写情况

[root@localhost] $ iotop -o  #实时查看当前进程对磁盘的读写（推荐）

[root@localhost] $ iotop -p 34323  #查看进程号为34323对磁盘的读写情况

监控告警可以使用如下命令获取io的数据

[root@localhost] $ iotop -botqqq --iter=3

 

## fio

### 概述

[root@localhost] $ yum install fio

ioengine: 负载引擎，我们一般使用libaio，发起异步IO请求。

bs: IO大小

direct: 直写，绕过操作系统Cache。因为我们测试的是硬盘，而不是操作系统的Cache，所以设置为1。

rw: 读写模式，有顺序写write、顺序读read、随机写randwrite、随机读randread等。

size: 寻址空间，IO会落在 [0, size)这个区间的硬盘空间上。这是一个可以影响IOPS的参数。一般设置为硬盘的大小。

filename: 测试对象

iodepth: 队列深度，只有使用libaio时才有意义。这是一个可以影响IOPS的参数。

runtime: 测试时长

 

### 参数

filename=/dev/emcpowerb支持文件系统或者裸设备，-filename=/dev/sda2或-filename=/dev/sdb

direct=1         测试过程绕过机器自带的buffer，使测试结果更真实

rw=randwread       测试随机读的I/O

rw=randwrite       测试随机写的I/O

rw=randrw         测试随机混合写和读的I/O

rw=read          测试顺序读的I/O

rw=write         测试顺序写的I/O

rw=rw           测试顺序混合写和读的I/O

bs=4k           单次io的块文件大小为4k

bsrange=512-2048     同上，提定数据块的大小范围

size=5g          本次的测试文件大小为5g，以每次4k的io进行测试

numjobs=30        本次的测试线程为30

runtime=1000   测试时间为1000秒，如果不写则一直将5g文件分4k每次写完为止

ioengine=psync  io引擎使用pync方式，如果要使用libaio引擎，需要yum install libaio-devel包

rwmixwrite=30    在混合读写的模式下，写占30%

group_reporting    关于显示结果的，汇总每个进程的信息

lockmem=1g      只使用1g内存进行测试

zero_buffers       用0初始化系统buffer

nrfiles=8         每个进程生成文件的数量

***\*测试场景：\****

100%随机，100%读，4K

fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=rand_100read_4k

 

100%随机，100%写，4K

fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=rand_100write_4k

 

100%顺序，100%读，4K

fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=sqe_100read_4k

 

100%顺序，100%写，4K

fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=sqe_100write_4k

 

100%随机，70%读，30%写4K

fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=randrw_70read_4k

 

***\*测试场景：\****

4K随机写测试：

[root@localhost] $ fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randwrite -size=100G -filename=/dev/vdb

-name="EBS 4KB randwrite test" -iodepth=32 -runtime=60

4K随机读测试：

[root@localhost] $ fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randread -size=100G -filename=/dev/vdb

-name="EBS 4KB randread test" -iodepth=8 -runtime=60

512KB顺序写测试：

[root@localhost] $ fio -ioengine=libaio -bs=512k -direct=1 -thread -rw=write -size=100G -filename=/dev/vdb

-name="EBS 512KB seqwrite test" -iodepth=64 -runtime=60

**mount**

磁盘挂载属性：mount -l

**lsblk**

磁盘设备显示：lsblk

 

## dd

磁盘IO测试（dd）

测试磁盘的IO写速度：

time dd if=/dev/zero of=test.dbf bs=8k count=300000 oflag=direct

测试磁盘的IO读速度：

dd if=test.dbf bs=8k count=300000 of=/dev/null

表示每次写入/读取8k的数据，执行300000次

 

# 存储IO

生产中经常遇到一些IO延时长导致的系统吞吐量下降、响应时间慢等问题，例如交换机故障、网线老化导致的丢包重传；存储阵列条带宽度不足、缓存不足、QoS限制、RAID级别设置不当等引起的IO延时。

参考：

https://blog.csdn.net/luyegang1/article/details/74453879

 

## IO模型

在实际的业务处理过程中，一般来说IO比较混杂，比如说读写比例、IO尺寸等等，都是有波动的。所以我们提炼IO模型的时候，一般是针对某一个特定的场景来建立模型，用于IO容量规划以及问题分析。

最基本的模型包括

IOPS

带宽

IO的尺寸（大小）

如果是磁盘IO，那么还需要关注：

磁盘IO分别在哪些盘

读IO和写IO的比例

读IO是顺序的还是随机的

写IO是顺序的还是随机的

 

为什么要提炼IO模型

不同模型下，同一台存储，或者说同一个LUN，能够提供的IOPS、带宽（MBPS）、响应时间3大指标的最大值是不一样的。

当存储中提到IOPS最大能力的时候，一般采用随机小IO进行测试，此时占用的带宽是非常低的，响应时间也会比顺序的IO要长很多。如果将随机小IO改为顺序小IO，那么IOPS还会更大。当测试顺序大IO时，此时带宽占用非常高，但IOPS却很低。

因此，做IO的容量规划、性能调优需要分析业务的IO模型是什么。

 

## 评估工具

磁盘IO评估工具

磁盘IO能力的评估工具有很多，例如orion、iometer，dd、xdd、iorate，iozone，postmark，不同的工具支持的操作系统平台有所差异，应用场景上也各具特色。

有的工具可以模拟应用场景，比如orion是oracle出品，模拟Oracle数据库IO负载（采用与Oracle相同的IO软件栈）。即模拟oracle应用对文件或磁盘分区进行读写（可指定读写比例、io size，顺序or随机）这里就需要提前知道自己的IO模型。如果不知道，可以采用自动模式，让orion自动的跑一遍，可以得出不同进程的并发读写下，最高的IOPS、MBPS，以及对应的响应时间。

比对dd，仅仅是对文件进行读写，没有模拟应用、业务、场景的效果。

postmark可以实现文件读写、创建、删除这样的操作。适合小文件应用场景的测试。

## 监控指标

磁盘IO

对于存储IO：unix、linux平台，Nmon、iostat是比较好的工具。

nmon用于事后分析，iostat可用于实时查看，也可以采用脚本记录下来事后分析。

### IOPS

总IOPS：Nmon DISK_SUMM Sheet：IO/Sec

每个盘对应的读IOPS ：Nmon DISKRIO Sheet

每个盘对应的写IOPS ：Nmon DISKWIO Sheet

总IOPS：命令行iostat -Dl：tps

每个盘对应的读IOPS ：命令行iostat -Dl：rps

每个盘对应的写IOPS ：命令行iostat -Dl：wps

### 带宽

总带宽：Nmon DISK_SUMM Sheet：Disk Read KB/s，Disk Write KB/s

每个盘对应的读带宽：Nmon DISKREAD Sheet

每个盘对应的写带宽：Nmon DISKWRITE Sheet

总带宽：命令行iostat -Dl：bps

每个盘对应的读带宽：命令行iostat -Dl：bread

每个盘对应的写带宽：命令行iostat -Dl：bwrtn

### 响应时间

每个盘对应的读响应时间：命令行iostat -Dl：read - avg serv，max serv

每个盘对应的写响应时间：命令行iostat -Dl：write - avg serv，max serv

### 其他

磁盘繁忙程度、队列深度、每秒队列满的次数等等。

# 故障定位分析

## 磁盘空间不足

参考：

[https://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&mid=2247493935&idx=2&sn=f854015c2f52b818e66dbab3417b41ac&chksm=ea77dd97dd00548137aaa61ec4e4bc1328ebb570e7870047e168ae2c372bae620444f862064f&mpshare=1&scene=24&srcid=0402djXI93H5TkRigXKYbxJo&sharer_sharetime=1617325744036&sharer_shareid=33f795d236f19ac7c128b2e279563f84#rd](#rd)

### 问题发现

最近在测试遇到一个问题，容器日志过大导致系统磁盘爆满，造成的影响就是该服务器的一些服务挂掉了。日志过大，解决方法就是删，直接定位到容器日志位置发现高达5G，但是仍然显示磁盘空间已满，但是du -sh命令也显示文件夹下为空。经过百度得知，***\*容器日志不能直接rm，要通过cat /dev/null > {log文件}方式将日志删除\****。***\*因为该文件被进程所引用，直接删除并不能擦除磁盘上的文件block信息，解决方式就是停止进程\****。

### 设计复现

由于容器也属于一种进程，引用文件的方式并没有不同于其他普通进程，所以就使用一个简单的demo复现。

首先进入一个目录，以/tmp/testfile目录为例，可以看到我的服务器还有2G的剩余空间：

[centos@** testfile]$ cd /tmp/testfile
	[centos@** testfile]$ df -h .
	文件系统    容量 已用 可用 已用% 挂载点
	/dev/vda1    50G  49G  2.0G  97% 	/

然后生成一个随机文件，再来查看剩余空间，看到还剩余1014M空间：

[centos@** testfile]$dd if=/dev/urandom of=/tmp/testfile/delfiletest  bs=1M count=1000

记录了1000+0 的读入
	记录了1000+0 的写出
	1048576000字节(1.0 GB)已复制，8.31491 秒，126 MB/秒
	[centos@** testfile]$
	[centos@** testfile]$ df -h .
	文件系统   容量 已用 可用 已用% 挂载点
	/dev/vda1   50G  49G 1014M  99%	 /

启动一个程序，引用该文件，不退出进程

func main() {
		file, err := os.Open("/tmp/testfile/delfiletest")
		defer file.Close()
		if err != nil{
			fmt.Println("open err :",err.Error())
			return
  		}
  		time.Sleep(100*time.Minute)
	}

接下来删除该文件文件，查看磁盘占用情况，看到虽然文件删除，但是磁盘空间并没有释放。

[centos@** testfile]$ rm -rf delfiletest
	[centos@** testfile]$ df -h .
	文件系统    容量 已用 可用 已用% 挂载点
	/dev/vda1    50G  49G 1015M  99% /

解决方法，找到引用该文件的进程，并停止该进程，可以Ctrl+C停止，也可以kill命令停止。

[centos@** testfile]$ lsof | grep deleted|grep delfile
	lsof: WARNING: can't stat() proc file system /run/docker/netns/default
	Output information may be incomplete.
	......
	testdelet 29604 29608 centos  3r   REG  253,1 1048576000 58925156 /tmp/testfile/delfiletest (deleted)
	......
	[centos@** testfile]$ kill 29604

停止后再次查看磁盘空间，发现磁盘已经释放。

[centos@** testfile]$ df -h .
	文件系统    容量 已用 可用 已用% 挂载点
	/dev/vda1    50G  49G  2.0G  97% /

重做一次上面的步骤，这次我们使用正确的方式释放磁盘的空间，可以看到磁盘空间腾出来了。

[centos@** testfile]$ cat /dev/null > /tmp/testfile/delfiletest
	[centos@** testfile]$
	[centos@** testfile]$ df -h .
	文件系统    容量 已用 可用 已用% 挂载点
	/dev/vda1    50G  49G  2.0G  97% /

 

### 原理分析

在Linux上，每个文件都有一个自己对应的索引节点即inode，在这个inode里记录了文件在磁盘的块信息，以及链接数量等信息，一个文件在是否要被真正删除释放空间，取决于两个值，一个是i_count ，代表引用计数；一个是i_nlink，代表硬链接数量，只有当两个都为0，文件才会真正释放。

struct inode{
		atomic_t i_count;
		unsignet int i_nlink;
		......
	};

当有进程使用该文件时候，i_count就会加1，当进程不在引用或进程结束，就会减一。

硬链接也是如此，当为文件创建一个硬链接时，i_nlink就会加一，删除就会减一，当减少为0时候，就会删除文件，释放空间。

在Linux中，硬链接指的是文件名与inode的链接，通常创建一个文件对应一个硬链接，我们可以手动通过ln命令或者程序触发link系统调用为一个文件创建一个硬链接，相当于两个文件名对应了同一个磁盘文件，两个都删除才会删除磁盘文件(没有进程引用的前提下)。而Linux的rm命令相当于执行了unlink系统调用，会使得i_nlink数量减1。

在上述示例中，由于文件并没有被真正删除，所以该文件是可以恢复的，只需要找到进程的pid，并进入/proc/{pid}/fd 中，找到对应的文件描述符，执行cp命令复制即可找回文件。

 

## IO问题排查

我们使用stress -i 1来模拟IO瓶颈问题，即死循环执行sync刷盘操作：  

uptime：使用uptime查看此时系统负载：

$ watch -d uptime

load average: 1.06, 0.58, 0.37

mpstat：查看此时 IO 消耗，但是实际上我们发现这里 CPU 基本都消耗在了 sys 即系统消耗上。

Average:CPU %usr %nice %sys %iowait  %irq  %soft  %steal  %guest  %gnice %idle

Average:  all  0.33  0.00  12.64  0.13  0.00  0.00  0.00  0.00  0.00  86.90

Average: 0   0.00  0.00  99.00  1.00  0.00   0.00   0.00   0.00   0.00   0.00

Average: 1   0.00  0.00  0.33  0.00   0.00   0.00   0.00   0.00   0.00  99.67

IO无法升高的问题：iowait 无法升高的问题，是因为案例中stress使用的是sync()系统调用，它的作用是刷新缓冲区内存到磁盘中。

对于新安装的虚拟机，缓冲区可能比较小，无法产生大的IO压力，这样大部分就都是系统调用的消耗了。所以，你会看到只有系统CPU使用率升高。

解决方法是使用stress的下一代stress-ng，它支持更丰富的选项，比如stress-ng -i 1 --hdd 1 --timeout 600（--hdd 表示读写临时文件）。

Average: CPU %usr %nice %sys %iowait  %irq  %soft  %steal %guest  %gnice  %idle

Average: all  0.25  0.00  0.44  26.22 0.00  0.00   0.00   0.00   0.00  73.09

Average: 0  0.00  0.00  1.02  98.98  0.00  0.00  0.00   0.00   0.00   0.00

pidstat：同上（略）可以看出CPU的IO升高导致系统平均负载升高。我们使用pidstat查找具体是哪个进程导致IO升高的。

top：这里使用top依旧是最方面的查看综合参数，可以得出stress是导致IO升高的元凶。

pidstat没有iowait选项：可能是CentOS 默认的sysstat太老导致，需要升级到11.5.5之后的版本才可用。

 

## IO争用

**对磁盘IO争用的调优思路有哪些？**

典型问题：针对主要争用是IO相关的场景下，调优的思路有哪些？主要的技术或者方法是什么？

### IO过大

**首先要搞清楚IO争用是因为应用等层面的IO量过大导致，还是系统层面不能承载这些IO量。**

如果应用层面有过多不必要的读写，首先解决应用问题。

举例1:数据库里面用于sort的buffer过小，当做sort的时候，有大量的内存与磁盘之间的数据交换，那么这类IO可以通过扩大sort buffer的内存来减少或避免。

举例2：从应用的角度，一些日志根本不重要，不需要写，那么可以把日志级别调低、甚至不记录日志，数据库层面可以加hint “no logging”。

 

**存储问题的分析思路**

存储IO问题可能出现在IO链路的各个环节，分析IO瓶颈是主机/网络/存储中的哪个环节导致的。

IO从应用->内存缓存->块设备层->HBA卡->驱动->交换网络->存储前端->存储cache->RAID组->磁盘，经过了一个很长的链条。

需要逐段分析：

**1、主机侧：应用->内存缓存->块设备层-->HBA卡->驱动**

**2、网络侧：交换网络**

**3、存储侧：存储前端->存储cache->RAID组->磁盘**

分析思路：

1、主机侧

当主机侧观察到的时延很大，存储侧的时延较小，则可能是主机侧或网络存在问题。

主机是I/O的发起端，I/O特性首先由主机的业务软件和操作系统软件和硬件配置等决定。例如，在“服务队列满”这一章节介绍的I/O 队列长度参数（queue_depth），当然，还有许多其他的参数（如：driver 可以向存储发的最大的 I/O、光纤卡DMA memor区域大小、块设备并发数、HBA卡并发数）。

若排查完成，性能问题还是存在，则需要对组网及链路、存储侧进行性能问题排查。

2、网络侧

当主机侧观察到的时延很大，存储侧的时延较小，且排查主机侧无问题时，则性能问题可能出现在链路上。

可能的问题有：带宽达到瓶颈、交换机配置不当、交换机故障、多路径选路错误、线路的电磁干扰、光纤线有损、接口松动等。带宽达到瓶颈、交换机配置不当、多路径选路错误、线路的电磁干扰等。

3、存储侧

如果主机侧时延与存储侧时延都很大且相差较小，说明问题可能出现在存储上。首先需要了解当前存储侧所承载的IO模型、存储资源配置，并从存储侧收集性能数据，按照I/O路径进行性能问题的定位。

常见原因如硬盘性能达到上限、镜像带宽达到上限、存储规划（如条带过小）、硬盘域和存储池划分（例如划分了低速的磁盘）、thin LUN还是thick LUN、LUN对应的存储的缓存设置（缓存大小、缓存类型，内存还是SSD）、IO的Qos限制的磁盘IO的带宽、LUN优先级设置、存储接口模块数量过小、RAID划分（比如RAID10>RAID5>RAID6）、条带宽度、条带深度、配置快照、克隆、远程复制等增值功能拖慢了性能、是否有重构、balancing等操作正在进行、存储控制器的CPU利用率过高、LUN未格式化完成引起短时的性能问题、cache刷入磁盘的参数（高低水位设置），甚至数据在盘片的中心还是边缘等等。

具体每个环节 都有一些具体的方法、命令、工具来查看性能表现，这里不再赘述。

 

### IO调优

**关于低延迟事务、高速交易的应用在IO方面可以有哪些调优思路和建议？**

典型问题：关于近期在一些证券行业碰到的低延迟事务、高速交易的应用需求，在IO模型路径方面可以有哪些可以调优的思路和建议？

对于低延迟事务，可以分析一下业务是否有持久化保存日志的需要，或者说保存的安全程度有多高，以此来决定采用什么样的IO。

1、从业务角度

比如说业务上不需要保存日志，那就不用写IO。

或者保存级别不高，那就可以只写一份数据，对于保存级别较高的日志，一般要双写、或多写。

2、从存储介质角度

1）可以全部采用SSD

2）或者采用SSD作为存储的二级缓存（一级缓存是内存）

3）或者存储服务器里面采用存储分级（将热点数据迁移到SSD、SAS等性能较好的硬盘上）

4）可以采用RAMDISK（内存作为磁盘用）

5）增加LUN所对应的存储服务器的缓存

3、从配置的角度

普通磁盘存储的LUN，可以设置合理的RAID模式（比如RAID10）去适应你的业务场景。

分条的深度大于等于一个IO的大小、有足够的宽度支持并发写。

4、IO路径的角度

采用高速的组网技术，而不用iSCSI之类的低速方式。

## 写磁盘太慢导致服务超时

https://blog.csdn.net/u011734144/article/details/84937838

***\*服务逻辑：\****

接收客户端的请求参数，写文件到本地磁盘

***\*问题发现：\****

客户端报超时， 每天日志理论上是有几万条的，但是近几天只收到了几条

***\*问题分析：\****

其实最开始的时候，以为是服务端处理不过来导致的，因为最开始服务端只有两个服务进程，后来增加了服务进程数，但是超时问题仍然没有得到解决。也怪自己没有做压力测试，如果做了压测，那么服务处理不过来的问题其实一开始就应该被排除的。

***\*现象：\**** 

机器的反应速度特别慢， 基本的mkdir，cp命令的反应速度都特别慢， 看来跟这个应该是有关系的。

***\*问题分析步骤：\****

1、查看CPU的使用情况

top命令查看了下负载情况：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps78C6.tmp.jpg) 

发现load average超过了20， 可用内存不足10G， 负载偏高， 内存勉强够用

2、查看磁盘使用情况：

执行如下命令：

$df -lh

得到如下结果：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps78C7.tmp.jpg) 

因为我们的服务是部署在"/search/odin"目录下， 服务写文件也是在这块磁盘上，可以看到这块磁盘"/search/sdc1"的使用率并不高，还有很大的可用空间， 所以应该不是因为磁盘不够导致的。

3、查看磁盘io的使用情况：

执行如下命令：

iostat -d -x 2 100

可以看到如下结果

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps78C8.tmp.jpg) 

可以看到sdc这块盘的使用率非常高，达到了100%

第二步中我们知道，我们的服务是要往“/search/sdc1”这块盘中写文件的， 看来就是sdc这块盘的资源不足导致的写文件速度慢，从而拉长了整个服务的时间， 从而导致了客户端的超时。

4、究竟是哪个进程消耗了这么多的io资源

执行如下命令查看：这个命令需要root权限

$iotop

可以看到的是和写es相关的服务占用了大量的磁盘资源

这个es服务每秒有大量的写请求，而它占用的磁盘io这么高，是因为内存不够，导致写请求直接写到磁盘了

 5、能不能释放点内存来缓解es写磁盘的压力？

top命令可以看到有一个进程占用了30多个G的内存，理论上这个服务是不会占用这么大的内存的，查了下原因是因为进程写磁盘失败，导致大量的数据被缓存到了内存中

于是把问题修复了下，并重启了这个服务，释放了30G左右的内存出来

过了一段时间后，重新查看磁盘的使用率：

iostat -d -x 2 100

得到如下结果：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps78C9.tmp.jpg) 

可以看到sdc这块盘的io使用率已经降下来了， 此时在这个盘上执行一些mkdir 或者 cp命令速度已经很快了。

这之后的几天，服务也没有再发生超时的情况。

***\*总结：\****对于这种反应慢的问题，一般遵循CPU->内存->磁盘->网络的排查顺序，CPU一般就是看一下是不是一直在计算，内存则是判断是否使用swap分区，磁盘则考虑空间大小和IO，网络则查看网络延时。一般对于磁盘IO问题，还需要判断是具体哪些进程或者线程导致的（iotop，ps）。

 

## MySQL磁盘I/O过高

***\*问题描述\****

线上数据库突发严重告警，业务方反馈写入数据一直堵住，很多锁超时回滚了，不知道怎么回事。

先采集现场的必要信息再说。

a. 系统负载，主要是磁盘I/O的负载数据

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps78D9.tmp.jpg) 

该服务器的磁盘是由6块2T SSD硬盘组成的RAID-5阵列。从上面的截图来看，I/O %util已经基本跑满了，iowait也非常高，很明显磁盘I/O压力太大了。那就再查查什么原因导致的这么高压力。

b. 活跃事务列表（information_schema.innodb_trx）

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps78DA.tmp.jpg) 

可以看到，有几个活跃的事务代价很高，锁定了很多行。其中有两个因为太久超时被回滚了。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps78EB.tmp.jpg) 

再看一次活跃事务列表，发现有个事务锁定的行更多了，说明活跃业务SQL的效率不太好，需要进行优化。这个算是原因之一，先记下。

c. 查看InnoDB状态

执行SHOW ENGINE INNODB STATUS\G查看InnoDB状态，这里只展示了几个比较关键的地方：

...

0x7f8f700e9700 INNODB MONITOR OUTPUT

...

LATEST DETECTED DEADLOCK

\------------------------

...

*** (2) TRANSACTION:

TRANSACTION 52970892097, ACTIVE 1 sec starting index read

mysql tables in use 2, locked 2

80249 lock struct(s), heap size 9691344, 351414 row lock(s),

 undo log entries 30005

\### 这里很明显，发生死锁的事务之一持有很多行锁，需要优化SQL

...

update a inner join b on a.uid=b.uid set a.kid=if(b.okid=0,b.kid,b.okid),a.aid=b.aid where

 a.date='2020-02-10'

...

TRANSACTIONS

\------------

Trx id counter 52971738624

Purge done for trx's n:o < 52971738461 undo n:o < 0

 state: running but idle

History list length 81

...

---TRANSACTION 52971738602, ACTIVE 0 sec inserting

mysql tables in use 1, locked 1

1 lock struct(s), heap size 1136, 0 row lock(s),

 undo log entries 348

 

\### 同样，也是有很多行锁

...

LOG

\---

Log sequence number 565123876918590

Log flushed up to  565123858946703

Pages flushed up to 565121518602442

Last checkpoint at  565121518602442

...

\### 注意到Last checkpoint和LSN之间的差距非常大，约为2249MB

\### 说明redo log的checkpoint有延迟比较厉害，有可能是因为磁盘I/O太慢，

\### 也有可能是因为产生的脏页太多太快，来不及刷新

\----------------------

BUFFER POOL AND MEMORY

\----------------------

Total large memory allocated 201200762880

Dictionary memory allocated 130361859

Internal hash tables (constant factor + variable factor)

  Adaptive hash index 3930999872    (3059599552 + 871400320)

  Page hash      23903912 (buffer pool 0 only)

  Dictionary cache   895261747    (764899888 + 130361859)

  File system     16261960     (812272 + 15449688)

  Lock system     478143288    (478120568 + 22720)

  Recovery system   0    (0 + 0)

Buffer pool size  11795040

Buffer pool size, bytes 193249935360

Free buffers    7035886

Database pages   4705977

Old database pages 1737005

Modified db pages  238613

 

\### 脏页比例约为2%，看着还好嘛，而且还有挺多free page的

...

d. 查看MySQL的线程状态*

+---------+------+--------------+---------------------

| Command | Time | State     | Info                                                 |

+---------+------+--------------+---------------------

| Query  |   1 | update    | insert xxx

| Query  |   0 | updating   | update xxx

| Query  |   0 | updating   | update xxx

| Query  |   0 | updating   | update xxx

| Query  |   0 | updating   | update xxx

+---------+------+--------------+---------------------

可以看到几个事务都处于updating状态。意思是正在扫描数据并准备更新，肉眼可见这些事务状态时，一般是因为系统负载比较高，所以事务执行起来慢；或者该事务正等待行锁释放。

 

2、问题分析及优化工作

分析上面的各种现场信息，我们可以得到以下几点结论：

a. 磁盘I/O压力很大。先把阵列卡的cache策略改成WB，不过由于已经是SSD盘，这个作用并不大，只能申请更换成RAID-10阵列的新机器了，还需等待资源调配。

b. 需要优化活跃SQL，降低加锁代价

[root@####]> desc  select * from a inner join b on

 a.uid=b.uid where a.date='2020-02-10';

+-------+--------+------+---------+----------+-------+----------+-----------------------+

| table | type  | key  | key_len | ref    | rows  | filtered | Extra         |

+-------+--------+------+---------+----------+-------+----------+-----------------------+

| a   | ref   | date | 3    | const   | 95890 |  100.00 | NULL          |

| b   | eq_ref | uid  | 4    | db.a.uid |   1 |  100.00 | Using index condition |

+-------+--------+------+---------+----------+-------+----------+-----------------------+

[root@####]> select count(*) from a inner join b on

 a.uid=b.uid where a.date='2020-02-10';

+----------+

| count(*) |

+----------+

|   40435 |

+----------+

1 row in set (0.22 sec)

执行计划看起来虽然能用到索引，但效率还是不高。检查了下，发现a表的uid列竟然没加索引。

c. InnoDB的redo log checkpoint延迟比较大，有2249MB之巨。先检查redo log的设置：

innodb_log_file_size = 2G

innodb_log_files_in_group = 2

这个问题就大了，redo log明显太小，等待被checkpoint的redo都超过2G了，那肯定要疯狂刷脏页，所以磁盘I/O的写入才那么高，I/O %util和iowait也很高。

 

建议把redo log size调整成4G、3组。

innodb_log_file_size = 4G

innodb_log_files_in_group = 2

此外，也顺便检查了InnoDB其他几个重要选项

innodb_thread_concurrency = 0

\# 建议维持设置0不变

innodb_max_dirty_pages_pct = 50

\# 由于这个实例每秒写入量较大，建议先调整到75，降低刷脏页的频率，

\# 顺便缓解redo log checkpoint的压力。

\# 在本案例，最后我们把这个值调整到了90。

***\*特别提醒\****

从MySQL 5.6版本起，修改redo log设置后，实例重启时会自动完成redo log的再次初始化，不过前提是要先干净关闭实例。因此建议在第一次关闭时，修改以下两个选项：

innodb_max_dirty_pages_pct = 0

innodb_fast_shutdown = 0

并且，再加上一个新选项，防止实例启动后，会有外部应用连接进来继续写数据：

skip-networking

在确保所有脏页（上面看到的Modified db pages为0）都刷盘完毕后，并且redo log也都checkpoint完毕（上面看到的Log sequence number和Last checkpoint at**值相等），此时才能放心的修改 innodb_log_file_size 选项配置并重启。确认生效后再关闭 skip-networking 选项对业务提供服务。

经过一番优化调整后，再来看下服务器和数据库的负载。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps78EC.tmp.jpg) 

可以看到，服务器的磁盘I/O压力再也不会那么大了，数据库中也不会频繁出现大量行锁等待或回滚的事务了。

[root@####]> SHOW ENGINE INNODB STATUS\g

Log sequence number 565749866400449

Log flushed up to  565749866400449

Pages flushed up to 565749866318224

Last checkpoint at  565749866315740

 

[root@####]> SHOW ENGINE INNODB STATUS\g

Log sequence number 565749866414660

Log flushed up to  565749866400449

Pages flushed up to 565749866320827

Last checkpoint at  565749866315740

 

[root@####]> SHOW ENGINE INNODB STATUS\g

Log sequence number 565749866414660

Log flushed up to  565749866414660

Pages flushed up to 565749866322135

Last checkpoint at  565749866315740

 

[root@####]> select (565749866400449-565749866315740)/1024;

+----------------------------------------+

| (565749866400449-565749866315740)/1024 |

+----------------------------------------+

|                 82.7236 |

+----------------------------------------+

1 row in set (0.00 sec)

 

[root@####]> select (565749866414660-565749866315740)/1024;

+----------------------------------------+

| (565749866414660-565749866315740)/1024 |

+----------------------------------------+

|                 96.6016 |

+----------------------------------------+

很明显，redo log checkpoint lag几乎没有了。

 

遇到数据库性能瓶颈，负载飚高这类问题，我们只需要根据一套完整的方法论优化系列：实例解析MySQL性能瓶颈排查定位，根据现场的各种蛛丝马迹，逐一进行分析，基本上都是能找出来问题的原因的。本案其实并不难，就是按照这套方法来做的，最后连perf top都没用上就很快能大致确定问题原因了。

 

## 句柄泄漏

# 磁盘修复

磁盘损坏或者挂载失效处理流程：

1、确认磁盘损坏信息

mdadm -D /dev/md0

2、卸载挂盘（在执行umount前需要移除挂载目录中的东西，傻吊所有可执行程序，否则有可能失败）

umount /dev/md0

3、停止磁阵

mdadm -S /dev/md0

4、重新做RAID流程（以PCIE软RAID为例）

mdadm -C /dev/md0 -1 10 -n 4 /dev/nvme0n1 /dev/nvme1n1 /dev/nvme1n2 /dev/nvme1n3 

查看RAID信息：

mdadm -detail /dev/md0

格式化：

mkfs -t xfs -f /dev/md0

创建目录：

mkdir /home/data_pcie

持久化RAID信息：

echo DEVICE /dev/nvme0n1 /dev/nvme1n1 /dev/nvme1n2 /dev/nvme1n3 >> /etc/mdadm.conf

mdadm -Ds >> /etc/mdadm.conf

持久化挂载目录：

echo /dev/md0 /home/data_pcie xfs defaults,nobarrier,noatime,nodiratime 0 0 >> /etc/fstab