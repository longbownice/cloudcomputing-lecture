# **内核内存分配**

在kernel里，通常有3种申请内存的方式：vmalloc, kmalloc, alloc_pages。	kmalloc与alloc_pages类似，均是申请连续的地址空间。而vmalloc则可以申请一段不连续的物理地址空间，并将其映射到连续的线性地址上。每次vmalloc之后，内核会创建一个vm_struct，用以映射分配到的不连续的内存区域。vm_struct类似vma，但是又不是一回事。vma是将物理内存映射到进程的虚拟地址空间。而vm_struct是将物理内存映射到内核的线性地址空间。　

​	

kmalloc从***\*通用缓冲区\****中申请空间，而vmalloc则无此要求，它利用普通的申请页面的函数进行申请。

malloc最终调用do_brk()，在***\*用户空间\****的***\*堆栈\****中申请空间，不过do_brk做“批发”，malloc做“零售”。

mmap()将一个已经打开的文件的内容映射到他的用户空间，使得能够像访问内存一样访问文件。

 

## **获得页**

## **kmalloc**

## **vmalloc**

## **高端内存映射**

## **Slab**

## **分配函数选择**

如果你需要连续的物理页，就可以使用某个低级页分配器或kmalloc()。这是内核中内存分配的常用方式，也是大多数情况下你自己应该使用的内存分配方式。

如果你想从高端内存进行分配，就使用alloc_pages()。为了获得真正的指针，应该调用kmap()，把高端内存映射到内核的逻辑地址空间。

如果你不需要物理上连续的页，而仅仅需要虚拟地址上连续的页，那么就使用vmalloc()（不过要记住vmalloc相对于kmalloc()来说，有一定的性能损失）。vmalloc()函数分配的内存虚拟地址是连续的，但它本身并不保证物理上的连续。

如果你要创建和撤销很大的数据结构，那么考虑建立slab高速缓存。Slab层会给每个处理器维持一个对象高速缓存（空闲链表），这种高速缓存会极大地提高对象分配和回收的性能。Slab层不是频繁地分配和释放内存，而是为你把事先分配好的对象存放到高速缓存中。当你需要一块新的内存来存放数据结构时，slab层一般无须另外去分配内存，而只需要从高速缓存中得到一个对象就可以了。

 

当需要分配一个不具有专用slab队列的数据结构而不必为之使用整个页面时，就应该通过kmalloc分配，这些一般都是些细小而又不常用的数据结构，如果数据结构大小接近一个页面，则应该直接通过alloc_pages为之分配一个页面；函数vmalloc从内核的虚存空间（3G）分配一块虚存以及相应的物理内存，类似与系统调用brk()。不过brk()是由进程在用户控件启动并从用户控件分配的，而vmalloc则是在系统空间，也就是内核启动，从内核空间分配的。由vmalloc分配的空间不会被kswapd换出，因为kswapd只扫描各个进程的用户空间，而根本看不到通过vmalloc分配到页面表项。至于通过kmalloc分配的数据结构，则kswapd只是从各个slab队列中寻找和收集空闲不用的slab，并释放占用的页面，但是不会将尚在使用的slab所占据的页面换出。

 

# **静态内存分配**

# **动态内存分配**

## **原理**

### **内存分布**

Linux系统上应用程序的内存分为text段、data段、bss段、内存映射区、栈。	其中data段和bss段被称为静态区，其大小在程序运行期间保持不变。

函数中的变量使用栈上的空间，函数调用结束后无需手动释放内存。

堆和内存映射区则是动态分配的，在其上申请的内存需要手动释放。

### **动态内存必要性**

静态区和栈上的数据无需手动释放，用起来很方便，为什么还需要动态内存呢？

（1）动态对象问题

在很多情况下，问题规模可以预知，这个时候可以用一个固定规模的容器如数组进行处理，但在另外一种场景下，问题的规模是不可预知的。例如对整数排序，输入的证书数目是未知的，用一个有限数量的数组显然不能满足超出容量的排序请求，而使用很长的数组在小规模的排序时又非常的浪费空间。因此静态化的结构不适合来处理动态化的问题。

（2）内存占用量

除了堆之外，应用程序还有静态区以及线程栈可以容纳内存对象，为什么不把内存都放在这两个部分呢？一般来说，多数内存对象在使用后就不需要了，而静态区内的内存对象在程序运行期间一直存在，这些短暂使用的对象放在静态区会大大增加内存占用量，造成不必要的浪费。线程栈长度是有限制的，栈的使用情况又比较难以估计，一旦超出就会产生栈溢出错误。因此静态区和线程栈都不适合存放所有的内存对象。

（3）部分数据结构的实现要求

虽然像二叉树这样的数据结构可以在数组这样的静态结构上实现，但更多更复杂的数据结构在数组上实现起来非常的麻烦。而数据结构又与影响程序执行效率的算法密切相关。因此相比于静态分配方法，更灵活的内存分配方式可以实现复杂多变的算法，从而提升程序的执行效率。

 

## **动态内存申请方式**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FAD.tmp.jpg) 

### **brk和mmap**

操作系统向应用程序提供了两种动态申请内存的方式：brk和mmap。

brk负责移动堆指针，也就是可以扩大或者缩小堆。mmap负责开辟新的内存映射段，具体映射的段，可以由系统选择，也可以自己指定首地址。

既然操作系统已经提供了这两种申请堆内存的方式，为什么还需要内存分配器呢？

应用程序直接使用brk和mmap会造成不少问题：

（1）brk和mmap都会触发系统调用，从而导致用户态和内核态的切换，反复调用时性能不够高；

（2）brk只能调整**堆顶指针**，释放内存很不灵活（比如先后申请1K->2K内存，则无法在申请完2K内存后释放前面的1K内存，需要先释放2K内存，然后再释放1K，即产生内存碎片）；

（3）mmap申请单位为一页（umap释放内存），如果每次申请都使用一页，浪费严重；如果切分页使用，则管理的复杂度很高（进行大块内存申请时使用mmap）。

在操作系统和应用程序之间增加一层内存分配器，提供一个通用方法，可以有效的解决上述问题。

### **显式/隐式内存分配器**

内存分配器在具体使用时分为两种：

一类是类似于C，需要程序员显式的申请和释放内存，这种称为显式内存分配器；

一种类似于Java，不需要程序员显式的释放内存，而由分配器自己判定内存是否需要释放，这种称为隐式内存分配器，又成为垃圾收集器。在检查内存是否需要释放的过程中，隐式内存分配器可能会暂停进程，并可能移动正在使用的内存块。

 

显式内存分配器的优缺点（以C为例）：

（1）程序员拥有更多的权限，发展出了很多高效的技巧；

（2）内存泄漏发生可能性比隐式更高，但检测反而相对容易；

（3）多种类的内存问题，包括内存溢出、释放已释放内存等，加大了程序风险；

（4）实现部分数据结构时可能有内存隐患；

（5）多层级结构的释放时，难于判断是否要释放，需要额外的手段比如引用计数和锁。

 

显式内存分配器的优缺点（以Java为例）：

（1）垃圾收集使得程序员不必花多少时间去关心内存释放问题，内存泄漏可能性小，但是排查相对更加困难；

（2）对内存使用可以附加更多的检查，防止内存问题，当然，这意味着一定的性能损失；

（3）在实现部分数据结构时更加有优势；

（4）执行垃圾收集时可能会暂停程序，或要移动内存，影响性能。

# **高端内存映射**

根据定义，在高端内存中的页不能永久映射到内核地址空间上。因此，通过alloc_pages()函数以_GFP_HIGHMEM标志获得的页不可能有逻辑地址。

在X86体系结构上，高于896MB的所有物理内存的范围大都是高端地址，它并不会永久地或自动地映射到内核地址空间，尽管X86处理器能够寻址物理RAM的范围达到4GB（启用PAE可以寻址到64GB）。一旦这些页被分配，就必须映射到内核的逻辑地址空间上。在X86上，高端内存中的页被映射到3GB~4GB。

## **永久映射**

### **kmap**

### **kunmap**

要映射一个给定的page结构到内核地址空间，可以使用函数kmap()，这个函数在高端内存或低端内存上都能用。如果page结构对应的是低端内存中的一页，函数只会单纯地返回该页的虚拟地址。如果页位于高端地址，则会建立一个永久映射，再返回地址。这个函数可以休眠，因此kmap()只能用在进程上下文中。

因为允许永久映射的数量是有限的（如果没有这个限制，我们就不必搞得那么复杂，把所有内存通通映射为永久内存就行了），就不再需要高端内存时，应该解除映射，可以通过kunmap()完成。

## **临时映射**

### **kmap_atomic**

### **kunmap_atomic**

当必须创建一个映射而当前的上下文又不能睡眠时，内核提供了临时映射（也就是所谓的原子映射）。有一组保留的映射，它们可以存放新创建的临时映射。内核可以原子地把高端内存中的一个页映射到某个保留的映射中。因此，临时映射可以用在不能睡眠的地方。

通过函数kmap_atomic()建立一个临时映射，这个函数不会阻塞，因此可以用在中断上下文和其他不能重新调度的地方。它也禁止内核抢占，这是必要的，因为映射对每个处理器都是唯一的（调度可能对哪个处理器执行哪个进程做变动）。

通过函数kunmap_atomic()取消映射，这个函数也不会阻塞，在很多体系结构中，除非激活了内核抢占，否则kmap_atomic()根本就无事可做，因为只有在下一个临时映射到来前上一个临时映射才有效。

# **显式内存分配器**

## **基本要求**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FAE.tmp.jpg) 

（1）能够处理任意的内存分配、释放请求队列；

（2）分配、释放内存的速度要快；

（3）分配的块是字节对齐的，访问非对齐内存地址会有性能损失；

（4）分配器本身不去修改或移动已分配的块，相反，隐式的分配器可以修改和移动已分配的块；

（5）内存空间的利用效率要高。

一般来说，内存分配器做的事情，可以看成是向系统申请了一个或多个较长的字节数组，然后再这个数组内部分割出小块内存来给应用程序使用。

## **基本模式**

​	一般来说，内存分配器做的事情，可以看做是向系统申请了一个或多个较长的字节数组，然后再这个数组内部分割出小块内存来给应用程序使用。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FBF.tmp.jpg) 

​	注：黄色表示已使用的块，白色为未使用的块。

 

## **块的长度**

### **头部保存size**

标准的内存释放函数free只有一个入参即malloc时返回的指针，不包含申请的长度信息。因此内存分配器需要自行维护分配的每个块的长度信息。其方法是，在实际返回给用户的内存块之前，预留一定的空间，来保存分配器的内存块长度信息。

 

### **size本身占用空间大小**

对于保存块长度的size，其本身所占用的大小也有要求，size可表示的长度范围，需要能容纳当前系统可能分配的内存块的长度。

一般来说，size本身所占大小与系统位数一致，32位系统size为4字节，64位操作系统size为8字节。

另一方面，出于字节对齐的要求，32位操作系统要求内存块按照4字节对齐，64位操作系统要求内存按照8字节对齐。比如64位系统用户需要50字节空间，实际分配的是56字节，以满足对齐的要求，且内存块起始地址一定是8的倍数。

### **利用size中的位**

一个内存块，是否已经被分配给用户使用，也需要标识出来。如果为了标识是否使用，单独分几个字节出来保存标志位，显然比较浪费。这里可以使用一个小技巧，以64位系统为例，由于字节对齐的原因，size一定是8的倍数，size最后的3bit一定都是0，这样可以取其中的一个bit为来表示该内存块是否被使用。

​	对于64位系统，由于字节对齐的原因，size的最后3位一定都是0：

| size | 000  | payload |
| ---- | ---- | ------- |
|      |      |         |

 

## **块的使用**

​	通常使用size中最后的bit位来记录内存块是否在使用，0表示未使用，1表示使用中。64位系统下如下所示：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FC0.tmp.jpg) 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FC1.tmp.jpg) 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FC2.tmp.jpg) 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FD2.tmp.jpg) 

## **内存块的分配**

### **从字节数组中申请内存**

考虑一个256字节的字节数组，初始时最大可申请248字节，申请了64字节后，只能在申请最多176字节的内存块了。注意内存块头部使用标志的变化情况。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FD3.tmp.jpg) 

### **寻找下一块可用内存**

​	以上面的情况为例，已经申请了一次之后，下次该如何申请？

​	这时内存块头部的size就派上用场了，由于字节数组的起始地址同时也是第一个块size的地址，因此可以读出size的值，通过（起始地址+8+size）即可算出下一个块的内存起始地址。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FD4.tmp.jpg) 

### **内存申请的流程**

​	有了前面的分配方法和查找方法，就可以给出一个简单的内存申请流程图。下面就是在一个字节数组中，当调用malloc时，从字节数组头部开始不断探索可用内存的过程。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FD5.tmp.jpg) 

### **空闲块选取**

​	经过多次的分配和释放，可能在内存分配器管理的这个大字节数组中出现多个空闲块，如下图的情况。这时就面临一个问题：下次申请内存时从哪个空闲块中来分配呢？

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FE6.tmp.jpg) 

### **首次适配**

​	前面流程图给出的探索方法称为首次适配。即从字节数组头部开始寻找足够荣南申请的内存大小的空闲块。这种方法易于实现，但是经过很多次的申请之后（比如连续申请100次），后面的申请需要经过很多的无谓遍历，才能找到第一个空闲块，这种情况下的性能不太高。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FE7.tmp.jpg) 

### **后继适配**

​	后继适配针对首次适配的问题做了改进，不再从字节数组头部开始搜索，而是从上次分配的内存块的尾部开始搜索。对之前的连续申请内存100次的情况，显然在后继适配下，每次都是从后面的空闲块中直接分配内存，无需从头遍历。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FE8.tmp.jpg) 

### **低效的分配**

​	考虑下图的情况，还剩下3个空闲的内存块，而一次申请16字节和32字节大小的空间。显然，如果将40字节的空闲空间用于分配16字节大小，那么32字节的malloc申请就会失败。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FE9.tmp.jpg) 

### **最佳适配**

​	为了应对上面的问题，可以引入最佳适配，即搜索能够满足分配要求的最小空闲的内存块。按照此方法，前页的示例即可满足32字节的分配要求了。最佳适配的问题在于查找效率不高。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FFA.tmp.jpg) 

## **内存块的释放**

### **内存使用情况**

​	考虑下面的分配场景，假设有一小段内存，被分配为5块供应用程序使用，其中第2、3、4块的载荷分别为24、16、32字节，如下图所示：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FFB.tmp.jpg) 

然后第2、3、4块被释放，可以计算出来，中间将空出96字节的空间。假如再分配一个长度为64字节的空间，显然实际上是由连续且足够的空闲空间来完成这个分配的。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FFC.tmp.jpg) 

但对于内存分配器来说，它只能先定位到第一个空闲块，然后从其中的size读到空闲块大小为24届，根据之前的流程，这个块不够分配，必须查找后面的块。根据前面内存分配流程的遍历结果，最终会返回分配失败。

### **碎片**

所谓碎片，是只由内存分配器管理的内存中，尚未被使用且可能无法被分配使用的空间。

### **内存块的合并**

#### **为什么需要合并？**

​	假如中间的内存块能够合并成一个完整的大空闲块，那么根据之前的申请流程图，就可以找到足够大的空闲内存块，能够容纳malloc申请的内存大小了。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps6FFD.tmp.jpg) 

#### **时机与判断**

​	如果想嫁给连续的空闲块合并，需要考虑：

1、 什么时候将连续空闲块合并？

2、 如何判断前后的块是空闲的还是在使用中的？

对于第1个问题，比较自然的想法是在释放内存块时进行处理，或者说是在free函数内处理该内存块与前后内存块的合并问题。

 

#### **得到后继块信息**

​	对于问题2，要点是能知道前后内存块是否在使用，以及具体所占用的空间大小。先考虑后继的内存块。根据free传入的指针p，得到r=p-8，取出size1的值，算得后继内存块的起始位置q=p+size1。从q处可取得size2的值以及使用标志位b的值，并算得后继块的结束位置s=q+8+size2。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps700D.tmp.jpg) 

#### **合并后继块**

​	合并后继块的示意图如下，根据p得到size1以及size2后，将当前块起始位置的size更新为size1+size2+8，并且将使用标志置0。注意，一般来说，payload内容并不一定清零。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps700E.tmp.jpg) 

#### **无法得到前继块信息**

​	对前继内存块，仍然可以根据free传入的p计算得到本内存块的起始地址r=p+8。前继内存块的起始位置与size2相关，即有q=r-size2-8，然而size2的起始地址又是q。这样要知道q必须先知道size2，要知道size2又必须先知道q，成了一个死循环，因此必须另外想办法。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps700F.tmp.jpg) 

#### **解决前继块合并问题**

​	为了解决前继块不能合并的问题，可以考虑在内存块的尾部加上一个与头部完全相同的size段，并且其中同样包含使用标志位。如下图所示，有了尾部size之后，可以通过p计算德奥t=r-8=p-16，从中取得前继块size2及使用标志b，并进而计算得到前继块起始地址q=t-size2-8。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7010.tmp.jpg) 

#### **提高空间利用率**

​	如果内存块首尾都有size，那么首尾的size部分共占16字节，当size比较小的时候，实际给应用程序使用的部分（payload）比例就比较低，内存未得到有效使用。通过更精细的分析发现，如果前继块正在使用中，那么无需合并前继块，也就无需读取前继块的size。如果能在当前块中能保存前继块的使用情况，那么正在被使用的前继块就无需保留尾部的size部分了。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7021.tmp.jpg) 

#### **更精细的处理**

​	从前面分析可以知道：对于空闲块，需要在尾部保留size部分；对于在使用的块，不需要在尾部保留size部分。另外，需要在内存块头部size的未3bit中用1bit表示前继块的使用情况。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7022.tmp.jpg) 

#### **合并前继块**

​	合并前继块的示意图如下，根据p得到size1以及size2后，将前继块起始位置的size更新为size1+size2+8+8，并且将使用标志置为0。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7023.tmp.jpg) 

#### **可能的合并情况**

​	释放内存块的时候，可能的合并情况如下，其中合并前继块与后继块分别使用之前提供的合并判断方法。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7034.tmp.jpg) 

### **基本方法总结**

​	上面列举了处理内存分配释放的一些基本技术，几乎所有更复杂的内存分配器都是基于这些基本技术并结合实际使用场景进行逐步拓展开发出来的。掌握这些基本技术，对一些实际问题的理解也会更加深入。比如内存越界会导致程序问题，其原因就是越界的部分破坏了本内存块尾部的size段或后继内存块的size和使用标志段，导致内存分配器在后续分配与释放时判断出错，从而导致一系列严重错误。

 

## **分类**

### **隐式空闲链表**

#### **实现方式**

​	隐式空闲链表是一种最简单的内存分配器实现。

​	隐式空闲链表用到了内存分配器的各种基本技术，包括头部size段、使用标志位、空闲块尾部的size段、释放内存时的合并技术等，称其为链表，是因为通过每个块的size可以计算得到下一个块的其实位置，就像是有指针指向下一个块一样；而隐式就是说并不是通过指针来记录的，而是由size间接计算。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7035.tmp.jpg) 

#### **特点**

​	隐式空闲链表的实现非常简单，其特点比较明显：

1、 申请内存时需要探索合适的空闲空间，其消耗与（已分配的块+空闲块）数量呈线性关系，因此随着使用时间增长，申请消耗开销较大；

2、 释放内存时，由于只需考虑前后块的合并问题，因此释放开销基本上是常数级的；

3、 随着使用的增长，碎片问题会越来越严重。

#### **使用场景**

​	隐式空闲链表存在不少问题，但并不意味着它无处可用。仔细分析可以发现：如果管理的总内存很小，那么（已分配的块+空闲块）总数不多，申请的消耗不算很大；由于块的数目较少，碎片也不会太多，这样碎片问题也不会太严重。因此在管理的总内存很小的情况下，仍然可以使用隐式空闲链表来管理内存。

 

### **显式空闲链表**

#### **引入指针**

​	与隐式空闲链表对应的，就是显式空闲链表。顾名思义，显式空闲链表需要通过内部指针来指向对应的块。从隐式空闲链表的缺点来看，它主要是在寻找空闲块的时候消耗比较大，为此，可以用显式的指针将空闲块都连接起来。空闲块的内部结构如下图所示，其中prev和next分别指向前后空闲块的地址：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7036.tmp.jpg) 

#### **总体情况**

​	下图显示了整体情况，其中s表示size，p表示指向前一个空闲块的指针，n表示指向后一个空闲块的指针。从指针连接情况来看，所有的空闲块组成了一个双向链表。

​	从搜索的角度来看，实际上只需要next指针就够了，为什么还需要指向前一个空闲块的prev指针呢？

​	合并完成之后，合并后的块的next指针是可以从原来的后继空闲块中取到的。然而由于合并后的块的起始位置与原来后继空闲块起始位置不同，前一个空闲块的next指针需要修改，这个时候就会发现，在缺少prev指针的情况下，无法找到前一个空闲块的起始位置，从而也就没法更新前一个空闲块的next指针。因此引入prev指针是必要的。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7039.tmp.jpg) 

#### **申请释放策略**

​	对于显式空闲链表，其申请时间无论采用哪种策略，都是正比于空闲链表的长度的，相比于隐式空闲链表申请时间有所下降。

​	释放策略1：后进先出的方法

​	即将最后释放得到的空闲块放在空闲链表头部，这样可以获得常数级的释放时间。在申请时采用首次适配或最佳适配策略，即从链表头部开始遍历，直到找到可以容纳申请大小的第一个空闲块或最小的空闲块。

​	释放策略2：按地址排序的方式

​	在这种方式下，空闲链表按照空闲块起始地址顺序进行排序。当被释放的块前后有空闲块时，可以通过前后块获取到链表中的指针。从而直接插入链表。但如果被释放的块（下图中蓝色块）前后均为正在使用的块时无法找到两个绿色空闲块的地址，此时需要遍历空闲链表来确定前后块，因此需要线性时间。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7047.tmp.jpg) 

#### **特点**

1、 申请时间正比于空闲块数量；

2、 空闲链表使用后进先出策略时，释放时间为常数；

3、 空闲链表使用地址排序策略时，部分情况下释放时间正比于空闲块数量；

4、 由于引入指针，管理数据比较多，空间利用率不高，碎片较多。

 

### **分离式空闲链表**

​	前面的两种内存分配器的实现方式将不同大小的内存在同一大块内存中一起进行分配。为了在申请和释放这些不同大小的内存块时进行管理，引入了头部size段、使用标志、尾部size段、前后指针等等管理数据。从反面来考虑，如果在同一大块内存中，只包含一种大小的内存块，这样是否能方便管理呢？是否能减少一些管理数据开销呢？答案是肯定的，这就需要使用一种所谓的分离式空闲链表的方法。

#### **基本思路**

​	分离式空闲链表的思想很简单，就是对内存申请，按照不同的申请大小分类，对不同的类分别管理对应的空闲块。比如可以划分为1B~8B、9B~16B、17B~24B……96B~128B、128B~256B、>256B这么一些类，对属于每一类的申请大小，均安好这一类中最大的那个块大小来分配空间。比如申请20B大小的内存，由于20B属于17B~24B范围，那么就按照24B来分配。为了保持字节对齐的要求，一般来说都根据系统位数按照4或8的倍数来划分不同的类。

 

### **简单分离链表**

#### **基本方法**

​	有了分类之后，可以给出一种简单的内存分配实现方法：

1、 不同类的内存申请在不同的大块内存中管理；

2、 申请内存时，只需要根据申请的大小找到对应的类，然后从该类的空闲链表取第一个空闲块给应用使用即可；

3、 释放内存时，根据释放的块的大小找到对应的类，然后放回该类的空闲链表头部即可。

问题：释放时只有一个指针p，如何根据p知道块大小呢？

#### **空间组织**

​	先看一下分离式空闲链表的组织空间。以32B大小的空闲链表为例，如下图，在整个大块内存的头部记录了本块管理的内存块size（=32）。申请时直接从head指针获取空闲块，然后修改head，释放时只需修改head及head->next即可。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7048.tmp.jpg) 

#### **定位size的技巧**

​	下面需要解决的就是通过释放时传入的指针确定整个块起始处的size的问题。注意到操作系统都是按照页作为基本单位来管理内存的，一页通常为4096 ，页的起始地址必定是4096的倍数。在向操作系统申请大块内存时，按页申请整块内存，然后按照前述的方式管理。对于free函数传入的指针p，只需要通过p&(~4095)即可取得p所在的页的起始地址，通过该地址，即可获得size的值。

#### **碎片问题**

​	简单分离式链表的性能很好，但可能面临碎片和内存回收问题。比如按照8字节倍数进行分类，即1B~8B、9B~16B、……505B~512B、>512B，如果先申请一堆1B~8B的内存，然后除了每个系统页内除了一个内存块之外全部释放掉，其余的类依法炮制，则会出现实际只占用很少内存，但却占用了大量的系统内存页的情况，这个显然对系统内存造成了压力。究其原因，在于不同类的内存块，不会共用同一系统页，从而导致了浪费。

#### **特点**

1、 申请和释放都是常数级时间；

2、 没有空闲块分割与合并，因此每个内存块都不需要头部size、使用标志、尾部size；

3、 只需要在空闲块中维护一个指针；

4、 在部分申请释放模式中，碎片情况较严重；

5、 空间占用较多。

简单分离链表虽然碎片问题，不少场景下仍然是不错的选择。

### **分离适配链表**

#### **减少碎片**

​	分离适配链表属于简单分离链表与显式空闲链表的一种混合，其主要目的是解决简单分离链表的碎片问题。

​	分离适配链表的实现模式：

1、 整块内存中可以分配不同大小的块；

2、 不同类的空闲块，使用不同的空闲链表管理；

3、 使用分割-合并方法，且分割或合并得到的空闲块，需要根据大小放入适当的空闲链表中。

#### **组织方式**

​	下图简单展示了分离适配链表的组织方式。图中有两类空闲块大小，分别为16B和40B，这两类空闲块分别组织成空闲链表，每个空闲链表有专门的链表头指针。内存分配器自己维护所有的空闲链表头指针。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7049.tmp.jpg) 

#### **分配流程**

​	在分配时，先根据申请大小确定所属的类。检查本类的空闲链表，如果链表非空，则直接分配；否则不断查找更大的块的那些类的空闲链表，只要有链表非空，即对更大的块做分割，分割出的一部分给应用使用，分割剩下的空闲块则放入合适的空闲链表。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps705A.tmp.jpg) 

#### **直接分配**

​	下图显示了分配一个13B内存块的情况，按照8的倍数对齐后13B应属于16B的类，本类空闲链表非空，可以直接分配。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps705B.tmp.jpg) 

#### **从更大的空闲块中分割**

​	对于分配21B，按照8的倍数对齐应属于24B的类，该类空闲链表为空，向上查找发现32B的也为空，而40B的空闲链表非空，取第一个空闲块分割，剩余的16B块放入16B的空闲链表。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps705C.tmp.jpg) 

#### **释放时合并并放入合适的链表**

​	对于释放的情况，如下图为例，假设需要释放绿色的24B大小的内存块，该内存块与后面的16B空闲块合并后，需要加入到40B的空闲链表，注意后面的16B空闲块需要从对应链表摘除。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps705D.tmp.jpg) 

#### **特点**

​	分离适配链表因为可以复用空闲块，大大改善简单分离链表的碎片问题。

1、 分配在多数情况下是常数时间的；

2、 释放内存时常数时间；

3、 内存利用率比较高，且改善了碎片问题。

由于分离适配链表的性能较好，碎片问题也得到控制，Linux上GNU提供的内存分配器即采用分离适配链表的方法。

 

### **伙伴系统**

为内核提供了一种用于分配一组连续的页而建立的一种高效的分配策略，并有效的***\*解决了外碎片问题\****。

分配的内存区是以***\*页框\****为基本单位的。

#### **一种特殊的分离适配链表**

​	伙伴系统是一种采用分离适配的特殊的内存管理方法。其特点：

1、 与以8的倍数对齐的分类方式不同，伙伴系统是利用计算机地址的二进制特点，按2的幂次进行分类的。比如可以按照32、64、128、256、1024这样的大小来分类；

2、 申请时，按申请大小向上舍入到最小的2的幂次；

3、 申请时分割空闲块时对半分割，分割出的两块互为伙伴；

4、 释放时，仅当伙伴也是空闲块时才进行合并，合并而成的空闲块需要递归检查自己的伙伴是否是空闲块，是则需要合并。

 

#### **内存块的结构**

​	伙伴系统的块结构与之前看到的块结构略有不同。对在使用的块，结构是类似的，头部包含size段，size段中也包含使用标志；对空闲块，头部size段与使用的块一致，由于需要放在空闲链表中，因此需要有prev、next指针，但在尾部，由于伙伴系统特别的合并算法，是不需要size段的。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps706D.tmp.jpg) 

#### **分割申请**

​	假定总共有512B的空闲内存，申请一块40B大小的内存，容纳40B的合适的大小为64B（必须是2的幂次），如下图所示，对整块内存不断二分，直到分出一块的64B的内存块。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps706E.tmp.jpg) 

#### **直接从空闲链表获取**

​	假设继续申请一块50B大小的内存，容纳50B的合适的块大小仍为64B，此时64B的空闲链表非空，因此直接从空闲链表头部取空闲块使用。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps706F.tmp.jpg) 

#### **从大的空闲块分割**

​	假设再申请一块56B大小的内存，合适的块大小仍为64B，此时64B的空闲链表为空，则检查更大的块即128B的空闲链表，将一个128B的空闲块分割后，分出一个64B块使用。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7070.tmp.jpg) 

#### **释放时与伙伴合并**

​	考虑合并的情况，如下图，绿色的64B块释放时，前后的两个64B块都是空闲的，但按照伙伴原则，这个块释放后只能与前一个64B空闲块合并。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7071.tmp.jpg) 

#### **释放时递归合并**

​	下图显示了递归合并的情况，绿色的64B块释放时，先与伙伴合并，合并后的128B块还可继续合并，最终得到最下图结果。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7082.tmp.jpg) 

#### **如何识别伙伴**

​	在合并当中还遗留了一个如何判断伙伴的问题。这个其实用到了二进制特性。以之前的512B的大内存块为例，如下图所示，假设其起始地址为0，观察图中4个64B的内存块的起始地址可以发现，伙伴之间恰有一bit不同，且这一bit所在位置又恰好表示了块的大小。整理成表达式即为：

块起始地址 ^ 块大小 = 伙伴块起始地址

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7083.tmp.jpg) 

#### **特点**

​	伙伴系统作为分离适配的一种特殊实现，具有如下特点：

1、 申请内存，除了需要分割空闲块的场景，均是常数时间；即使有分割，分割次数也有上限，因此申请比较快；

2、 释放内存，除了需要合并空闲块的场景，均是常数时间；即使有合并，合并次数也有上限，因此合并也比较快；

3、 由于实际分配的内存大小需要向上舍入到2的幂次，因此颞部浪费较严重。对于块大小可预知的场景，伙伴系统仍然是可以使用的，比如Linux内核中就使用了伙伴系统。

 

### **显式内存分配器设计**

#### **碎片化问题**

​	https://blog.csdn.net/aurorayqz/article/details/79671785

#### **多线程并发问题**

##### **锁和性能**

​	前面介绍的显式内存分配器的基本方法都没有考虑多线程场景。当多个线程同时向内存分配器申请或释放内存是时，对内存分配器的元数据的操作很可能是由并发隐患。一种简单的处理方法是对这些操作加锁。但锁本身是有开销的，而且在多个线程争用锁的情况下，性能比较低。这种情况下很难满足应用程序对内存分配器的性能要求。因此，如何处理多线程情况下的内存分配是实现时必须考虑的问题。

##### **解决思路**

​	多线程问题有多种解决方法。一种是锁方面做文章，比如使用锁分段技术，向系统申请多个大块内存，不同的块使用不同的锁保护，而让线程在其中选择一个固定的块申请内存，这样至少不同的块之间可以并行分配。更进一步的想法是，如果每个这样的大块内存均被某个线程独占，则根本不会发生共享，也就不需要加锁进行保护了。当然，这个可能会导致空间的浪费，需要对两者进行平衡。

##### **实际使用的显式内存分配器**

​	实际使用中的内存分配器往往综合考虑了性能、空间利用率等多方面因素，以下是几种常用的内存分配器：

​	dlmalloc——Doug Lee开发的内存分配器

​	ptmalloc——GNU默认的内存分配器，基于dlmalloc

​	tcmalloc——google开发的内存分配器

​	jemalloc——FreeBSD开发的内存分配器

 

# **内存分配算法**

## **内存碎片**

### **基本原理**

产生原因：内存分配较小，并且分配的这些小的内存生存周期又较长，反复申请后将产生内存碎片的出现。

 

外部碎片是进程与进程间未分配的内存空间，外部碎片的出现和进程频繁的分配和释放内存有直接关系。

内部碎片主要因为分配器粒度问题以及一些地址限制导致实际分配的内存大于所需内存，这样在进程内部就会出现内存空洞。

 

虽然虚拟地址让进程使用的内存在物理内存上是离散的，但是很多时候进程需要一定量连续物理内存，如果大量碎片存在，就会造成无法启动进程的问题。

### **分类**

所谓碎片，是只由内存分配器管理的内存中，尚未被使用且可能无法被分配使用的空间。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7084.tmp.jpg) 

#### **外部碎片**

按照碎片的位置和产生原因，内存碎片分为外部碎片和内部碎片：

外部碎片是***\*进程与进程间\****未分配的内存空间，外部碎片的出现和进程频繁的分配和释放内存有直接关系。

#### **内部碎片**

内部碎片主要因为***\*分配器粒度问题以及一些地址限制导致实际分配的内存大于所需内存\****，这样在***\*进程内部\****就会出现内存空洞。

虽然虚拟地址让进程使用的内存在物理内存上是离散的，但是很多时候进程需要一定量连续物理内存，如果大量碎片存在，就会造成无法启动进程的问题。

 

​	碎片分为内部碎片和外部碎片。

​	**外部碎片：**即***\*无法被使用的空闲块\****，比如内存分配器发现有多个24B的分散空闲块，总的空闲空间足够，但却无法分配出32B的空间。

​	**内部碎片：**由***\*字节对齐导致的不可用空间\****。比如8字节对齐下，申请50B实际需要分配56B，多出的6B即为内部碎片。

 

外部碎片指的是还没有被分配出去（不属于任何进程），但由于太小了无法分配给申请内存空间的新进程的内存空闲区域3)组织结构。

把所有的空闲页分组为 11 个块链表，每个块链表分别包含大小为 1，2，4，8，16，32，64，128，256，512 和 1024 个连续页框的页块。最大可以申请 1024 个连续页，对应 4MB 大小的连续内存。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7095.tmp.jpg) 

 

### **特点**

#### **优点**

优点：提高分配速度，便于内存管理，防止内存泄露

#### **缺点**

缺点：大量的内存碎片会使系统缓慢，内存使用率低，浪费大

 

### **如何避免内存碎片**

1、少用动态内存分配的函数(尽量使用栈空间)

2、分配内存和释放的内存尽量在同一个函数中

3、尽量一次性申请较大的内存，而不要反复申请小内存

4、尽可能申请大块的 2 的指数幂大小的内存空间

**5、*****\*外部碎片避免——伙伴系统算法\****

**6、*****\*内部碎片避免——slab算法\****

7、自己进行内存管理工作，设计内存池

### **说明**

​	外部碎片的典型例子是简单分离链表，前面已经给出了导致大量空间浪费的例子。

​	***\*内部碎片的典型例子是伙伴系统\****，由于分配时向上舍入到2的幂次，其内部碎片问题相当严重。

​	前面已经看到如何通过块的合并来解决可能的碎片。但实际上，曾有人给出论证说明，对任意的内存分配器，都可以找到相应的分配-释放队列，使得该分配器产生碎片问题，只是有的分配器在面向多数场景时不易发生碎片问题。

 

## Buddy伙伴系统算法

### **背景**

***\*解决外部碎片的思路：\****

第一种思路：把已经存在的外部碎片通过新的技术把这些非连续的空闲内存映射到连续的线性空间，其实相当于没有去降低外部碎片的产生而是治理型方案，但是这种方案在真实需要连续物理内存时是无效的。

第二种思路：把这些小的空闲的不连续内存记录在案，如果有新的分配需求就从中搜索合适的将空闲内存分配出去，这样就避免了在新的区域进行分配内存，有种变废为宝的感觉，其实这样场景也很熟悉当你想吃一包饼干时，你妈妈肯定会说先把之前剩下一半没吃完的吃掉，不要先开新的了。

基于一些其他方面的考量，linux内核选择了第二种思路来解决外部碎片。

 

***\*伙伴内存块的定义：\****

在伙伴系统中把大小相同且物理地址连续的两块内存区域称为伙伴，连续地址的要求其实是比较苛刻了，但是这也是算法的关键，因为这样的两块内存区域可以合并成一块更大的区域。

 

***\*伙伴系统的核心思想：\****

伙伴系统将不同大小的连续物理页框进行管理，在申请时从最接近的页框大小进行分配，剩余的进行新的拆解，并将有伙伴关系的内存会进行合并成为大的页框。

 

### **概念**

伙伴系统算法是解决外部碎片的有力工具，简单来说它针对频繁请求和释放不同大小的一组连续页框的场景，建立一套管理机制来高效的分配和回收资源，降低外部碎片。

为内核提供了一种用于分配一组连续的页而建立的一种高效的分配策略，并有效的解决了外碎片问题。

分配的内存区是以页框为基本单位的。

 

### **外部碎片**

外部碎片指的是还没有被分配出去（不属于任何进程），但由于太小了无法分配给申请内存空间的新进程的内存空闲区域3)  

把所有的空闲页分组为11个块链表，每个块链表分别包含大小为1，2，4，8，16，32，64，128，256，512和1024个连续页框的页块。最大可以申请1024个连续页，对应4MB大小的连续内存。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7096.tmp.jpg) 

### **申请和回收**

伙伴系统维护了n=0~10共11个块链表，每个块链表分别包含了大小为2^n个连续的物理页。当n=10时即1024个4KB页对应4MB大小的连续物理内存块，这里的n称为order，在伙伴系统中order为0～10，也就是最小的是4KB，最大的内存块是4MB，这些相同大小的物理块组成双向链表进行管理，如图展示了order=0和order=2的两个双向链表的情况：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7097.tmp.jpg) 

***\*申请内存过程：\****假设请求一个页框块，伙伴系统算法先在order=0的链表中检查是否有空闲块可分配。如果没有则查找下一个更大的块，在order=1的链表中找一个空闲块，链表中存在就把2个页框拆分，1个页框分配出去1个页框加入到order=0的链表中。如果order=1的链表中仍未找到空闲块，就继续向更大的order搜索，如果找到进行拆分处理，如果最终至order=10的链表也没有空闲块，则算法报错。

合并内存过程：合并内存的过程是伙伴算法中伙伴块的体现，算法把两个块具有相同大小 A且它们物理地址连续的内存合并为一个大小为2A的单独块。伙伴算法是自底向上迭代合并的，其实这个过程和leveldb中sst的合并过程很相似，区别在于伙伴算法要求内存块是连续的，这个过程也体现了伙伴系统对于大块内存的友好。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7098.tmp.jpg) 

#### **申请算法**

1、申请2^i个页块存储空间，如果2^i对应的块链表有空闲页块，则分配给应用

2、如果没有空闲页块，则查找2^(i 1)对应的块链表是否有空闲页块，如果有，则分配2^i块链表节点给应用，另外2^i块链表节点插入到2^i对应的块链表中

3、如果2^(i 1)块链表中没有空闲页块，则重复步骤2，直到找到有空闲页块的块链表

4、如果仍然没有，则返回内存分配失败

 

#### **回收算法**

1、释放2^i个页块存储空间，查找2^i个页块对应的块链表，是否有与其物理地址是连续的页块，如果没有，则无需合并

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70A8.tmp.jpg) 

2、如果有，则合并成2^（i 1）的页块，以此类推，继续查找下一级块链接，直到不能合并为止

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70A9.tmp.jpg) 

#### **条件**

1、两个块具有相同的大小

2、它们的物理地址是连续的

3、页块大小相同

 

### **如何分配 4M 以上内存？**

1、为何限制大块内存分配

分配的内存越大, 失败的可能性越大

大块内存使用场景少

 

​	2、内核中获取4M以上大内存的方法

修改MAX_ORDER, 重新编译内核

内核启动选型传递"mem="参数, 如"mem=80M，预留部分内存；然后通过

request_mem_region 和 ioremap_nocache 将预留的内存映射到模块中。需要修改内核启动参数, 无需重新编译内核. 但这种方法不支持 x86 架构, 只支持 ARM, PowerPC 等非 x86 架构

在 start_kernel 中 mem_init 函数之前调用 alloc_boot_mem 函数预分配大块内存, 需要重新编译内核

vmalloc 函数，内核代码使用它来分配在虚拟内存中连续但在物理内存中不一定连续的内存

 

### **特点**

#### **优点**

伙伴系统算法较好地解决了外部碎片问题，并且对于大内存块的分配比较友好小粒度的内存可能造成内部碎片。

 

#### **缺点**

伙伴系统对于伙伴块的定义很苛刻，并且在合并伙伴块的过程涉及较多的链表操作，对于一些频繁的申请可能刚合并就会被拆分掉，这就做了无用功，所以伙伴系统还是存在一些问题的。

 

### **反碎片机制**

#### **不可移动页**

这些页在内存中有固定的位置，不能够移动，也不可回收

内核代码段，数据段，内核 kmalloc() 出来的内存，内核线程占用的内存等

 

#### **可回收页**

这些页不能移动，但可以删除。内核在回收页占据了太多的内存时或者内存短缺时进行页面回收

#### **可移动页**

这些页可以任意移动，用户空间应用程序使用的页都属于该类别。它们是通过页表映射的

当它们移动到新的位置，页表项也会相应的更新

 

## **slab算法**

一般来说，内核对象的生命周期是这样的：分配内存-初始化-释放内存，内核中有大量的小对象，比如文件描述结构对象、任务描述结构对象，如果按照伙伴系统按页分配和释放内存，对小对象频繁的执行分配内存-初始化-释放内存会非常消耗性能。

伙伴系统分配出去的内存还是以页框为单位，而对于内核的很多场景都是分配小片内存，远用不到一页内存大小的空间。slab分配器，通过将内存按使用对象不同再划分成不同大小的空间，应用于内核对象的缓存。

伙伴系统和slab不是二选一的关系，slab 内存分配器是对伙伴分配算法的补充。

### **背景**

Linux所使用的slab分配器的基础是Jeff Bonwick为SunOS操作系统首次引入的一种算法。Jeff的分配器是围绕对象缓存进行的。在内核中，会为有限的对象集（例如文件描述符和其他常见结构）分配大量内存。Jeff发现对内核中普通对象进行初始化所需的时间超过了对其进行分配和释放所需的时间。因此他的结论是不应该将内存释放回一个全局的内存池，而是将内存保持为针对特定初始化的状态。

 

***\*从伙伴系统的介绍可以知道其分配的最小单位是4KB的页框，这对于一些频繁申请的小到几十字节的内存来说还是十分浪费的\****，所以我们需要更细粒度的分配器，这就是slab分配器。

slab分配器并不是和伙伴系统分立的，而是***\*建立在伙伴系统之上\****，可以看作是伙伴系统的二级分销商，更加靠近用户侧，但是slab分配器因为更靠近使用方，因此在结构实现上比伙伴系统更加复杂。

如果你要创建和撤销很大的数据结构，那么考虑建立slab高速缓存。Slab层会给诶个处理器维持一个对象高速缓存（空闲链表），这种高速缓存会极大地提高对象分配和回收的性能。Slab层不是频繁地分配和释放内存，而是为你把事先分配好的对象存放到高速缓存中。当你需要一块新的内存来存放数据结构时，slab层一般无须另外去分配内存，而只需要从高速缓存中得到一个对象就可以了。

 

### **基本****原理**

它的基本思想是将内核中经常使用的对象放到高速缓存中，并且由系统保持为初始的可利用状态。比如进程描述符，内核中会频繁对此数据进行申请和释放。

 

***\*内部碎片\****

已经被分配出去的的内存空间大于请求所需的内存空间

***\*基本目标\****

减少伙伴算法在分配小块连续内存时所产生的内部碎片

将频繁使用的对象缓存起来，减少分配、初始化和释放对象的时间开销

通过着色技术调整对象以更好的使用硬件高速缓存

 

### **slab分配器结构**

由于对象是从slab中分配和释放的，因此单个slab可以在slab列表之间进行移动

slabs_empty列表中的slab是进行回收（reaping）的主要备选对象

slab还支持通用对象的初始化，从而避免了为同一目而对一个对象重复进行初始化

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70AA.tmp.jpg) 

### **slab高速缓存**

#### **普通高速缓存**

slab分配器所提供的小块连续内存的分配是通过通用高速缓存实现的。

通用高速缓存所提供的对象具有几何分布的大小，范围为32到131072字节。

内核中提供了kmalloc()和kfree()两个接口分别进行内存的申请和释放。

 

#### **专用高速缓存**

内核为专用高速缓存的申请和释放提供了一套完整的接口，根据所传入的参数为具体的对象分配slab缓存。

kmem_cache_create()用于对一个指定的对象创建高速缓存。它从 cache_cache 普通高速缓存中为新的专有缓存分配一个高速缓存描述符，并把这个描述符插入到高速缓存描述符形成的cache_chain链表中。

kmem_cache_alloc()在其参数所指定的高速缓存中分配一slab。相反，kmem_cache_free()在其参数所指定的高速缓存中释放一个slab。

 

### **优点**

1、slab内存管理基于内核小对象，不用每次都分配一页内存，充分利用内存空间，避免内部碎片。

2、slab对内核中频繁创建和释放的小对象做缓存，重复利用一些相同的对象，减少内存分配次数。

 

## **内核态内存池**

### **基本原理**

先申请分配一定数量的、大小相等(一般情况下) 的内存块留作备用

当有新的内存需求时，就从内存池中分出一部分内存块，若内存块不够再继续申请新的内存

这样做的一个显著优点是尽量避免了内存碎片，使得内存分配效率得到提升

 

### **内核API**

mempool_create 创建内存池对象

mempool_alloc 分配函数获得该对象

mempool_free 释放一个对象

mempool_destroy 销毁内存池

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70AB.tmp.jpg) 

## **用户态内存池**

 

## **DMA 内存**

### **什么是 DMA**

直接内存访问是一种硬件机制，它允许外围设备和主内存之间直接传输它们的I/O数据，而不需要系统处理器的参与。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70BC.tmp.jpg) 

***\*DMA控制器的功能\****

1、能向CPU发出系统保持（HOLD）信号，提出总线接管请求

2、当CPU发出允许接管信号后，负责对总线的控制，进入DMA方式

3、能对存储器寻址及能修改地址指针，实现对内存的读写操作

4、能决定本次DMA传送的字节数，判断DMA传送是否结束

5、发出DMA结束信号，使CPU 恢复正常工作状态

 

### **DMA 信号**

DREQ：DMA请求信号。是外设向DMA控制器提出要求，DMA 操作的申请信号

DACK：DMA响应信号。是DMA控制器向提出DMA请求的外设表示已收到请求和正进行处理的信号

HRQ：DMA控制器向CPU发出的信号，要求接管总线的请求信号。

HLDA：CPU向DMA控制器发出的信号，允许接管总线的应答信号：

 

# **内存管理接口**

## **内存分配函数**

### 用户态内存分配函数

#### alloca

#### **malloc**

#### **calloc**

#### **realloc**

1、alloca是向栈申请内存,因此无需释放

2、malloc所分配的内存空间未被初始化，使用malloc()函数的程序开始时(内存空间还没有被重新分配) 能正常运行，但经过一段时间后(内存空间已被重新分配) 可能会出现问题

3、calloc会将所分配的内存空间中的每一位都初始化为零

4、realloc扩展现有内存空间大小

4.1 如果当前连续内存块足够 realloc的话，只是将p所指向的空间扩大，并返回p的指针地址。这个时候q和p指向的地址是一样的

4.2 如果当前连续内存块不够长度，再找一个足够长的地方，分配一块新的内存q，并将p指向的内容copy到q，返回 q。并将p所指向的内存空间删除。

 

### **内核态内存分配函数**

#### **alloc_pages**

#### **kmalloc**

#### **vmalloc**

函数分配原理最大内存其他_get_free_pages直接对页框进行操作4MB适用于分配较大量的连续物理内存kmem_cache_alloc。

基于slab机制实现128KB适合需要频繁申请释放相同大小内存块时使用kmalloc。

基于kmem_cache_alloc实现128KB最常见的分配方式，需要小于页框大小的内存时可以使用vmalloc建立非连续物理内存到虚拟地址的映射物理不连续，适合需要大内存，但是对地址连续性没有要求的场合dma_alloc_coherent。

基于_alloc_pages 实现4MB适用于DMA操作ioremap实现已知物理地址到虚拟地址的映射适用于物理地址已知的场合，如设备驱动alloc_bootmem在启动 kernel时，预留一段内存，内核看不见小于物理内存大小，内存管理要求较高。

 

## **C**

### **内存申请**

#### **malloc**

malloc用于申请用户空间的虚拟内存，当申请小于128KB小内存的时，malloc使用sbrk或brk分配内存；当申请大于128KB的内存时，使用mmap函数申请内存。

***\*延时分配和实时分配\****

linux系统有内核态和用户态之分，内核态申请内存就立刻满足并且认为这个请求一定是合理的。然而用户态申请内存的请求，总是尽量延后分配物理内存，所以用户态进程是先获得一个虚拟内存区，在运行时通过缺页异常获得一块真正的物理内存，我们执行 malloc 时获取的只是虚拟内存而已，并不是真实的物理内存，也是这个原因造成的。

 

​	***\*函数原型：\****

​	(void *)malloc(unsigned size);  //字节数

​	***\*说明：\****

​	malloc函数在内存中开辟的是一块连续的空间，size是所需要空间的长度，开辟的大小为size *参数类型，开辟完之后返回这块空间的首地址。

***\*存在问题\*******\*：\****

由于brk/sbrk/mmap属于系统调用，如果每次申请内存都要产生系统调用开销，cpu在用户态和内核态之间频繁切换，非常影响性能。

而且，堆是从低地址往高地址增长，如果低地址的内存没有被释放，高地址的内存就不能被回收，容易产生内存碎片。

***\*解决\*******\*：\****

因此，malloc采用的是内存池的实现方式，先申请一大块内存，然后将内存分成不同大小的内存块，然后用户申请内存时，直接从内存池中选择一块相近的内存块分配出去。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70BD.tmp.jpg) 

 

kmalloc和vmalloc分别用于分配不同映射区的虚拟内存：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70BE.tmp.jpg) 

#### **kmalloc**

kmalloc()分配的虚拟地址范围在内核空间的直接内存映射区。

按字节为单位虚拟内存，一般用于分配小块内存，释放内存对应于kfree，可以分配连续的物理内存。函数原型在<linux/kmalloc.h>中声明，一般情况下在驱动程序中都是调用kmalloc()来给数据结构分配内存。

注：如果需要整个页，则可以使用内核的alloc_pages申请整个页。

 

***\*kmalloc是基于slab分配器的\****，同样可以用cat /proc/slabinfo命令，查看 kmalloc相关slab对象信息，下面的kmalloc-8、kmalloc-16等等就是基于slab分配的kmalloc高速缓存。

 

#### **vmalloc**

vmalloc分配的***\*虚拟地址区间\****，位于vmalloc_start与vmalloc_end之间的动态内存映射区。

一般用分配大块内存，释放内存对应于vfree，分配的虚拟内存地址连续，物理地址上不一定连续（kmalloc分配地址是连续的）。函数原型在<linux/vmalloc.h>中声明。一般用在为活动的交换区分配数据结构，为某些I/O驱动程序分配缓冲区，或为内核模块分配空间。

 

下图总结了上述两种内核空间虚拟内存分配方式。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70CF.tmp.jpg) 

#### **calloc**

​	函数原型：

​	void * calloc(size_t numElements, size_t sizeOfElements);  

//(元素的个数,单个元素的字节数)

说明：

和malloc相似，它也是开辟一块连续的空间，空间大小为：元素的个数*单个元素的字节数。

#### **realloc**

​	函数原型：

​	void* realloc(void* ptr, unsigned newsize);  //(地址,字节数)

​	说明：

​	给一个已经分配地址的指针重新分配空间，参数ptr为原有的空间指针，newsize为重新申请的地址长度。它与malloc的区别就是如果你给的指针是NULL，那么你使用的就是malloc，如果你给出的指针是一个已经分配了地址的指针（ptr），那么你使用的就是realloc。

 

​	上述三种申请内存方式的区别：

1、 函数malloc不能初始化所分配的空间，而函数calloc可以初始化。也就是说，如果由malloc函数分配的空间原来没有被分配过，则其中每一位都可能是0；反之，如果这一块数据块原来被分配过，那里面可能遗留着各种各样的数据。所以，当你在使用malloc开辟一块新空间的时候，要重新初始化那一块空间（一般调用memset函数来初始化空间），否则在多次释放开辟之后，可能会出现使用错误。

2、 calloc函数会将所分配的内存空间中的每一位都初始化为0（这也是它和malloc的主要不同之一）。也就是说，如果你是为字符类或者整型类的元素分配空间，那么这些元素会保证被初始化为0；如果你是为指针类函数分配内存，那么这些元素都会被初始化为空指针。

3、 malloc向系统申请size个字节的空间，申请完之后返回的是这个空间的首地址，类型为void *，而void*表示未确定的类型，在C/C++中void*被强转成任意类型的指针。

4、 realloc可以对给定的指针所指向的空间进行扩大或者缩小，无论是扩大还是缩小，原有内存中的内容将保持不变（如果对于缩小之后的空间，被缩小的那部分空间的数据还是会丢失）。realloc并不保证调整后的内存空间和原来的内存空间保持同一个地址。相反，realloc指针很可能指向一个新的地址。

5、 realloc是从堆上分配空间的，但当你进行扩大的时候，realloc会试图从堆上现存的数据后面的那些字节中获取附加的字节，如果能满足，就刚好。但如果后面的字节数不够，其就会使用堆上第一个有足够大小的自由块，然后将现存的数据拷贝到新的位置，将老块放回到堆上。在这个过程中，数据会被移动。也就是说，当你使用realloc的时候，数据可能被移动。

### **内存释放**

#### **free**

​	有分配就一定有释放。malloc对应的就是free函数，free函数只有一个参数，就是要释放的内存块的首地址。

​	free函数做的事情：斩断指针变量与这块 内存的关系。就像上面的例子中malloc开辟的这一个数据块空间是属于p，你只能通过p来访问这个数据块空间，而free函数做的事情就是斩断malloc和p之间的联系。但是p指针本身存放的地址并没有发生变化，只有它对指针指向的那块内存已经没有所有权了，不能对内存块进行操作。而那块内存块里面的数据也没有被改变，只有你没有办法去访问或者修改那块数据块中的内容中。

​	malloc和free是一一对应的，如果malloc两次但是只free一次就会存在内存泄露，如果malloc一次但是free两次，就会出错（第一次使用free的时候，malloc所开辟的空间就已经被释放，第二次使用free就无内存空间可以释放了，这种对内存的误操作就有可能会导致程序的崩溃）。

### **内存拷贝**

#### **memcpy**

​	函数原型：

​	void* memcpy(void* dest, const void*src,size_t n);

​	说明：

​	从源src所指的内存地址的起始位置开始拷贝n个字节到目标dest所指的内存地址的起始位置中。

​	这里需要对memcpy这个函数做一些说明，memcpy函数后经常还会有一个malloc函数。

#### **memmove**

​	函数原型：

​	void* memmove(void* dest, const void* src, size_t count);

​	说明：

​	由src所指内存区域复制count个字节到dest所指内存区域。

### **字符串拷贝**

#### **bcopy**

​	主要作用是将字符串src的前n个字节复制到dest中

### **内存清零**

#### **memset**

​	函数原型：

​	void* memset(void* s ,int ch, size_t n);

​	说明：

​	该函数是将s中当前位置后面的n个字节（typedef unsigned intsize_t）的ch替换并返回s。作用是在一段内存块中填充某个指定的值，它是对较大的结构体或数组进行清零的一种最快方法。

## **C++**

### **内存申请**

#### **new**

​	我们知道C++是兼容C语言的，那么已经有malloc和free来进行动态内存管理，为什么还需要C++定义new和delete运算符来动态管理内存？

​	它们的区别和联系：

1、 都是动态管理内存的入口

2、 malloc/free是C/C++标准库的函数，new/delete是C++操作符

3、 malloc/free只是动态分配/释放内存空间，而new/delete出来分配空间还会调用构造函数和析构函数进行初始化和清理

4、 malloc/free需要手动计算类型大小且会返回void*，new/delett可以自己计算类型的大小，返回对应类型的指针

我们在C++中运行重载的，那么我们也可以重载new和delete（其实new和delete是不同重载了，即使你进行了重载，也只是重载了operator new和operator delete）。

### **内存释放**

#### delete

## **Java**

# **零拷贝**

## **背景**

一般来说，服务端发送一个文件给客户端的步骤如下：

1、首先需要调用read读取文件的数据到用户空间缓冲区中。

2、然后再调用write把缓冲区的数据发送给客户端Socket。

***\*伪代码如下：\****

while((n = read(diskfd, buf, BUF_SIZE)) > 0)

write(sockfd, buf , n);

基本操作就是循环的从磁盘读入文件内容到缓冲区，再将缓冲区的内容发送到socket。但是由于***\*Linux的I/O操作默认是缓冲I/O\****。这里面主要使用的也就是read和write两个系统调用，我们并不知道操作系统在其中做了什么。***\*实际上在以上I/O操作中，发生了多次的数据拷贝\****。

当应用程序访问某块数据时，操作系统首先会检查，是不是最近访问过此文件，文件内容是否缓存在内核缓冲区，如果是，操作系统则直接根据read系统调用提供的buf地址，将内核缓冲区的内容拷贝到buf所指定的用户空间缓冲区中去。如果不是，操作系统则首先将磁盘上的数据拷贝的内核缓冲区，这一步目前主要依靠***\*DMA来传输\****，然后再把内核缓冲区上的内容拷贝到用户缓冲区中。

接下来，write系统调用再把用户缓冲区的内容拷贝到网络堆栈相关的内核缓冲区中，最后socket再把内核缓冲区的内容发送到网卡上。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70D0.tmp.jpg) 

数据拷贝

从上图中可以看出，共产生了四次数据拷贝，即使使用了DMA来处理了与硬件的通讯，CPU仍然需要处理两次数据拷贝，与此同时，在用户态与内核态也发生了多次上下文切换，无疑也加重了CPU负担。

在此过程中，我们没有对文件内容做任何修改，那么在内核空间和用户空间来回拷贝数据无疑就是一种浪费，而零拷贝主要就是为了解决这种低效性。

### **read**

在上面的过程中，调用了read和write两个系统调用。read系统调用是从文件中读取数据到用户空间的缓冲区中，所以调用read时需要从内核空间复制数据到用户空间，如图所示：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70D1.tmp.jpg) 

上图就是数据的复制过程，首先会从文件中读取数据到内核的页缓存（page cache），然后再从页缓存中复制到用户空间的缓冲区中。

### **write**

而当调用write系统调用把用户空间缓冲区中的数据发送到客户端Socket时，首先会把缓冲区的数据复制到内核的Socket缓冲区中，网卡驱动会把Socket缓冲区的数据发送出去，如图所示：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70D2.tmp.jpg) 

从上图可以看出，服务端发送文件给客户端的过程中需要进行两次数据复制，第一次是从内核空间的页缓存复制到用户空间的缓冲区，第二次是从用户空间的缓冲区复制到内核空间的Socket缓冲区。

### **sendfile**

仔细观察我们可以发现，上图中的页缓存其实可以直接复制到Socket缓冲区，而不需要复制到用户空间缓冲区的。如图所示：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70E2.tmp.jpg) 

如上图所示，不需要用户空间作为数据中转的技术叫零拷贝技术。那么，我们可以通过哪个系统调用来实现上图中的技术呢？答案就是sendfile。

 

## **概述**

零拷贝主要的任务就是避免CPU将数据从一块存储拷贝到另外一块存储，主要就是利用各种零拷贝技术，避免让CPU做大量的数据拷贝任务，减少不必要的拷贝，或者让别的组件来做这一类简单的数据传输任务，让CPU解脱出来专注于别的任务。这样就可以让系统资源的利用更加有效。

我们如何减少数据拷贝的次数呢？一个很明显的着力点就是减少数据在内核空间和用户空间来回拷贝，这也引入了零拷贝的一个类型：***\*让数据传输不需要经过user space\****。

## **实现**

传统的I/O操作读取文件并通过Socket发送，需要经过4次上下文切换、2次CPU数据拷贝和2次DMA控制器数据拷贝，如下图：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70E3.tmp.jpg) 

从中也可以看得出提高性能可以从减少数据拷贝和上下文切换的次数着手，在Linux操作系统层面上有4种实现方案：内存映射mmap、sendfile、splice、tee，这些实现中或多多少的减少数据拷贝次数或减少上下文切换次数。

操作系统层面的减少数据拷贝次数主要是指用户空间和内核空间的数据拷贝，因为只有他们的拷贝是大量消耗CPU时间片的，而DMA控制器拷贝数据CPU参与的工作较少，只是辅助作用。

现实中对零拷贝的概念有广义和狭义之分，广义上是指只要减少了数据拷贝的次数就称之为零拷贝；狭义上是指真正的零拷贝，比如上例中避免2和3的CPU拷贝。

下面我们逐一看看他们的设计思想和实现方案：

### **mmap**

我们减少拷贝次数的一种方法是调用mmap()来代替read调用：

buf = mmap(diskfd, len);

write(sockfd, buf, len);

应用程序调用mmap()，磁盘上的数据会通过DMA被拷贝的内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用write(),操作系统直接将内核缓冲区的内容拷贝到socket缓冲区中，这一切都发生在内核态，最后，socket缓冲区再把数据发到网卡去。同样的，看图很简单：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70E4.tmp.jpg) 

使用mmap替代read很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。

#### **原理**

既然是内存映射，首先来了解解下虚拟内存和物理内存的映射关系，虚拟内存是操作系统为了方便操作而对物理内存做的抽象，他们之间是靠页表(Page Table)进行关联的，关系如下：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70E5.tmp.jpg) 

每个进程都有自己的PageTable，进程的虚拟内存地址通过PageTable对应于物理内存，内存分配具有惰性，它的过程一般是这样的：进程创建后新建与进程对应的PageTable，当进程需要内存时会通过PageTable寻找物理内存，如果没有找到对应的页帧就会发生缺页中断，从而创建PageTable与物理内存的对应关系。虚拟内存不仅可以对物理内存进行扩展，还可以更方便地灵活分配，并对编程提供更友好的操作。

内存映射(mmap)是指用户空间和内核空间的虚拟内存地址同时映射到同一块物理内存，用户态进程可以直接操作物理内存，避免用户空间和内核空间之间的数据拷贝。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70F6.tmp.jpg) 

它的具体执行流程是这样的：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70F7.tmp.jpg) 

1、用户进程通过系统调用mmap函数进入内核态，发生第1次上下文切换，并建立内核缓冲区；

2、发生缺页中断，CPU通知DMA读取数据；

3、DMA拷贝数据到物理内存，并建立内核缓冲区和物理内存的映射关系；

4、建立用户空间的进程缓冲区和同一块物理内存的映射关系，由内核态转变为用户态，发生第2次上下文切换；

5、用户进程进行逻辑处理后，通过系统调用Socket send，用户态进入内核态，发生第3次上下文切换；

6、系统调用Send创建网络缓冲区，并拷贝内核读缓冲区数据；

7、DMA控制器将网络缓冲区的数据发送网卡，并返回，由内核态进入用户态，发生第4次上下文切换；

 

#### **问题**

但是使用mmap是有代价的。当你使用mmap时，你可能会遇到一些隐藏的陷阱。例如，当你的程序map了一个文件，但是当这个文件被另一个进程截断(truncate)时, write系统调用会因为访问非法地址而被SIGBUS信号终止。SIGBUS信号默认会杀死你的进程并产生一个coredump,如果你的服务器这样被中止了，那会产生一笔损失。

通常我们使用以下解决方案避免这种问题：

***\*1、为SIGBUS信号建立信号处理程序\****

当遇到SIGBUS信号时，信号处理程序简单地返回，write系统调用在被中断之前会返回已经写入的字节数，并且errno会被设置成success,但是这是一种糟糕的处理办法，因为你并没有解决问题的实质核心。

***\*2、使用文件租借锁\****

通常我们使用这种方法，在文件描述符上使用租借锁，我们为文件向内核申请一个租借锁，当其它进程想要截断这个文件时，内核会向我们发送一个实时的RTSIGNALLEASE信号，告诉我们内核正在破坏你加持在文件上的读写锁。这样在程序访问非法内存并且被SIGBUS杀死之前，你的write系统调用会被中断。write会返回已经写入的字节数，并且置errno为success。

我们应该在mmap文件之前加锁，并且在操作完文件后解锁：

if(fcntl(diskfd, F_SETSIG, RT_SIGNAL_LEASE) == -1) {

  perror("kernel lease set signal");

return -1;

}

/* l_type can be F_RDLCK F_WRLCK  加锁*/

/* l_type can be  F_UNLCK 解锁*/

if(fcntl(diskfd, F_SETLEASE, l_type)){

  perror("kernel lease set type");

return -1;

}

#### **总结**

避免了内核空间和用户空间的2次CPU拷贝，但增加了1次内核空间的CPU拷贝，整体上相当于只减少了1次CPU拷贝；

针对大文件比较适合mmap，小文件则会造成较多的内存碎片，得不偿失；

当mmap一个文件时，如果文件被另一个进程截获可能会因为非法访问导致进程被SIGBUS 信号终止；

 

### **sendfile**

从2.1版内核开始，Linux引入了sendfile来简化操作:

\#include <sys/sendfile.h>

ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);

下面介绍一下sendfile各个参数的作用：

out_fd：数据接收方文件句柄（一般为Socket句柄）。

in_fd：数据提供方文件句柄（一般为文件句柄）。

offset：如果offset不为NULL，表示从哪里开始发送数据的偏移量。

count：表示需要发送多少字节的数据。

***\*说明：\****

系统调用sendfile()在代表输入文件的描述符infd和代表输出文件的描述符outfd之间传送文件内容（字节）。

描述符outfd必须指向一个套接字，而infd指向的文件必须是可以mmap的。这些局限限制了sendfile的使用，使sendfile只能将数据从文件传递到套接字上，反之则不行。

使用sendfile不仅减少了数据拷贝的次数，还减少了上下文切换，数据传送始终只发生在kernel space。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70F8.tmp.jpg) 

在我们调用sendfile时，如果有其它进程截断了文件会发生什么呢？

假设我们没有设置任何信号处理程序，sendfile调用仅仅返回它在被中断之前已经传输的字节数，errno会被置为success。如果我们在调用sendfile之前给文件加了锁，sendfile的行为仍然和之前相同，我们还会收到RTSIGNALLEASE的信号。

#### **原理**

它的内部执行流程是这样的：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps70F9.tmp.jpg) 

1、用户进程系统调用senfile，由用户态进入内核态，发生第1次上下文切换；

2、CPU通知DMA控制器把文件数据拷贝到内核缓冲区；

3、内核空间自动调用网络发送功能并拷贝数据到网络缓冲区；

4、CPU通知DMA控制器发送数据；

5、sendfile系统调用结束并返回，进程由内核态进入用户态，发生第2次上下文切换；

 

#### **总结**

数据处理完全是由内核操作，减少了2次上下文切换，整个过程2次上下文切换、1次CPU拷贝，2次DMA拷贝；

虽然可以设置偏移量，但不能对数据进行任何的修改；

 

### **sendfile+DMA gather**

#### **原理**

目前为止，我们已经减少了数据拷贝的次数了，但是仍然存在一次拷贝，就是页缓存到socket缓存的拷贝。那么能不能把这个拷贝也省略呢？

Linux2.4对sendfile进行了优化，为DMA控制器引入了gather功能，就是在不拷贝数据到网络缓冲区，而是将待发送数据的内存地址和偏移量等描述信息存在网络缓冲区，DMA根据描述信息从内核的读缓冲区截取数据并发送。它的流程是如下：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7109.tmp.jpg) 

***\*过程分析：\****

用户进程系统调用senfile，由用户态进入内核态，发生第1次上下文切换；

CPU通知DMA控制器把文件数据拷贝到内核缓冲区；

把内核缓冲区地址和sendfile的相关参数作为数据描述信息存在网络缓冲区中；

CPU通知DMA控制器，DMA根据网络缓冲区中的数据描述截取数据并发送；

sendfile系统调用结束并返回，进程由内核态进入用户态，发生第2次上下文切换；

 

借助于硬件上的帮助，我们是可以办到的。之前我们是把页缓存的数据拷贝到socket缓存中，实际上，我们仅仅需要把缓冲区描述符传到socket缓冲区，再把数据长度传过去，这样DMA控制器直接将页缓存中的数据打包发送到网络中就可以了。

#### **总结**

需要硬件支持，如DMA；

整个过程2次上下文切换，0次CPU拷贝，2次DMA拷贝，实现真正意义上的零拷贝；

依然不能修改数据；

但那时的sendfile有个致命的缺陷，如果你查看Sendfild手册，你会发现如下描述：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps710A.tmp.jpg) 

in_fd不仅仅不能是socket，而且在2.6.33之前Sendfile的out_fd必须是socket，因此sendfile几乎成了专为网络传输而设计的，限制了其使用范围比较狭窄。2.6.33之后out_fd才可以是任何file，于是乎出现了splice。

 

总结一下，sendfile系统调用利用DMA引擎将文件内容拷贝到内核缓冲区去，然后将带有文件位置和长度信息的缓冲区描述符添加socket缓冲区去，这一步不会将内核中的数据拷贝到socket缓冲区中，DMA引擎会将内核缓冲区的数据拷贝到协议引擎中去，避免了最后一次拷贝。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps710B.tmp.jpg) 

带DMA的sendfile

不过这一种收集拷贝功能是需要硬件以及驱动程序支持的。

### **splice**

鉴于Sendfile的缺点，在Linux2.6.17中引入了Splice，它在读缓冲区和网络操作缓冲区之间建立管道避免CPU拷贝：先将文件读入到内核缓冲区，然后再与内核网络缓冲区建立管道。

 

sendfile只适用于将数据从文件拷贝到套接字上，限定了它的使用范围。Linux在2.6.17版本引入splice系统调用，用于在两个文件描述符中移动数据：

\#define _GNU_SOURCE     

/* See feature_test_macros(7) */

\#include <fcntl.h>

ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsignedint flags);

splice调用在两个文件描述符之间移动数据，而不需要数据在内核空间和用户空间来回拷贝。他从fdin拷贝len长度的数据到fdout，但是有一方必须是管道设备，这也是目前splice的一些局限性。flags参数有以下几种取值：

SPLICEFMOVE：尝试去移动数据而不是拷贝数据。这仅仅是对内核的一个小提示：如果内核不能从pipe移动数据或者pipe的缓存不是一个整页面，仍然需要拷贝数据。Linux最初的实现有些问题，所以从2.6.21开始这个选项不起作用，后面的Linux版本应该会实现。

SPLICEFNONBLOCK：splice操作不会被阻塞。然而，如果文件描述符没有被设置为不可被阻塞方式的I/O ，那么调用splice有可能仍然被阻塞。

SPLICEFMORE：后面的splice调用会有更多的数据。

splice调用利用了Linux提出的管道缓冲区机制， 所以至少一个描述符要为管道。

#### **原理**

它的执行流程如下：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps710C.tmp.jpg) 

1、用户进程系统调用splice，由用户态进入内核态，发生第1次上下文切换；

2、CPU通知DMA控制器把文件数据拷贝到内核缓冲区；

3、建立内核缓冲区和网络缓冲区的管道；

4、CPU通知DMA控制器，DMA从管道读取数据并发送；

5、splice系统调用结束并返回，进程由内核态进入用户态，发生第2次上下文切换；

#### **总结**

整个过程2次上下文切换，0次CPU拷贝，2次DMA拷贝，实现真正意义上的零拷贝；

依然不能修改数据；

fd_in和fd_out必须有一个是管道；

 

以上几种零拷贝技术都是减少数据在用户空间和内核空间拷贝技术实现的，但是有些时候，数据必须在用户空间和内核空间之间拷贝。这时候，我们只能针对数据在用户空间和内核空间拷贝的时机上下功夫了。Linux通常利用写时复制(copy on write)来减少系统开销，这个技术又时常称作COW。

### **tee**

tee与splice类同，但fd_in和fd_out都必须是管道。

### **O_DIRECT**

零拷贝技术不单只有sendfile，如mmap、splice和直接I/O等都是零拷贝技术的实现。

## **总结**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps711D.tmp.jpg) 

如果多个程序同时访问同一块数据，那么每个程序都拥有指向这块数据的指针，在每个程序看来，自己都是独立拥有这块数据的，只有当程序需要对数据内容进行修改时，才会把数据内容拷贝到程序自己的应用空间里去，这时候，数据才成为该程序的私有数据。如果程序不需要对数据进行修改，那么永远都不需要拷贝数据到自己的应用空间里。这样就减少了数据的拷贝。

除此之外，还有一些零拷贝技术，比如传统的Linux I/O中加上O_DIRECT标记可以直接I/O，避免了自动缓存，还有尚未成熟的fbufs技术，本文尚未覆盖所有零拷贝技术，只是介绍常见的一些，如有兴趣，可以自行研究，一般成熟的服务端项目也会自己改造内核中有关I/O的部分，提高自己的数据传输速率。

注：MySQL就是使用O_DIRECT方式，自己实现了一套缓存管理机制，这样更加灵活。

 

# **内存使用场景**

内存的使用场景

1、page管理

2、slab（kmalloc、内存池）

3、用户态内存使用（malloc、relloc文件映射、共享内存）

4、程序的内存map（栈、堆、code、data）

5、内核和用户态的数据传递（copy_from_user、copy_to_user）

6、内存映射（硬件寄存器、保留内存）

7、DMA内存

## **用户态内存分配函数**

alloca是向栈申请内存,因此无需释放

malloc所分配的内存空间未被初始化，使用malloc()函数的程序开始时(内存空间还没有被重新分配) 能正常运行，但经过一段时间后(内存空间已被重新分配) 可能会出现问题

calloc会将所分配的内存空间中的每一位都初始化为零

realloc扩展现有内存空间大小

1、如果当前连续内存块足够realloc的话，只是将p所指向的空间扩大，并返回p的指针地址。这个时候q和p指向的地址是一样的

2、如果当前连续内存块不够长度，再找一个足够长的地方，分配一块新的内存，q，并将p指向的内容copy到q，返回q。并将p所指向的内存空间删除

 

## **内核态内存分配函数**

函数分配原理最大内存其他_get_free_pages直接对页框进行操作4MB适用于分配较大量的连续物理内存kmem_cache_alloc基于slab机制实现128KB适合需要频繁申请释放相同大小内存块时使用kmalloc基于kmem_cache_alloc实现128KB最常见的分配方式，需要小于页框大小的内存时可以使用vmalloc建立非连续物理内存到虚拟地址的映射物理不连续，适合需要大内存，但是对地址连续性没有要求的场合dma_alloc_coherent基于_alloc_pages实现4MB适用于DMA操作ioremap实现已知物理地址到虚拟地址的映射适用于物理地址已知的场合，如设备驱动alloc_bootmem在启动kernel时，预留一段内存，内核看不见小于物理内存大小，内存管理要求较高。

## **malloc申请内存**

调用malloc函数时，它沿free_chuck_list连接表寻找一个大到足以满足用户请求所需要的内存块

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps711E.tmp.jpg) 

free_chuck_list 连接表的主要工作是维护一个空闲的堆空间缓冲区链表

如果空间缓冲区链表没有找到对应的节点，需要通过系统调用 sys_brk 延伸进程的栈空间

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps711F.tmp.jpg) 

## **缺页异常**

通过get_free_pages申请一个或多个物理页面

换算addr在进程pdg映射中所在的pte地址

将addr对应的pte设置为物理页面的首地址

系统调用：Brk—申请内存小于等于128kb，do_map—申请内存大于128kb

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7130.tmp.jpg) 

## **用户进程访问内存分析**

用户态进程独占虚拟地址空间，两个进程的虚拟地址可相同

在访问用户态虚拟地址空间时，如果没有映射物理地址，通过系统调用发出缺页异常

缺页异常陷入内核，分配物理地址空间，与用户态虚拟地址建立映射

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7131.tmp.jpg) 

## **共享内存**

### **原理**

它允许多个不相关的进程去访问同一部分逻辑内存

两个运行中的进程之间传输数据，共享内存将是一种效率极高的解决方案

两个运行中的进程共享数据，是进程间通信的高效方法，可有效减少数据拷贝的次数

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7132.tmp.jpg) 

### **Shm接口**

shmget创建共享内存

shmat启动对该共享内存的访问，并把共享内存连接到当前进程的地址空间

shmdt将共享内存从当前进程中分离

# **内存故障**

## **内存泄漏**

### **原因**

1、在类的构造函数和析构函数中没有匹配地调用new和delete函数

2、没有正确地清除嵌套的对象指针

3、没有将基类的析构函数定义为虚函数

4、当基类的指针指向子类对象时，如果基类的析构函数不是virtual，那么子类的析构函数将不会被调用，子类的资源没有得到正确释放，因此造成内存泄露

5、缺少拷贝构造函数，按值传递会调用（拷贝）构造函数，引用传递不会调用

6、指向对象的指针数组不等同于对象数组，数组中存放的是指向对象的指针，不仅要释放每个对象的空间，还要释放每个指针的空间

7、缺少重载赋值运算符，也是逐个成员拷贝的方式复制对象，如果这个类的大小是可变的，那么结果就是造成内存泄露

### **分类**

内存泄漏的分类（按发生方式来分类）

常发性内存泄漏。发生内存泄漏的代码会被多次执行到，每次被执行的时候都会导致一块内存泄漏。

偶发性内存泄漏。发生内存泄漏的代码只有在某些特定环境或操作过程下才会发生。常发性和偶发性是相对的。对于特定的环境，偶发性的也许就变成了常发性的。所以测试环境和测试方法对检测内存泄漏至关重要。

一次性内存泄漏。发生内存泄漏的代码只会被执行一次，或者由于算法上的缺陷，导致总会有一块仅且一块内存发生泄漏。比如，在类的构造函数中分配内存，在析构函数中却没有释放该内存，所以内存泄漏只会发生一次。

隐式内存泄漏。程序在运行过程中不停的分配内存，但是直到结束的时候才释放内存。严格的说这里并没有发生内存泄漏，因为最终程序释放了所有申请的内存。但是对于一个服务器程序，需要运行几天，几周甚至几个月，不及时释放内存也可能导致最终耗尽系统的所有内存。所以，我们称这类内存泄漏为隐式内存泄漏。

 

### **工具**



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7133.tmp.jpg) |

首先是 top free df 三连，结果还真发现了些异常：



我们的探测进程CPU占用率特别高，达到了900%。

我们的Java进程，并不做大量CPU运算，正常情况下，CPU应该在100~200% 之间，出现这种CPU飙升的情况，要么走到了死循环，要么就是在做大量的GC。



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7143.tmp.jpg) |

使用 jstat -gc pid [interval]命令查看了Java进程的GC状态，果然，FULL GC 达到了每秒一次：



这么多的 FULL GC，应该是内存泄漏没跑了，于是使用 jstack pid > jstack.log 保存了线程栈的现场。

使用 jmap -dump:format=b,file=heap.log pid 保存了堆现场，然后重启了探测服务，报警邮件终于停止了。

 

jstat

jstat 是一个非常强大的 JVM 监控工具，一般用法是：

jstat [-options] pid interval

它支持的查看项有：

class 查看类加载信息。

compile 编译统计信息。

gc 垃圾回收信息。

gcXXX 各区域 GC 的详细信息，如 -gcold。

使用它，对定位 JVM 的内存问题很有帮助。

### 排查思路

参考：https://www.ibm.com/developerworks/cn/linux/l-cn-memleak/

有些简单的内存泄漏问题可以从在代码的检查阶段确定。还有些泄漏比较严重的，即在很短的时间内导致程序或系统崩溃，或者系统报告没有足够内存，也比较容易发现。最困难的就是泄漏比较缓慢，需要观测几天、几周甚至几个月才能看到明显异常现象。那么如何在比较短的时间内检测出有没有潜在的内存泄漏问题呢？实际上不同的系统都带有内存监视工具，我们可以从监视工具收集一段时间内的堆栈内存信息，观测增长趋势，来确定是否有内存泄漏。在 Linux 平台可以用 ps 命令，来监视内存的使用，比如下面的命令 (观测指定进程的VSZ值)：

| 1    | ps -aux |
| ---- | ------- |
|      |         |

#### **静态分析**

包括手动检测和静态工具分析，这是代价最小的调试方法。

代码静态扫描和分析的工具比较多，比如 splint, PC-LINT, BEAM 等。

#### **动态分析**

实时检测工具主要有 valgrind, Rational purify 等。

#### **JAVA**

案例：

https://blog.csdn.net/ityouknow/article/details/82230051

https://www.jianshu.com/p/22c5eafbdeb9

 

##### **分析栈**

栈的分析很简单，看一下线程数是不是过多，多数栈都在干嘛：

\> grep 'java.lang.Thread.State' jstack.log  | wc -l

\> 464

才四百多线程，并无异常：

\> grep -A 1 'java.lang.Thread.State' jstack.log  | grep -v 'java.lang.Thread.State' | sort | uniq -c |sort -n

   10   at java.lang.Class.forName0(Native Method)

   10   at java.lang.Object.wait(Native Method)

   16   at java.lang.ClassLoader.loadClass(ClassLoader.java:404)

   44   at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)

  344   at sun.misc.Unsafe.park(Native Method)

线程状态好像也无异常，接下来分析堆文件。

 

##### **下载堆dump文件**

堆文件都是一些二进制数据，在命令行查看非常麻烦，Java 为我们提供的工具都是可视化的，Linux 服务器上又没法查看，那么首先要把文件下载到本地。

由于我们设置的堆内存为 4G，所以 dump 出来的堆文件也很大，下载它确实非常费事，不过我们可以先对它进行一次压缩。

gzip 是个功能很强大的压缩命令，特别是我们可以设置 -1~-9 来指定它的压缩级别。

数据越大压缩比率越大，耗时也就越长，推荐使用 -6~7，-9 实在是太慢了，且收益不大，有这个压缩的时间，多出来的文件也下载好了。

 

##### **使用 MAT 分析 jvm heap**

MAT 是分析 Java 堆内存的利器，使用它打开我们的堆文件（将文件后缀改为 .hprof）, 它会提示我们要分析的种类。

对于这次分析，果断选择 memory leak suspect：

从上面的饼图中可以看出，绝大多数堆内存都被同一个内存占用了，再查看堆内存详情，向上层追溯，很快就发现了罪魁祸首。

 

##### **分析代码**

找到内存泄漏的对象了，在项目里全局搜索对象名，它是一个 Bean 对象，然后定位到它的一个类型为 Map 的属性。

这个 Map 根据类型用 ArrayList 存储了每次探测接口响应的结果，每次探测完都塞到 ArrayList 里去分析。

 

由于 Bean 对象不会被回收，这个属性又没有清除逻辑，所以在服务十来天没有上线重启的情况下，这个 Map 越来越大，直至将内存占满。

 

内存满了之后，无法再给 HTTP 响应结果分配内存了，所以一直卡在 readLine 那里。而我们那个大量 I/O 的接口报警次数特别多，估计跟响应太大需要更多内存有关。

 

#### **C++**

参考：

https://blog.csdn.net/zvall/article/details/53526829

 

##### **top定位高负载进程**

1、使用top定位高负载PID

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7144.tmp.jpg)

通过观察load average，以及负载评判标准（8核），可以确认服务器存在负载较高的情况：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7145.tmp.jpg)

观察各个进程资源使用情况，可以看出进程id为682的进程，有着较高的CPU占比。

##### **找到异常线程**

2、找到异常线程：top -Hp 进程PID

##### **打印线程号**

3、printf “0x%x\n”线程PID

将线程PID转换为 16进制，为后面查找 jstack 日志做准备

##### **pstack/jstack查看线程内存申请释放**

4、pstack/jstack  进程PID | vim +/十六进制线程PID

 

## **内存溢出**

参考：

https://blog.csdn.net/ruiruihahaha/article/details/70270574

内存泄漏memory leak :是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄漏似乎不会有大的影响，但内存泄漏堆积后的后果就是内存溢出。
	内存溢出 out of memory :指程序申请内存时，没有足够的内存供申请者使用，或者说，给了你一块存储int类型数据的存储空间，但是你却存储long类型的数据，那么结果就是内存不够用，此时就会报错OOM,即所谓的内存溢出。
	

***\*内存泄漏与内存溢出的关系：\****

内存泄漏的堆积最终会导致内存溢出

内存溢出就是你要的内存空间超过了系统实际分配给你的空间，此时系统相当于没法满足你的需求，就会报内存溢出的错误。

内存泄漏是指你向系统申请分配内存进行使用(new)，可是使用完了以后却不归还(delete)，结果你申请到的那块内存你自己也不能再访问（也许你把它的地址给弄丢了），而系统也不能再次将它分配给需要的程序。就相当于你租了个带钥匙的柜子，你存完东西之后把柜子锁上之后，把钥匙丢了或者没有将钥匙还回去，那么结果就是这个柜子将无法供给任何人使用，也无法被垃圾回收器回收，因为找不到他的任何信息。

内存溢出：一个盘子用尽各种方法只能装4个果子，你装了5个，结果掉倒地上不能吃了。这就是溢出。比方说栈，栈满时再做进栈必定产生空间溢出，叫上溢，栈空时再做退栈也产生空间溢出，称为下溢。就是分配的内存不足以放下数据项序列,称为内存溢出。说白了就是我承受不了那么多，那我就报错，

 

内存溢出的原因及解决方法：

### **原因**

内存溢出原因：
	1、内存中加载的数据量过于庞大，如一次从数据库取出过多数据；

2、集合类中有对对象的引用，使用完后未清空，使得JVM不能回收；

3、代码中存在死循环或循环产生过多重复的对象实体；

4、使用的第三方软件中的BUG；

5、启动参数内存值设定的过小

### **解决方案**

内存溢出的解决方案：
	第一步，修改JVM启动参数，直接增加内存。(-Xms，-Xmx参数一定不要忘记加。)

第二步，检查错误日志，查看“OutOfMemory”错误前是否有其 它异常或错误。

第三步，对代码进行走查和分析，找出可能发生内存溢出的位置。

重点排查以下几点：
	1、检查对数据库查询中，是否有一次获得全部数据的查询。一般来说，如果一次取十万条记录到内存，就可能引起内存溢出。这个问题比较隐蔽，在上线前，数据库中数据较少，不容易出问题，上线后，数据库中数据多了，一次查询就有可能引起内存溢出。因此对于数据库查询尽量采用分页的方式查询。

2、检查代码中是否有死循环或递归调用。

3、 检查是否有大循环重复产生新对象实体。

4、检查对数据库查询中，是否有一次获得全部数据的查询。一般来说，如果一次取十万条记录到内存，就可能引起内存溢出。这个问题比较隐蔽，在上线前，数据库中数据较少，不容易出问题，上线后，数据库中数据多了，一次查询就有可能引起内存溢出。因此对于数据库查询尽量采用分页的方式查询。

5、检查List、MAP等集合对象是否有使用完后，未清除的问题。List、MAP等集合对象会始终存有对对象的引用，使得这些对象不能被GC回收。

第四步，使用内存查看工具动态查看内存使用情况

 

## **C野指针**

1、指针变量没有初始化

2、指针被free或delete后，没有设置为NULL

3、指针操作超越了变量的作用范围，比如返回指向栈内存的指针就是野指针

4、访问空指针（需要做空判断）

5、sizeof无法获取数组的大小

6、试图修改常量，如：char p='1234';p=\'1\';

 

## **C资源访问冲突**

1、多线程共享变量没有用valotile修饰

2、多线程访问全局变量未加锁

3、全局变量仅对单进程有效

4、多进程写共享内存数据，未做同步处理

5、mmap内存映射，多进程不安全

 

## **STL迭代器失效**

1、被删除的迭代器失效

2、添加元素（insert/push_back 等）、删除元素导致顺序容器迭代器失效

错误示例：删除当前迭代器，迭代器会失效 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7146.tmp.jpg) 

正确示例：迭代器 erase 时，需保存下一个迭代器

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7157.tmp.jpg) 

 

## **C++ 11智能指针**

1、auto_ptr 替换为 unique_ptr

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7158.tmp.jpg) 

2、使用 make_shared 初始化一个 shared_ptr

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps7159.tmp.jpg) 

weak_ptr 智能指针助手

（1）原理分析：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps715A.tmp.jpg) 

（2）数据结构：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wps716B.tmp.jpg) 

（3）使用方法：

a.lock() 获取所管理的对象的强引用指针 

b. expired() 检测所管理的对象是否已经释放 

c. get() 访问智能指针对象

 

## **C++ 11更小更快更安全**

1、std::atomic原子数据类型 多线程安全。

2、std::array定长数组开销比array小和std::vector不同的是array的长度是固定的，不能动态拓展。

3、std::vector vector瘦身shrink_to_fit()：将capacity减少为于size()相同的大小

4、td::forward_list

 

forward_list是单链表（std::list 是双链表），只需要顺序遍历的场合，forward_list 能更加节省内存，插入和删除的性能高于list

std::unordered_map、std::unordered_set用hash实现的无序的容器，插入、删除和查找的时间复杂度都是 O(1)，在不关注容器内元素顺序的场合，使用 unordered 的容器能获得更高的性能

 

 

 

 

 

 

 

 